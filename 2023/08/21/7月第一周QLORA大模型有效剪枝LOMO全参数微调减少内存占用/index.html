<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="123456789101112title: 翟景治周报0626-0702date: 2023-07-05tags:    - Qlora    - 模型剪枝    - Lomocategories:    - 翟景治    - 微调    - 剪枝author:    - 翟景治  7月第一周QLORA&#x2F;大模型有效剪枝&#x2F;LOMO:全参数微调减少内存占用论文1：QLORA：量化L">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8QLORA%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%89%E6%95%88%E5%89%AA%E6%9E%9DLOMO%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83%E5%87%8F%E5%B0%91%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="123456789101112title: 翟景治周报0626-0702date: 2023-07-05tags:    - Qlora    - 模型剪枝    - Lomocategories:    - 翟景治    - 微调    - 剪枝author:    - 翟景治  7月第一周QLORA&#x2F;大模型有效剪枝&#x2F;LOMO:全参数微调减少内存占用论文1：QLORA：量化L">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927766.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927895.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927319.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021429688.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021447017.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021459806.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020953125.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021044049.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021046738.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021048215.png">
<meta property="article:published_time" content="2023-08-21T07:31:26.231Z">
<meta property="article:modified_time" content="2023-08-21T07:52:50.008Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927766.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-7月第一周QLORA大模型有效剪枝LOMO全参数微调减少内存占用" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8QLORA%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%89%E6%95%88%E5%89%AA%E6%9E%9DLOMO%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83%E5%87%8F%E5%B0%91%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T07:31:26.231Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">title: 翟景治周报0626-0702</span><br><span class="line">date: 2023-07-05</span><br><span class="line">tags:</span><br><span class="line">    - Qlora</span><br><span class="line">    - 模型剪枝</span><br><span class="line">    - Lomo</span><br><span class="line">categories:</span><br><span class="line">    - 翟景治</span><br><span class="line">    - 微调</span><br><span class="line">    - 剪枝</span><br><span class="line">author:</span><br><span class="line">    - 翟景治</span><br></pre></td></tr></table></figure>

<h1 id="7月第一周QLORA-大模型有效剪枝-LOMO-全参数微调减少内存占用"><a href="#7月第一周QLORA-大模型有效剪枝-LOMO-全参数微调减少内存占用" class="headerlink" title="7月第一周QLORA&#x2F;大模型有效剪枝&#x2F;LOMO:全参数微调减少内存占用"></a>7月第一周QLORA&#x2F;大模型有效剪枝&#x2F;LOMO:全参数微调减少内存占用</h1><h1 id="论文1：QLORA：量化LLM的有效微调"><a href="#论文1：QLORA：量化LLM的有效微调" class="headerlink" title="论文1：QLORA：量化LLM的有效微调"></a>论文1：QLORA：量化LLM的有效微调</h1><h1 id="基本信息："><a href="#基本信息：" class="headerlink" title="基本信息："></a>基本信息：</h1><ul>
<li>题目：QLORA：量化LLM的有效微调 （QLORA：高效微调量化语言模型）</li>
<li>作者：蒂姆·德特默斯、阿蒂多罗·帕格诺尼、阿里·霍尔茨曼、卢克·泽特勒莫耶 （蒂姆·德特默斯、阿蒂多罗·帕格诺尼、阿里·霍尔茨曼、卢克·泽特尔莫耶）</li>
<li>隶属关系： 华盛顿大学</li>
<li>关键词：语言模型，微调，量化，低秩适配器（LLA），NormalFloat，双重量化，分页优化器，指令遵循，聊天机器人性能</li>
<li>网址：纸张：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14314v1%EF%BC%8CGitHub%EF%BC%9Ahttps://github.com/artidoro/qlora">https://arxiv.org/abs/2305.14314v1，GitHub：https://github.com/artidoro/qlora</a> 和 <a target="_blank" rel="noopener" href="https://github.com/TimDettmers/bitsandbytes">https://github.com/TimDettmers/bitsandbytes</a></li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><ul>
<li>a. 理论背景：<ul>
<li>本文讨论了QLORA和LoRA这两种参数高效的神经网络微调方法，以减少训练过程中的内存需求。LoRA通过分解投影来增强线性投影，而QLORA则使用4位NormalFloat量化和双量化来减少内存使用。QLORA还引入了分页优化器来防止梯度检查点过程中的内存溢出错误。</li>
</ul>
</li>
<li>b. 技术路线：<ul>
<li>QLORA使用LoRA方法在每个网络层引入适配器，以避免先前工作中出现的准确性折衷。QLORA使用4位NormalFloat（NF4）作为新的数据类型，该类型对于正态分布的权重是最优的。双量化用于量化量化常数，并使用分页优化器来管理内存峰值。</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927766.png" alt="image-20230625110446430"></p>
<blockquote>
<ol>
<li>LoRA（Low-Rank Adaptation）：这是一种微调方法，它通过低秩适应（low-rank adaptation）的技术来减少微调过程中的内存需求。具体而言，LoRA使用低秩矩阵分解的方法来近似和压缩原始的Transformer模型，以减少参数数量和内存占用。</li>
<li>QLORA（Quantized Low-Rank Adaptation）：这是QLORA对LoRA的改进。QLORA通过对Transformer模型进行量化，将模型参数的精度减少到4位，并使用分页式优化器（paged optimizers）来处理内存峰值，从而减少内存需求。</li>
</ol>
<p>关于QLORA相对于LoRA的改进，主要有两个方面：</p>
<ol>
<li>量化（Quantization）：QLORA使用较低的位精度（4位）来表示模型参数，从而降低了内存需求。通过减少每个参数的位数，可以大幅度减少存储参数所需的内存空间。</li>
<li>分页式优化器（Paged Optimizers）：QLORA使用分页式优化器来处理内存峰值问题。分页式优化器将模型参数分成多个页（pages），每次只加载一部分参数到内存中进行计算，以减少内存使用量。通过逐页加载和计算参数，可以处理大型模型在微调过程中可能产生的内存峰值问题。</li>
</ol>
</blockquote>
<h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><blockquote>
<p><strong>Block-wise k-bit Quantization</strong> 量化是将输入从保持更多信息的表示离散化到具有更少信息的表示的过程。这通常意味着将具有更多位的数据类型转换为更少位，例如从32位浮点数转换为8位整数。为了确保使用低位数据类型的整个范围，输入数据类型通常通过由输入元素的绝对最大值进行归一化而被重新缩放到目标数据类型范围中，输入元素通常被构造为张量。例如，将32位浮点（FP 32）张量量化为Int 8张量，范围为[-127，127]：<br>$$<br>\begin{aligned}<br>&amp; \<br>&amp;\mathbf{X}^{\mathrm{Int}}&amp; {}^{8}&#x3D;\mathrm{round}\left(\frac{127}{\mathrm{absmax}(\mathbf{X}^{\mathrm{FP32}})}\mathbf{X}^{\mathrm{FP32}}\right)&#x3D;\mathrm{round}(c^{\mathrm{FP32}}\cdot\mathbf{X}^{\mathrm{FP32}}),<br>\end{aligned}<br>$$</p>
<p>$$<br>\mathbf{dequant}(c^{\mathrm{FP32}},\mathbf{X}^{\mathrm{Int8}})&#x3D;{\frac{\mathbf{X}^{\mathrm{Int8}}}{c^{\mathrm{FP32}}}}&#x3D;\mathbf{X}^{\mathrm{FP32}}<br>$$</p>
<p><strong>Low-rank Adapters</strong>  低阶适配器（LoRA）微调是一种通过使用一小组可训练参数（通常称为适配器）来降低内存需求的方法，同时不更新保持固定的完整模型参数。随机梯度下降期间的梯度通过固定的预训练模型权重传递到适配器，该适配器被更新以优化损失函数。LoRA通过额外的因子分解投影来增强线性投影。</p>
<p>在参数高效微调（PEFT）方法中，LLM微调的大部分存储器占用来自激活梯度而不是来自学习的LoRA参数。对于在批量大小为1的FLAN v2上训练的7B LLaMA模型，LoRA权重相当于常用的原始模型权重的0.2%[28，37]，LoRA输入梯度的内存占用量为567 MB，而LoRA参数仅占用26 MB。使用梯度检查点[9]，输入梯度减少到每个序列平均18 MB，使其比所有LoRA权重组合更占用内存。相比之下，4位基本模型消耗5，048 MB内存。这突出了梯度检查点设置是重要的，但也突出了积极地减少LoRA参数的量仅产生较小的存储器益处。这意味着我们可以使用更多的适配器，而不会显著增加整体训练内存占用（详细的分类请参见附录G）。如后所述，这对于恢复完整的16位精度性能至关重要。</p>
</blockquote>
<h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>QLORA包含两个组件：4-bit NormalFloat量化和Double Quantization。其中：4-bit NormalFloat数据类型是基于Quantile Quantization技术开发的，通过估计输入张量的分位数来保证每个量化区间分配相等的值。Double Quantization是将额外的量化常数进行量化以减小内存开销的过程。</p>
<h3 id="4-bit-NormalFloat量化"><a href="#4-bit-NormalFloat量化" class="headerlink" title="4-bit NormalFloat量化"></a>4-bit NormalFloat量化</h3><p>NormalFloat（NF）数据类型建立在分位数量化[15]的基础上，这是一种信息理论上的最佳数据类型，可确保每个量化二进制具有从输入张量分配的相等数量的值。分位数量化通过经验累积分布函数估计输入张量的分位数来工作。分位数量化的主要限制是分位数估计的过程是昂贵的。因此，快速分位数近似算法，如SRAM分位数[15]，用于估计它们。由于这些分位数估计算法的近似性质，数据类型对于离群值具有较大的量化误差，离群值通常是最重要的值。当输入张量来自固定到量化常数的分布时，可以避免昂贵的分位数估计和近似误差。在这种情况下，输入张量具有相同的分位数，使得精确的分位数估计在计算上可行。</p>
<p>NF数据类型基于Quantile Quantization方法，它是一种信息论上最优的数据类型，可以确保将输入张量中的每个量化区间分配相等数量的值。Quantile quantization通过估计输入张量的分位数来实现，估计过程基于经验累积分布函数。Quantile quantization的主要限制在于分位数估计的过程比较昂贵。因此，需要使用快速的分位数近似算法（如SRAM quantiles）来进行估计。由于这些分位数估计算法是近似的，所以对于异常值（通常是最重要的值），数据类型存在较大的量化误差。当输入张量来自一个固定到量化常数的分布时，可以避免昂贵的分位数估计和近似误差。在这种情况下，输入张量具有相同的分位数，因此可以通过精确的分位数估计来降低计算成本。由于预训练的神经网络权重通常具有以零为中心的正态分布，标准差为σ（参见附录F），我们可以通过缩放σ来将所有权重转换为单个固定分布，使得该分布完全适配于我们的数据类型范围。对于我们的数据类型，我们将其范围设置为[-1, 1]。因此，数据类型的分位数和神经网络权重都需要被归一化到该范围内。对于零均值正态分布，标准差为σ且落在[-1, 1]范围内的情况，信息论上最优的数据类型计算步骤如下：（1）估计理论N(0, 1)分布的2k+1个分位数，以获得正态分布的k位量化数据类型；（2）将该数据类型的值归一化到[-1, 1]范围内；（3）通过绝对最大重缩放，将输入的权重张量归一化到[-1, 1]范围内。一旦权重范围和数据类型范围匹配，就可以像通常一样进行量化。步骤（3）等价于将权重张量的标准差重新缩放以匹配k位数据类型的标准差。<br>$$<br>q_i&#x3D;\frac{1}{2}\left(Q_X\left(\frac{i}{2^k+1}\right)+Q_X\left(\frac{i+1}{2^k+1}\right)\right),<br>$$</p>
<h3 id="Double-Quantization"><a href="#Double-Quantization" class="headerlink" title="Double Quantization"></a>Double Quantization</h3><p>这段文本介绍了一种称为Double Quantization (DQ) 的过程，用于对量化常数进行量化以实现更高的内存节省。精确的4位量化通常需要较小的块大小，但同时也会带来相当大的内存开销。例如，对于权重W，使用32位常数和块大小为64，平均每个参数需要增加32&#x2F;64 &#x3D; 0.5位的内存。Double Quantization可以帮助减少量化常数的内存占用。</p>
<p>具体而言，Double Quantization将第一次量化的量化常数cFP32作为第二次量化的输入。第二次量化得到了量化后的量化常数cFP8和第一层量化常数cFP32。我们在第二次量化中使用了8位浮点数（Floats），块大小为256，因为研究人员发现8位量化不会降低性能，这与Dettmers和Zettlemoyer的研究结果一致。</p>
<p>由于cFP32为正值，我们在量化之前从c2中减去均值，使值围绕零对称分布，以实现对称量化。平均而言，对于块大小为64，这种量化将每个参数的内存占用从32&#x2F;64 &#x3D; 0.5位减少到8&#x2F;64 + 32&#x2F;(64 · 256) &#x3D; 0.127位，每个参数减少了0.373位的内存占用。</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927895.png" alt="image-20230625141917832"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927319.png" alt="image-20230625142005506"></p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><ul>
<li>a. 详细的实验设置：<ul>
<li>使用QLORA，Guanaco模型系列在Vicuna基准测试中表现优于所有先前发布的模型，达到ChatGPT性能的99.3%，仅需要在单个GPU上进行24小时的微调。</li>
<li>QLORA用于微调超过1，000个模型，并对8个指令数据集、多个模型类型（LLaMA、T5）和模型规模进行了详细的指令跟随和聊天机器人性能分析。</li>
<li>结果表明，使用QLORA在小型高质量数据集上进行微调可以获得最先进的结果，即使使用比先前最先进的模型更小的模型。</li>
</ul>
</li>
<li>b. 详细的实验结果：<ul>
<li>QLORA在三种架构类型上的实验评估表明，其性能与完整模型微调相当，并优于16位适配器微调。</li>
<li>4位NormalFloat数据类型减少了量化误差，而双量化减少了内存开销，使QLORA成为一种有效且内存高效的微调方法。</li>
</ul>
</li>
</ul>
<h1 id="论文2：A-Simple-and-Effective-Pruning-Approach-for-Large-Language-Models"><a href="#论文2：A-Simple-and-Effective-Pruning-Approach-for-Large-Language-Models" class="headerlink" title="论文2：A Simple and Effective Pruning Approach for Large Language Models"></a>论文2：A Simple and Effective Pruning Approach for Large Language Models</h1><blockquote>
<ul>
<li>Title: A Simple and Effective Pruning Approach for Large Language Models (大型语言模型的简单有效修剪方法)</li>
<li>Authors: Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter</li>
<li>Affiliation: Carnegie Mellon University (卡内基梅隆大学)</li>
<li>Keywords: Large Language Models, Network pruning, Weight reconstruction, Sparsity, Magnitude pruning (大型语言模型，网络修剪，权重重构，稀疏性，幅度修剪)</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.11695v1">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/locuslab/wanda">GitHub</a></li>
<li>2023.06</li>
</ul>
</blockquote>
<h3 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h3><p>本研究提出了一种简单而有效的修剪方法，名为Wanda（基于权重和激活的修剪），用于大型语言模型（LLMs），通过在预训练的LLMs中修剪具有最小幅度的权重乘以相应的输入激活来诱导稀疏性。该方法无需重新训练或权重更新，并且修剪后的LLM可以直接使用。实验证明，Wanda在各种语言基准测试中明显优于幅度修剪，并与涉及大量权重更新的最新方法竞争激烈。</p>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 大型语言模型（LLMs）在复杂语言基准测试中表现出色，但由于其庞大的参数规模，需要大量的计算资源。为了降低LLMs的计算成本，研究人员一直在努力寻找有效的压缩方法。</li>
<li>过去方案: 以往的修剪方法要么需要重新训练，这对于规模庞大的LLMs来说很少可行，要么需要解决依赖于二阶信息的权重重构问题，这可能也会带来计算负担。这些方法在压缩LLMs方面的应用相对较少，这与在LLM之前的模型压缩领域的趋势相矛盾。</li>
<li>论文的Motivation: 鉴于以往方法的局限性，本研究旨在提出一种简单而有效的修剪方法，用于直接从预训练的LLMs中找到高效的稀疏网络，而无需重新训练或权重更新。通过观察LLMs中出现的大幅度特征，作者提出了一种基于权重和激活的修剪度量，并在每个输出上比较权重，以实现对LLMs的稀疏化。作者通过实验证明，该方法在性能上明显优于幅度修剪，并且在计算成本上要低于最新的LLMs修剪方法。</li>
<li>幅度修剪（Magnitude Pruning）：是一种剪枝技术，用于稀疏化神经网络中的权重参数。该技术通过保留模型中最重要的权重，将较小幅度的权重置为零或接近零的值，以达到减少模型复杂度和提高推理速度的目的。</li>
<li>Emergent Large Magnitude Features：（ELM特征）是指在训练神经网络时，由于网络的非线性特性和优化过程中的相互作用，某些具有较大幅度的特征或神经元在网络的上层逐渐变得重要。</li>
</ul>
<h1 id="方法-2"><a href="#方法-2" class="headerlink" title="方法:"></a>方法:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021429688.png" alt="image-20230702142920614"></p>
<blockquote>
<p>举个例子说明传统剪枝的额局限性：考虑一个具有两个输入和相应权重的神经元，其中y &#x3D; w1x1 + w2x2，且|w1| ≤ |w2|。传统的幅度剪枝方法总是会选择移除权重w1，但是在LLMs中，输入特征x1和x2的尺度可能存在显著差异。在某些情况下，可能有|x1| ≫ |x2|，导致|w1x1| ≫ |w2x2|。因此，在这种情况下，我们应该选择移除权重w2，因为相比于移除权重w1，它对神经元输出y的影响更小。</p>
<p>为了解决幅度剪枝方法的这个局限性，该段介绍了一种针对LLMs设计的剪枝度量。对于完全连接的线性层，考虑形状为(Cout, Cin)的权重W，输入激活为形状为(N × L, Cin)的X。对于每个权重，作者提出通过其幅度和相应输入特征范数的乘积来评估其重要性。具体地，当前权重Wij的得分由以下公式定义： Sij &#x3D; |Wij| · ∥Xj∥2 其中|·|表示绝对值运算符，∥Xj∥2计算了聚合在N × L个不同标记上的第j个特征的ℓ2范数，最终得分由这两个标量值的乘积计算而得。作者发现，ℓ2范数在衡量激活幅度时往往比其他范数函数（如ℓ1和ℓ∞范数）效果更好。</p>
<p>2范数（L2 norm），也称为欧几里德范数，用于计算向量的长度或矩阵的平方和的平方根。对于一个向量x&#x3D;(x1, x2, …, xn)，其ℓ2范数定义如下：</p>
<p>∥x∥2 &#x3D; sqrt(x1^2 + x2^2 + … + xn^2)</p>
</blockquote>
<ul>
<li>a. 理论背景:<ul>
<li>大型语言模型（LLMs）的重要性和由于其规模和计算要求而带来的挑战。</li>
<li>网络修剪方法的需求，以减小LLMs的规模。</li>
<li>现有方法要么需要重新训练，要么需要计算昂贵的权重重构。</li>
<li>引入一种名为Wanda的新型修剪方法，根据权重的大小乘以相应的输入激活来修剪权重。</li>
<li>Wanda不需要重新训练或权重更新，可以直接用于预训练的LLMs。</li>
<li>Wanda在性能和计算成本方面优于幅度修剪和其他最近的方法。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>Wanda是为修剪大型语言模型（LLMs）而设计的。</li>
<li>引入了一种修剪度量，根据输入特征激活的范数向量和权重的绝对值的点积来计算权重的重要性。</li>
<li>这种度量是稳健的，并且可以使用适量的校准样本进行估计。</li>
<li>Wanda建议使用更局部的修剪粒度级别，按输出逐个比较和删除权重。</li>
<li>修剪过程可以在LLM模型的单次前向传递中无缝实现，无需权重更新或进一步训练。</li>
<li>Wanda还可以扩展到结构化的N:M稀疏性，其中每M个连续权重中最多有N个非零。</li>
</ul>
</li>
</ul>
<h1 id="结果-1"><a href="#结果-1" class="headerlink" title="结果:"></a>结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021447017.png" alt="image-20230702144742941"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021459806.png" alt="image-20230702145942743"></p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>在LLaMA模型系列上评估了Wanda方法，该系列包括各种参数级别的Transformer语言模型。</li>
<li>将修剪方法应用于四个LLaMA模型：LLaMA-7B，LLaMA-13B，LLaMA-30B和LLaMA-65B。</li>
<li>通过在保留验证集上计算困惑度来评估修剪网络的性能。</li>
<li>使用128个从C4训练数据中采样的序列作为校准数据，用于估计输入统计信息。</li>
<li>Perplexity: 是一种用于评估语言模型性能的指标。它衡量模型对给定序列的预测能力和不确定性。较低的困惑度表示模型在给定序列上的预测更准确和更自信。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>将Wanda与两种先前的修剪方法（幅度修剪和SparseGPT）进行比较。</li>
<li>Wanda保持了幅度修剪的简单性，但在发现预训练LLMs中的稀疏网络方面非常有效。</li>
<li>Wanda比SparseGPT更快，因为它不涉及逆计算。</li>
<li>SparseGPT（稀疏GPT）是一种基于稀疏注意力机制的语言模型，它是对GPT（Generative Pre-trained Transformer）模型的改进和扩展。SparseGPT使用稀疏的注意力权重矩阵，其中只有少数非零元素，而其他元素为零。这种稀疏性允许SparseGPT在保持相对较低的计算成本的同时，仍能保持较好的模型性能。</li>
<li>提供了一个比较表，显示每种方法使用的时间复杂度和修剪度量。</li>
</ul>
</li>
</ul>
<h1 id="论文3：LOMO-Full-Parameter-Fine-tuning-for-Large-Language-Models-with-Limited-Resources"><a href="#论文3：LOMO-Full-Parameter-Fine-tuning-for-Large-Language-Models-with-Limited-Resources" class="headerlink" title="论文3：LOMO:Full Parameter Fine-tuning for Large Language Models with Limited Resources"></a>论文3：LOMO:Full Parameter Fine-tuning for Large Language Models with Limited Resources</h1><p><strong>论文：LOMO：利用有限的资源对大型语言模型进行全参数微调</strong></p>
<blockquote>
<p>ArXiv: <a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/2306.09782.pdf">https://arxiv.org/pdf/2306.09782.pdf</a><br>机构：复旦大学<br>时间：2023.6.16<br>Github：<a href="https://link.zhihu.com/?target=https://github.com/OpenLMLab/LOMO">https://github.com/OpenLMLab/LOMO</a></p>
</blockquote>
<h2 id="摘要：-1"><a href="#摘要：-1" class="headerlink" title="摘要："></a>摘要：</h2><p>大型语言模型（LLMs）已经彻底改变了自然语言处理（NLP），但是训练LLMs需要大量的GPU资源。降低LLMs训练的门槛将鼓励更多研究人员参与，从而使学术界和社会受益。虽然现有的方法着重于参数高效微调，即微调或添加少量参数，但很少有人解决了有限资源下调整LLMs的全部参数的挑战。在本文中，我们提出了一种新的优化器LOw-Memory Optimization（LOMO），它将梯度计算和参数更新融合在一步中以减少内存使用。通过将LOMO与现有的内存节省技术集成，我们将内存使用降低到10.8％，与标准方法（DeepSpeed解决方案）相比。因此，我们的方法使单台机器上的65B模型的全参数微调成为可能，该机器配有8×RTX 3090，每个显存为24GB。</p>
<h2 id="IDEA"><a href="#IDEA" class="headerlink" title="IDEA"></a>IDEA</h2><blockquote>
<p>分析了SGD可以finetune LLM的原因，不用Adam改用SGD，在SGD的基础上提出了一个LOw-Memory Optimization（LOMO）的优化器，来全参数finetune LLM，并在下游任务上获得了比lora等更好的效果。（可能因为资源问题没对比Adam的全参数finetune的结果，这个还不够有说服力）8张3090能微调65B的模型了</p>
</blockquote>
<h2 id="背景知识："><a href="#背景知识：" class="headerlink" title="背景知识："></a>背景知识：</h2><blockquote>
<h3 id="1-SGD代表随机梯度下降（Stochastic-Gradient-Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。"><a href="#1-SGD代表随机梯度下降（Stochastic-Gradient-Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。" class="headerlink" title="1.SGD代表随机梯度下降（Stochastic Gradient Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。"></a>1.SGD代表随机梯度下降（Stochastic Gradient Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。</h3><p>下面是SGD的详细解释：</p>
<ol>
<li><p>梯度下降：梯度下降是一种优化算法，用于找到函数的局部最小值或全局最小值。在机器学习中，我们使用梯度下降来最小化模型的损失函数。梯度表示函数在给定点的变化率方向，通过朝着梯度的反方向调整模型参数，我们可以逐步降低损失函数的值，从而找到最优参数。</p>
</li>
<li><p>随机梯度下降：随机梯度下降是梯度下降的一种变种，其目标是加速收敛过程。与传统的梯度下降一次性使用整个训练数据来计算损失函数的梯度相比，随机梯度下降每次迭代仅使用一个训练样本来估计梯度。这样做的好处是计算成本较低，特别是在大规模数据集上，同时它也使得算法更具随机性，有助于避免陷入局部最优解。</p>
</li>
<li><p>迭代过程：SGD的迭代过程如下：</p>
<ul>
<li>初始化模型参数，例如权重和偏置。</li>
<li>将训练数据打乱顺序，以随机顺序遍历样本。</li>
<li>对于每个训练样本，计算损失函数关于模型参数的梯度。</li>
<li>使用梯度来更新模型参数，通常是通过将梯度乘以一个学习率来确定更新步长。</li>
<li>重复以上步骤，直到达到指定的迭代次数或满足停止准则（例如损失函数的收敛）。</li>
</ul>
</li>
<li><p>学习率：学习率是SGD算法中的一个重要参数，用于控制参数更新的步长。较大的学习率可能导致参数更新过大，错过最优解；而较小的学习率可能导致收敛速度缓慢。因此，选择合适的学习率是使用SGD的关键之一。在实践中，可以使用学习率调度策略来逐步减小学习率，以获得更好的收敛性能。</p>
</li>
<li><p>批量大小：除了使用单个样本计算梯度外，SGD还支持使用多个样本计算梯</p>
</li>
</ol>
<p>度。将一批样本一起计算梯度称为小批量随机梯度下降（mini-batch SGD）。小批量SGD通常比单个样本的SGD更稳定，因为它可以减少参数更新的方差，并利用并行计算的优势。批量大小是一个需要调整的超参数，通常根据可用的计算资源和训练数据的规模进行选择。</p>
<p>总结来说，SGD是一种基于梯度下降的优化算法，通过每次迭代使用一个样本（或一小批样本）的梯度来更新模型参数。它是一种高效的训练算法，特别适用于大规模数据集和复杂模型。然而，SGD也有一些缺点，例如可能陷入局部最优解和对学习率的敏感性。因此，在使用SGD时需要进行适当的参数调整和正则化技术的应用。</p>
</blockquote>
<blockquote>
<p>2.Adam算法</p>
<p>Adam是一种优化算法，全称为自适应矩估计优化算法（Adaptive Moment Estimation）。它是一种基于梯度的优化算法，结合了梯度下降和动量优化的思想。Adam算法具有较快的收敛速度和良好的性能，在深度学习领域广泛使用。</p>
<p>下面是Adam算法的详细解释：</p>
<ol>
<li><p>梯度下降：梯度下降是一种基本的优化算法，用于最小化损失函数。它通过计算损失函数关于模型参数的梯度，并朝着梯度的反方向更新参数，以逐步降低损失函数的值。</p>
</li>
<li><p>动量优化：动量优化是一种改进的梯度下降算法，引入了动量（momentum）的概念。动量表示模型更新的惯性，使得参数更新在当前梯度方向的基础上，还考虑了历史梯度的影响。这有助于加速收敛，尤其在存在平坦区域或峡谷的情况下。</p>
</li>
<li><p>自适应学习率：Adam算法引入了自适应学习率的概念，通过自动调整学习率的大小来适应每个参数的变化情况。它基于梯度的一阶矩估计（mean）和二阶矩估计（variance）来自适应地调整学习率。</p>
</li>
<li><p>Adam算法的更新步骤：</p>
<ul>
<li>初始化模型参数和累计变量（一阶和二阶矩估计的初始值）。</li>
<li>在每次迭代中，计算当前的梯度。</li>
<li>更新一阶矩估计和二阶矩估计。</li>
<li>根据一阶和二阶矩估计计算参数更新的方向和大小。</li>
<li>更新模型参数。</li>
<li>重复以上步骤，直到达到指定的迭代次数或满足停止准则。</li>
</ul>
</li>
<li><p>Adam算法的优点：</p>
<ul>
<li>自适应学习率：Adam算法可以自适应地调整每个参数的学习率，根据梯度的一阶和二阶矩估计进行缩放。这有助于在训练过程中平衡收敛速度和参数稳定性。</li>
<li>适用于大规模数据和高维参数：Adam算法对于大规模数据和高维参数的训练具有较好的效果，因为它可以有效地利用梯度信息和自适应学习率。</li>
<li>低内存要求：相对于其他优化算法（如基于Hessian矩阵的方法），Adam算法的内存要求较低，因为它仅需要存储一阶和二阶矩估计。</li>
</ul>
<p>需要注意的是，Adam算法也有一些超参数需要调整，如学习率、动量系数和指数衰减率等，这些超参数的选择可能会对算法的性能产生影响。因此，在实践中，常常需要进行超参数调优来获得最佳的性能。</p>
</li>
</ol>
</blockquote>
<h2 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020953125.png" alt="image-20230702095337036"></p>
<p>为什么会用SGD：以前的研究经常讨论SGD的三个挑战：1）大曲率损失表面，2）局部最优解，以及3）鞍点（Ruder，2016; Sun等，2020）。现代优化器已经显示出处理1）问题的有效性，并且在某些情况下可以减轻2）和3）问题。然而，当我们将范围限定为微调LLM时，这三个挑战可能会有所不同。</p>
<p>更平滑的损失面（<strong>Smoother loss surface</strong>）</p>
<p>一个重要的假设是LLM的参数空间非常平滑，对参数进行小的扰动不会显着改变损失。有实证结果和理论分析支持这一假设（Hao等，2019）。如果我们相信更大的模型具有更平滑的损失曲面，那么我们可以得出结论：由于LLM的损失表面不应具有很大的曲率，因此1）的问题不是问题。请注意，仅当我们教LLM基于自然语言的任务（或者像以前一样用代码预训练）时，才有效。与预训练任务无关的合成损失函数确实会面临大曲率问题。</p>
<p>局部最优解已经足够（<strong>Local optimum is good enough</strong>）</p>
<p>微调的目标是将LLM调整到新任务和领域中，并且不会显着改变模型本身。因此，局部最优解通常是足够好的解决方案，并且有限的训练数据（与预训练语料库相比）使其难以将模型推向遥远的全局最优解。</p>
<p>算法： LOMO中的融合更新</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021044049.png" alt="image-20230702104423997"></p>
<blockquote>
<ol>
<li><p>输入：模型f(·)具有L层和p个参数，参数θ ∈ Rp，学习率α，最大步数T，训练数据集D，损失函数L。</p>
</li>
<li><p>对于每个步骤t &#x3D; 1, . . . , T 进行以下操作：</p>
<p>2.1 从训练数据集D中随机抽取一个批次B &#x3D; (x, y)，其中x是输入数据，y是对应的目标标签。</p>
<p>2.2 通过模型f(·)进行前向传播，计算预测值ŷ： ŷ ← f(x, θ) 这一步骤用于获取模型的输出。</p>
<p>2.3 计算损失函数ℓ： ℓ ← L(y, ŷ) 这一步骤用于计算模型预测值ŷ与真实标签y之间的差异。</p>
<p>2.4 对于每一层l &#x3D; L, . . . , 1 进行以下操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1 反向传播（Backward propagation）：</span><br><span class="line">      获取当前层的参数集合θl。</span><br><span class="line">2 计算当前层的梯度gl：</span><br><span class="line">      gl ← ∂ℓ/∂θl</span><br><span class="line">      这一步骤通过计算损失函数ℓ关于当前层参数θl的偏导数来获取梯度。</span><br><span class="line"></span><br><span class="line">3 参数更新：</span><br><span class="line">      θl ← θl - α * gl</span><br><span class="line">      使用梯度下降的方式更新当前层的参数θl。学习率α确定了参数更新的步长。</span><br><span class="line">4 清除梯度：</span><br><span class="line">      将当前层的梯度gl设置为None，以清除梯度信息。</span><br></pre></td></tr></table></figure>

<p>2.5 结束当前步骤的循环。</p>
</li>
<li><p>结束算法。</p>
</li>
</ol>
</blockquote>
<blockquote>
<p>这个算法是LOMO中的融合更新算法，它使用随机梯度下降（SGD）的方式对模型的参数进行更新。它在每个步骤中，通过随机抽取一个批次样本进行前向传播和损失计算，然后进行反向传播来计算梯度，并使用梯度下降的方式更新模型的参数。整个过程重复T个步骤，直到达到最大步数T。</p>
</blockquote>
<h2 id="实验："><a href="#实验：" class="headerlink" title="实验："></a>实验：</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021046738.png" alt="image-20230702104644688"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021048215.png" alt="image-20230702104809171"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8QLORA%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%89%E6%95%88%E5%89%AA%E6%9E%9DLOMO%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83%E5%87%8F%E5%B0%91%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8/" data-id="cllkmcmev0003ksu711yk9go2" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%92%B8%E9%A6%8F%E8%AE%BA%E6%96%87/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E5%9B%9B%E5%91%A8%E4%BE%9D%E8%B5%96%E5%9B%BE%E8%AE%BA%E6%96%87%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/QLORA-%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/" rel="tag">QLORA 剪枝 lomo全参数微调</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%89%AA%E6%9E%9D-%E8%92%B8%E9%A6%8F/" rel="tag">剪枝+蒸馏</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E9%9B%86%E8%BF%9E%E6%8E%A5%E3%80%81%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%89%AA%E6%9E%9D/" rel="tag">密集连接、滤波器剪枝</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%8Cpromot%EF%BC%8Clora/" rel="tag">线性注意力，promot，lora</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B7%A8%E8%AF%AD%E8%A8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" rel="tag">跨语言多模态知识蒸馏</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" rel="tag">通用剪枝</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/QLORA-%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/" style="font-size: 10px;">QLORA 剪枝 lomo全参数微调</a> <a href="/tags/%E5%89%AA%E6%9E%9D-%E8%92%B8%E9%A6%8F/" style="font-size: 10px;">剪枝+蒸馏</a> <a href="/tags/%E5%AF%86%E9%9B%86%E8%BF%9E%E6%8E%A5%E3%80%81%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%89%AA%E6%9E%9D/" style="font-size: 10px;">密集连接、滤波器剪枝</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%8Cpromot%EF%BC%8Clora/" style="font-size: 10px;">线性注意力，promot，lora</a> <a href="/tags/%E8%B7%A8%E8%AF%AD%E8%A8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" style="font-size: 10px;">跨语言多模态知识蒸馏</a> <a href="/tags/%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" style="font-size: 10px;">通用剪枝</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8cvil%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">7月第二周cvil多模态知识蒸馏</a>
          </li>
        
          <li>
            <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%89%E5%91%A8/">8月第三周HomoDistil</a>
          </li>
        
          <li>
            <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3/">8月第二周剪枝相关</a>
          </li>
        
          <li>
            <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%92%B8%E9%A6%8F/">8月第一周自然语言大模型蒸馏</a>
          </li>
        
          <li>
            <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%91%A8%E6%8A%A5%E4%BE%9D%E8%B5%96%E5%9B%BE%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/">7月第四周周报依赖图通用剪枝</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>