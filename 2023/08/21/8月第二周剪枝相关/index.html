<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>8月第二周剪枝相关 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="8月第二周 以前的剪枝相关论文论文1：Densely Connected Convolutional Networks (密集连接卷积网络)Basic Information: Title: Densely Connected Convolutional Networks (密集连接卷积网络) Authors: Gao Huang, Zhuang Liu, Laurens van der Maat">
<meta property="og:type" content="article">
<meta property="og:title" content="8月第二周剪枝相关">
<meta property="og:url" content="http://example.com/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="8月第二周 以前的剪枝相关论文论文1：Densely Connected Convolutional Networks (密集连接卷积网络)Basic Information: Title: Densely Connected Convolutional Networks (密集连接卷积网络) Authors: Gao Huang, Zhuang Liu, Laurens van der Maat">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604156.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604162.png">
<meta property="og:image" content="http://example.com/doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812120140992.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604167.png">
<meta property="og:image" content="http://example.com/doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812120217759.png">
<meta property="og:image" content="http://example.com/doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812120300514.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604154.png">
<meta property="og:image" content="http://example.com/doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812121225490.png">
<meta property="og:image" content="http://example.com/doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812121245625.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604181.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604492.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604736.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604371.png">
<meta property="og:image" content="http://example.com/doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812124123590.png">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604674.png">
<meta property="og:image" content="http://example.com/doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125926446.png">
<meta property="og:image" content="http://example.com/doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812130521561.png">
<meta property="og:image" content="http://example.com/doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812130609383.png">
<meta property="og:image" content="http://example.com/doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131322763.png">
<meta property="og:image" content="http://example.com/doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131413844.png">
<meta property="og:image" content="http://example.com/doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131533751.png">
<meta property="og:image" content="http://example.com/doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131557560.png">
<meta property="og:image" content="http://example.com/doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131517584.png">
<meta property="og:image" content="http://example.com/doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125737860.png">
<meta property="og:image" content="http://example.com/doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125805978.png">
<meta property="og:image" content="http://example.com/doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125815070.png">
<meta property="article:published_time" content="2023-08-21T08:03:58.000Z">
<meta property="article:modified_time" content="2023-08-21T08:04:59.709Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="密集连接、滤波器剪枝">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604156.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-8月第二周剪枝相关" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T08:03:58.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      8月第二周剪枝相关
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="8月第二周-以前的剪枝相关论文"><a href="#8月第二周-以前的剪枝相关论文" class="headerlink" title="8月第二周 以前的剪枝相关论文"></a>8月第二周 以前的剪枝相关论文</h1><h1 id="论文1：Densely-Connected-Convolutional-Networks-密集连接卷积网络"><a href="#论文1：Densely-Connected-Convolutional-Networks-密集连接卷积网络" class="headerlink" title="论文1：Densely Connected Convolutional Networks (密集连接卷积网络)"></a>论文1：Densely Connected Convolutional Networks (密集连接卷积网络)</h1><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Densely Connected Convolutional Networks (密集连接卷积网络)</li>
<li>Authors: Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger</li>
<li>Affiliation: First author’s affiliation: Cornell University (第一作者所属机构：康奈尔大学)</li>
<li>Keywords: convolutional networks, deep learning, dense connectivity, vanishing-gradient problem, feature reuse (卷积网络，深度学习，密集连接，梯度消失问题，特征重用)</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.06993">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/liuzhuang13/DenseNet">GitHub Code</a></li>
<li>16年8月</li>
</ul>
<h1 id="论文简要"><a href="#论文简要" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604156.png" alt="b8624ba6215bcbdf5fbd0071bbd3eccf_0_Figure_1"></p>
<p>图1：DenseNet在ResNet的基础上(ResNet介绍)，进一步扩展网络连接，该层前面所有层的feature map都是这层的输入，该层的feature map是后面所有层的输入。网络结构如上图:</p>
<ul>
<li>动机：随着CNN网络层数的不断增加，gradient vanishing和modeldegradation问题出现在了人们面前，BatchNormalization的广泛使用在一定程度上缓解了gradient vanishing的问题，而ResNet和HighwayNetworks通过构造恒等映射设置旁路，进一步减少了gradient vanishing和model degradation的产生。</li>
</ul>
<p>创新点总结:</p>
<p>(1)相比ResNet拥有更少的参数数量<br>(2)旁路加强了特征的重用.<br>(3)网络更易于训练，并具有一定的正则效果<br>(4)缓解了gradient vanishing和model degradation的问题</p>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 卷积神经网络（CNNs）已成为视觉目标识别的主要机器学习方法。然而，随着网络层数的增加，信息和梯度在网络中传递时可能会消失，这成为一个新的研究问题。</li>
<li>过去方案: 过去的研究提出了一些解决梯度消失问题的方法，如ResNets和Highway Networks，通过建立早期层与后期层之间的短路径来保留信息。然而，这些方法仍存在一些问题，如参数冗余和训练困难。</li>
<li>CNN史上的一个里程碑事件是ResNet模型的出现，ResNet可以训练出更深的CNN模型，从而实现更高的准确度。ResNet模型的核心是通过建立前面层与后面层之间的“短路连接”（shortcuts，skip connection），这有助于训练过程中梯度的反向传播，从而能训练出更深的CNN网络。</li>
<li>DenseNet模型，它的基本思路与ResNet一致，但是它建立的是前面所有层与后面层的密集连接（dense connection），它的名称也是由此而来。DenseNet的另一大特色是通过特征在channel上的连接来实现特征重用（feature reuse）。这些特点让DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能，DenseNet也因此斩获CVPR 2017的最佳论文奖。</li>
<li>论文的Motivation: 基于过去方法的观察，本文提出了一种新的网络架构，即DenseNet，通过密集连接每一层，实现了信息的最大流动和特征的重用。这种架构具有更好的参数效率和训练性能。</li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><p>Residual Connection 能够让模型趋向于宽网络，Densenet论文核心思想:对每一层的前面所有层都加一个单独的 shortcut到该层，使得任意两层网络都可以直接”沟通”。即下图：</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604162.png" alt="b8624ba6215bcbdf5fbd0071bbd3eccf_2_Figure_2"></p>
<h3 id="1-密集块（Dense-Block）受到GoogleNet的启发，论文提出了Dense-Block，即在每个Block内，所有layer都保持dense-connectivity，而在Block之间是没有dense-connectivity，而是通过transition-layer连接的"><a href="#1-密集块（Dense-Block）受到GoogleNet的启发，论文提出了Dense-Block，即在每个Block内，所有layer都保持dense-connectivity，而在Block之间是没有dense-connectivity，而是通过transition-layer连接的" class="headerlink" title="1. 密集块（Dense Block）受到GoogleNet的启发，论文提出了Dense Block，即在每个Block内，所有layer都保持dense connectivity，而在Block之间是没有dense connectivity，而是通过transition layer连接的"></a>1. 密集块（Dense Block）受到GoogleNet的启发，论文提出了Dense Block，即在每个Block内，所有layer都保持dense connectivity，而在Block之间是没有dense connectivity，而是通过transition layer连接的</h3><p>DenseNet的核心是一种名为“密集块”的结构。在密集块中，每一层都与之前所有层直接连接。这意味着第 ( i ) 层的输入不仅是第 ( i-1 ) 层的输出，还包括第 ( i-2 ) 层、第 ( i-3 ) 层等的输出。每一层的输出又成为其后所有层的输入。</p>
<p>这种密集连接的数学表达可以用下式表示：</p>
<p>$$<br>x_{l} &#x3D; H_{l}([x_{0}, x_{1}, \ldots, x_{l-1}])<br>$$</p>
<p>其中，( x_{l} ) 是第 ( l ) 层的输出，( H_{l} ) 是第 ( l ) 层的转换函数（如卷积、激活函数等），方括号表示将所有输入连接在一起。</p>
<p>H()就是Composite function，,每个Composite function的结构如下:<br>即单个Block内，层与层之间的非线性转换函数：</p>
<p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812120140992.png" alt="image-20230812120140992"></p>
<h3 id="2-过渡层（Transition-Layer）"><a href="#2-过渡层（Transition-Layer）" class="headerlink" title="2. 过渡层（Transition Layer）"></a>2. 过渡层（Transition Layer）</h3><p>由于密集连接可能导致特征图尺寸和通道数急剧增加，DenseNet引入了“过渡层”来控制网络的复杂性。过渡层通常包括卷积层和池化层，用于减少特征图的尺寸和通道数。结构如下</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604167.png" alt="image-20230812120040464"></p>
<h3 id="3-增长率（Growth-Rate）"><a href="#3-增长率（Growth-Rate）" class="headerlink" title="3. 增长率（Growth Rate）"></a>3. 增长率（Growth Rate）</h3><p>增长率是DenseNet中的一个关键参数，用于控制每一层添加的特征数量。假设增长率为 ( k )，则每一层产生 ( k ) 个新特征图，并将其与之前的特征图连接在一起。增长率有助于平衡网络的容量和计算复杂性。</p>
<p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812120217759.png" alt="image-20230812120217759"></p>
<h3 id="4-Bottleneck-Layer"><a href="#4-Bottleneck-Layer" class="headerlink" title="4. Bottleneck Layer"></a>4. Bottleneck Layer</h3><p>为了进一步减少计算负担（即使是每层只产生k个feature maps，但还是很多），DenseNet通常在每个卷积层之前添加一个“瓶颈层”（Bottleneck Layer）。瓶颈层包括1x1的卷积操作，用于减少特征图的通道数，然后再传递到更复杂的卷积层。</p>
<p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812120300514.png" alt="image-20230812120300514"></p>
<h3 id="5-合适的损失函数和优化器"><a href="#5-合适的损失函数和优化器" class="headerlink" title="5. 合适的损失函数和优化器"></a>5. 合适的损失函数和优化器</h3><p>与其他深度学习模型一样，DenseNet也需要选择合适的损失函数和优化器来进行训练。常用的损失函数如交叉熵损失，优化器如Adam或SGD等。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>DenseNet通过引入密集连接实现了特征的有效传播和重用，减轻了梯度消失问题，降低了参数数量。其独特的密集块、过渡层、增长率和瓶颈层等设计使其在多项计算机视觉任务上表现出色。</p>
<ul>
<li>a. 理论背景:<ul>
<li>本文介绍了密集卷积网络（DenseNets），它以前馈方式将每一层与其他每一层相连接。这种密集连接模式具有几个优点，包括缓解梯度消失问题、增强特征传播、鼓励特征重用和减少参数数量。DenseNets在目标识别基准任务上取得了显著的改进，同时需要更少的计算量。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>DenseNets通过特征重用来利用网络的潜力，从而得到易于训练和参数高效的紧凑模型。它们将任意一层与所有后续层直接连接，改善了层之间的信息流动。每一层的复合函数包括批归一化、修正线性单元和卷积。为了便于下采样，网络被划分为具有过渡层的密集连接的密集块。网络的增长率决定了每一层对网络全局状态的贡献程度。还使用瓶颈层和压缩因子来提高计算效率和模型紧凑性。</li>
</ul>
</li>
</ul>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果:"></a>实验结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604154.png" alt="image-20230812121120350"></p>
<ul>
<li><p>使用的是DenseNet-BC</p>
</li>
<li><p>使用4个Dense Block</p>
</li>
<li><p>在送入第一个Dense Block前，会先送到一个7x7x2k的stride&#x3D;2的卷积层</p>
</li>
<li><p>o所有的layers的feature map都设置为k</p>
</li>
<li><p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812121225490.png" alt="image-20230812121225490"></p>
</li>
<li><p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812121245625.png" alt="image-20230812121245625"></p>
</li>
<li><p>a. 详细的实验设置:</p>
<ul>
<li>DenseNet-BC在CIFAR数据集上取得了3.46%的错误率（C10+）和17.18%的错误率（C100+），优于现有的最先进结果。</li>
<li>DenseNet-BC在SVHN数据集上取得了比wide ResNet更好的结果，L&#x3D;100，k&#x3D;24，使用了dropout。</li>
</ul>
</li>
<li><p>b. 详细的实验结果:</p>
<ul>
<li>250层的DenseNet-BC在SVHN上的性能与较短的对应模型相比没有进一步提高。</li>
<li>DenseNet-BC在ImageNet分类任务上与最先进的ResNet架构相比，性能相当，但需要更少的参数和计算量。</li>
</ul>
</li>
</ul>
<h3 id="Q1-论文试图解决什么问题？"><a href="#Q1-论文试图解决什么问题？" class="headerlink" title="Q1 论文试图解决什么问题？"></a>Q1 论文试图解决什么问题？</h3><p>论文介绍了一种名为密集连接卷积网络（Dense Convolutional Network，简称 DenseNet）的新型结构。这一结构旨在解决卷积神经网络（CNN）中的一些核心问题，如梯度消失问题、特征传播的困难、特征重用的不足以及参数数量的增多。</p>
<h3 id="Q2-这是否是一个新的问题？"><a href="#Q2-这是否是一个新的问题？" class="headerlink" title="Q2 这是否是一个新的问题？"></a>Q2 这是否是一个新的问题？</h3><p>DenseNet提供了一种新的解决方案，通过密集连接每一层到其它所有层来解决这些问题。</p>
<h3 id="Q3-这篇文章要验证一个什么科学假设？"><a href="#Q3-这篇文章要验证一个什么科学假设？" class="headerlink" title="Q3 这篇文章要验证一个什么科学假设？"></a>Q3 这篇文章要验证一个什么科学假设？</h3><p>文章的科学假设是，通过在网络中引入密集连接，可以增强特征传播，减轻梯度消失问题，增强特征重用，并显著减少参数数量。这种连接方式被认为可以提高准确性并降低训练成本。</p>
<h3 id="Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？"><a href="#Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？" class="headerlink" title="Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？"></a>Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h3><p>相关研究包括不同类型的卷积神经网络结构，如LeNet、VGG等。这些研究可以归类为深度学习和计算机视觉领域内的网络架构研究。此论文的作者，如Gao Huang、Zhuang Liu、Laurens van der Maaten和Kilian Q. Weinberger等，都是该领域值得关注的研究员。</p>
<h3 id="Q5-论文中提到的解决方案之关键是什么？"><a href="#Q5-论文中提到的解决方案之关键是什么？" class="headerlink" title="Q5 论文中提到的解决方案之关键是什么？"></a>Q5 论文中提到的解决方案之关键是什么？</h3><p>解决方案的关键在于每一层与其它所有层之间的密集连接。这种连接方式使得每一层的特征图都作为其后每一层的输入，并且自身的特征图也作为所有后续层的输入。这增强了特征传播和重用，降低了参数数量。</p>
<h3 id="Q6-论文中的实验是如何设计的？"><a href="#Q6-论文中的实验是如何设计的？" class="headerlink" title="Q6 论文中的实验是如何设计的？"></a>Q6 论文中的实验是如何设计的？</h3><p>论文评估了DenseNet在四个竞争激烈的对象识别基准任务（CIFAR-10、CIFAR-100、SVHN和ImageNet）上的性能。实验设计用于比较DenseNet与现有最先进方法的性能，以证明其优越性和效率。</p>
<h3 id="Q7-用于定量评估的数据集是什么？代码有没有开源？"><a href="#Q7-用于定量评估的数据集是什么？代码有没有开源？" class="headerlink" title="Q7 用于定量评估的数据集是什么？代码有没有开源？"></a>Q7 用于定量评估的数据集是什么？代码有没有开源？</h3><p>用于定量评估的数据集包括CIFAR-10、CIFAR-100、SVHN和ImageNet。论文中提到了代码和预训练模型已在GitHub上开源。</p>
<h3 id="Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？"><a href="#Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？" class="headerlink" title="Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？"></a>Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？</h3><p>是的，论文中的实验结果显示DenseNets在多个基准任务上实现了显著改进，证明了其科学假设的有效性，即密集连接可以提高性能并降低计算成本。</p>
<h3 id="Q9-这篇论文到底有什么贡献？"><a href="#Q9-这篇论文到底有什么贡献？" class="headerlink" title="Q9 这篇论文到底有什么贡献？"></a>Q9 这篇论文到底有什么贡献？</h3><p>这篇论文的主要贡献是引入了DenseNet结构，这一新型结构通过密集连接解决了传统卷积神经网络中的一些问题，提高了准确性并降低了训练成本。它为深度学习和计算机视觉领域提供了一种新的研究方向。</p>
<h3 id="Q10-下一步呢？有什么工作可以继续深入？"><a href="#Q10-下一步呢？有什么工作可以继续深入？" class="headerlink" title="Q10 下一步呢？有什么工作可以继续深入？"></a>Q10 下一步呢？有什么工作可以继续深入？</h3><p>下一步的工作可能包括将DenseNet结构应用于更多类型的任务和领域，如自然语言处理、医学图像分析等。此外，也可以探索更多优化DenseNet结构的方法，如更高效的训练策略、更精细的连接模式等，以进一步提高其性能和通用性。</p>
<h3 id="密集连接放到transformer"><a href="#密集连接放到transformer" class="headerlink" title="密集连接放到transformer"></a>密集连接放到transformer</h3><h1 id="论文2-Pruning-Filters-for-Efficient-ConvNets-高效卷积神经网络的滤波器剪枝"><a href="#论文2-Pruning-Filters-for-Efficient-ConvNets-高效卷积神经网络的滤波器剪枝" class="headerlink" title="论文2 Pruning Filters for Efficient ConvNets (高效卷积神经网络的滤波器剪枝)"></a>论文2 Pruning Filters for Efficient ConvNets (高效卷积神经网络的滤波器剪枝)</h1><h1 id="Basic-Information-1"><a href="#Basic-Information-1" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Pruning Filters for Efficient ConvNets (高效卷积神经网络的滤波器剪枝)</li>
<li>Authors: Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf</li>
<li>Affiliation: University of Maryland (马里兰大学), NEC Labs America (NEC美国实验室)</li>
<li>Keywords: CNNs, computation costs, parameter storage costs, pruning filters, convolutional layers, accuracy, acceleration, sparsity, BLAS libraries</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.08710">Paper</a>, [GitHub: None]</li>
<li>16年8月</li>
</ul>
<h2 id="核心思想：用weight值的大小来评判filter的重要性，对于一个filter，对所有weight的绝对值求和-求L1范数-，作为该filter的评价指标，排序结果并将一层中值低的filter裁掉。属于Filters剪枝"><a href="#核心思想：用weight值的大小来评判filter的重要性，对于一个filter，对所有weight的绝对值求和-求L1范数-，作为该filter的评价指标，排序结果并将一层中值低的filter裁掉。属于Filters剪枝" class="headerlink" title="核心思想：用weight值的大小来评判filter的重要性，对于一个filter，对所有weight的绝对值求和(求L1范数)，作为该filter的评价指标，排序结果并将一层中值低的filter裁掉。属于Filters剪枝"></a>核心思想：用weight值的大小来评判filter的重要性，对于一个filter，对所有weight的绝对值求和(求L1范数)，作为该filter的评价指标，排序结果并将一层中值低的filter裁掉。属于Filters剪枝</h2><h1 id="论文简要-1"><a href="#论文简要-1" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>本研究提出了一种用于卷积神经网络（CNNs）的加速方法，通过剪枝对输出准确性影响较小的滤波器，从而显著降低计算成本。</li>
</ul>
<h1 id="背景信息-1"><a href="#背景信息-1" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: CNNs在各种应用中取得了成功，但计算和参数存储成本也显著增加。过去的研究主要集中在减少这些开销，包括剪枝和压缩各层的权重，以减少计算和存储成本，同时保持原始准确性。</li>
<li>过去方案: 过去的剪枝方法主要集中在剪枝权重，但这种方法在卷积层中可能无法充分减少计算成本，因为剪枝网络中存在不规则的稀疏性。此外，这些方法通常需要支持稀疏卷积库，而本研究提出的方法不需要，并且可以与现有的高效BLAS库一起使用。</li>
<li>论文的Motivation: 鉴于过去的剪枝方法存在的问题，本研究提出了一种新的滤波器剪枝方法，通过剪枝对输出准确性影响较小的滤波器，从而显著降低计算成本。与剪枝权重不同，这种方法不会导致稀疏连接模式，因此不需要稀疏卷积库的支持，并且可以与现有的高效BLAS库一起使用。通过实验证明，即使是简单的滤波器剪枝技术也可以在保持准确性的同时，将VGG-16的推理成本降低高达34%，将ResNet-110的推理成本降低高达38%。</li>
</ul>
<h1 id="方法-1"><a href="#方法-1" class="headerlink" title="方法:"></a>方法:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604181.png" alt="003b8469928619fec974c021af486830_2_Figure_1"></p>
<p>1.PRUNING FILTERS AND FEATURE MAPS<br>n表示的是第个卷积层的输入通道数，ni+1表示的是第i+1个filter，(hwi)表示的是输入featuremap的尺寸大小，单个filter(3D)的大<br>小表示为(k，k，ni)，总的filters表示为(ni+1，k，k，ni)，那么可以得到:一个feature map经过总的filters得到输出feature map后，需要进<br>行的计算量是ni<em>ni+1 <em>k</em>k</em>hi+1 <em>Wi+1，对应下图的kernel matrix。<br>那么，根据图片所示，<br>如果剪掉一个 feature map，可以直接减少ni</em>k<em>hi+1</em>Wi+1个运算(总的有ni+1个卷积核，输出通道数少了一个)。<br>输出 feature map 的减少一个，就会导致后面附加移除ni+2<em>k”</em>hi+2*wi+2个运算(输入通道数少一个，共有ni+1个输入通道<br>数)。所以减少m个 feature maps 可以减少m&#x2F;ni+1的计算量。图中的 kernel matrix每一列表示一个3Dfilter，一个3Dfilter会有一个feature map输出。</p>
<ol start="2">
<li>DETERMINING WHICH FILTERS TO PRUNE WITHIN A SINGLE LAYER<br>第i个卷积层剪掉m个卷积核的过程算法流程如下:<br>1.对每个卷积核，计算它的权重绝对值之和s。<br>2.根据s结果排序。<br>3.将m个权重绝对值之和最小的卷积核以及对应的 feature maps 剪掉。下一个卷积层中与剪掉的 feature maps 相关的核也要移除。<br>4.一个对干策i层和第i+1层的新的权重矩阵被创建，并目剩下的权重参数被复制到新模型中。 <img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604492.png" alt="image-20230812124443795"></li>
<li>DETERMINING SINGLE LAYER’S SENSITIVITY TO PRUNING<br>为了弄清楚每层的敏感度，论文对每一层独立剪枝并在验证集上对剪枝后的网络进行评估，Fiqure2(b)展示了结果，很明显斜率比较平缓的层对剪枝的敏感度更高，我们根据经验来决定对每一层的卷积核进行剪枝，对于深度网络(如VGG-16或ResNets)，我们观察到同一stage(相同尺寸的特征图)对应的层对剪枝的敏感度相似，为了避免引入layer-wise meta-parameters，我们对于同一stage的所有层使用相同的剪枝比例。对于那些敏感度高的层，我们剪枝时比例很小，甚至完全不进行剪枝。</li>
<li>Puring filters across multiple layers<br>之前的工作是逐层剪枝，然后重复训练来恢复精度。然而，理解如何能一次性对多层进行剪枝是非常有必要的:<br>1.对于深度网络，逐层剪枝再训练太耗时;<br>2.整体剪枝的方法提供给网络稳健性的一个全面视野，从而导致产生一个更小的网络;<br>3.对于复杂的网络，一个整体的方法很有必要，比如对于ResNet，对恒等映射特征图或者每个残差模块的第二个层剪枝会导致额外层的修剪:为了对多层同时剪枝，我们考虑了两个策略:<br>  4.每一层独立剪枝，即在计算(求权重绝对值之和)时不考虑上一层的修剪情况，所以计算时下图中的黄点仍然参与计算;<br>   5.贪心策略，计算时不计算已经修剪过的，即黄点不参与计算;</li>
</ol>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604736.png" alt="image-20230812124802841"></p>
<h3 id="1-连续层的剪枝："><a href="#1-连续层的剪枝：" class="headerlink" title="1. 连续层的剪枝："></a>1. <strong>连续层的剪枝</strong>：</h3><ul>
<li>这里涉及的是一种剪枝策略，考虑了卷积神经网络中连续层之间的关系。剪枝一个层的滤波器可能会影响下一个层的计算，因为滤波器之间存在依赖关系。</li>
</ul>
<h3 id="2-独立剪枝策略："><a href="#2-独立剪枝策略：" class="headerlink" title="2. 独立剪枝策略："></a>2. <strong>独立剪枝策略</strong>：</h3><ul>
<li>这个策略在计算每个滤波器的重要性时不考虑前一层已经被剪枝的滤波器。</li>
<li>例如，假设我们正在查看三个连续的层，用 ��,��+1,��+2<em>n**i</em>,<em>n**i</em>+1,<em>n**i</em>+2 来表示每层的滤波器数量。在计算第 �+1<em>i</em>+1 层的滤波器重要性时，这个策略不考虑第 �<em>i</em> 层哪些滤波器被剪除（蓝色标记）。</li>
<li>由于不考虑前一层的剪枝，所以某些卷积核权重（黄色标记）仍然会被包括在计算中。</li>
</ul>
<h3 id="3-贪婪剪枝策略："><a href="#3-贪婪剪枝策略：" class="headerlink" title="3. 贪婪剪枝策略："></a>3. <strong>贪婪剪枝策略</strong>：</h3><ul>
<li>与独立剪枝策略不同，贪婪剪枝策略在计算滤波器重要性时会考虑前一层已经剪除的滤波器。</li>
<li>这意味着如果前一层的某个特征映射已经被剪除，那么在下一层的计算中，与该特征映射相对应的卷积核不会被计算。</li>
</ul>
<h3 id="4-结果的卷积核矩阵："><a href="#4-结果的卷积核矩阵：" class="headerlink" title="4. 结果的卷积核矩阵："></a>4. <strong>结果的卷积核矩阵</strong>：</h3><ul>
<li>无论采用哪种策略，剪枝过程都将导致新的卷积核矩阵大小为 (��+1−1)×(��+2−1)(<em>n**i</em>+1−1)×(<em>n**i</em>+2−1)。实测第二种策略精度会更高</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604371.png" alt="image-20230812125105617"></p>
<p>这段话讨论了在卷积神经网络中剪枝残差块的特殊情况。残差块是一种特定的网络结构，它包括一个或多个卷积层和一个称为“快捷路径”或“投影快捷路径”的特殊连接。我们来详细解释这段文字的含义：</p>
<h3 id="1-残差块和投影快捷路径："><a href="#1-残差块和投影快捷路径：" class="headerlink" title="1. 残差块和投影快捷路径："></a>1. <strong>残差块和投影快捷路径</strong>：</h3><ul>
<li>残差块包括一系列卷积层，这些层的输出与输入相加，形成“残差”连接。</li>
<li>投影快捷路径是一种特殊的连接，它可以改变输入的维度以匹配残差块内层的输出维度。通常通过卷积操作实现。</li>
</ul>
<h3 id="2-剪枝残差块的第一层："><a href="#2-剪枝残差块的第一层：" class="headerlink" title="2. 剪枝残差块的第一层："></a>2. <strong>剪枝残差块的第一层</strong>：</h3><ul>
<li>这段文字指出，残差块的第一层可以“无限制”地剪枝。也就是说，您可以自由选择要剪除的滤波器，而不必担心会影响其他部分的网络。</li>
</ul>
<h3 id="3-剪枝残差块的第二层："><a href="#3-剪枝残差块的第二层：" class="headerlink" title="3. 剪枝残差块的第二层："></a>3. <strong>剪枝残差块的第二层</strong>：</h3><ul>
<li><p>残差块的第二层的剪枝与投影快捷路径的剪枝结果有关。</p>
</li>
<li><p>这里的逻辑是，由于投影快捷路径直接与残差块的第二层连接，因此如果投影快捷路径中的某个滤波器被剪除（例如，由于被认为不重要），那么残差块的第二层中与该滤波器相对应的滤波器也应被剪除。</p>
</li>
<li><p>这些要剪除的滤波器被标记为绿色。</p>
</li>
<li><p>a. 理论背景:</p>
<ul>
<li>本文讨论了卷积神经网络（CNNs）的计算和参数存储成本的增加以及需要高效和小型网络尺寸的需求。还强调了减少处理大量图像的网络服务的推理时间的重要性。</li>
</ul>
</li>
<li><p>b. 技术路线:</p>
<ul>
<li>本文提到了减少CNNs中计算成本的模型压缩和技术的先前研究。其中包括权重修剪、低秩逼近、基于FFT的卷积和量化等技术。还提到了其他方法，这些方法专注于去除冗余的特征图或使用稀疏约束训练紧凑的CNNs。</li>
</ul>
</li>
</ul>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812124123590.png" alt="image-20230812124123590"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604674.png" alt="image-20230812124135465"></p>
<ul>
<li>b. 详细的实验结果:<ul>
<li>本文提出的滤波器修剪技术在VGG-16和ResNet-110模型上的性能进行了实验。结果显示，该技术在CIFAR10数据集上可以将推理成本降低高达34%（对于VGG-16）和38%（对于ResNet-110），同时保持接近原始准确性。即使是相对于AlexNet或VGGNet而言参数和推理成本较少的ResNets，也可以在不牺牲太多准确性的情况下实现30%的浮点运算（FLOP）的减少。此外，本文还发现，修剪第一层残差块比修剪第二层更有效地减少总体FLOP。最小滤波器修剪优于随机滤波器修剪和具有最大ℓ1范数的滤波器修剪。最小滤波器修剪还优于使用各种标准（如σmean-mean、σmean-ℓ1、σmean-ℓ2和σvar-ℓ2）的基于激活的特征图修剪。综上所述，本文提出的方法可以在不显著损失准确性的情况下，将VGGNet和深度ResNets的FLOP减少约30%。</li>
</ul>
</li>
</ul>
<h3 id="Q1-论文试图解决什么问题？-1"><a href="#Q1-论文试图解决什么问题？-1" class="headerlink" title="Q1 论文试图解决什么问题？"></a>Q1 论文试图解决什么问题？</h3><p>论文关注卷积神经网络（CNN）的计算和参数存储成本的增加问题。作者提出了一种剪枝方法，通过从CNN中剪除滤波器来加速计算，同时保持原始精度。</p>
<h3 id="Q2-这是否是一个新的问题？-1"><a href="#Q2-这是否是一个新的问题？-1" class="headerlink" title="Q2 这是否是一个新的问题？"></a>Q2 这是否是一个新的问题？</h3><p>该问题不是全新的。在卷积神经网络的训练和部署中，计算和存储效率一直是一个关键问题。过去已有研究人员尝试通过剪枝和压缩权重来减轻这些开销。</p>
<h3 id="Q3-这篇文章要验证一个什么科学假设？-1"><a href="#Q3-这篇文章要验证一个什么科学假设？-1" class="headerlink" title="Q3 这篇文章要验证一个什么科学假设？"></a>Q3 这篇文章要验证一个什么科学假设？</h3><p>文章的科学假设是，与基于权重大小的剪枝方法相比，通过剪除滤波器来减少计算成本更为有效，因为后者可以减少卷积层的计算成本，而不仅仅是全连接层。</p>
<h3 id="Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？-1"><a href="#Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？-1" class="headerlink" title="Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？"></a>Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h3><ul>
<li>早期的工作由Le Cun等人于1989年引入，提出了名为“Optimal Brain Damage”的剪枝方法，采用理论支持的显着性度量来剪枝权重。</li>
<li>Hassibi和Stork于1993年提出了“Optimal Brain Surgeon”，以根据二阶导数信息确定并去除不重要的权重。</li>
<li>Mariet和Sra（2016）通过识别不需要重新训练的神经元的子集来减少网络冗余。但是，这种方法仅在全连接层上操作，并引入稀疏连接。</li>
<li>过去的工作还提出了通过将权重矩阵表示为两个较小矩阵的低秩乘积来近似卷积操作，以降低卷积层的计算成本（参见Denil et al. (2013); Jaderberg et al. (2014); Zhang et al. (2015b;a); Tai et al. (2016); Ioannou et al. (2016)）。</li>
<li>其他减少卷积开销的方法包括使用基于FFT的卷积（Mathieu等人）。</li>
</ul>
<p>这些研究可以归类为：</p>
<ul>
<li>基于理论的权重剪枝方法</li>
<li>网络冗余减少</li>
<li>低秩近似和特殊卷积方法</li>
</ul>
<h3 id="Q5-论文中提到的解决方案之关键是什么？-1"><a href="#Q5-论文中提到的解决方案之关键是什么？-1" class="headerlink" title="Q5 论文中提到的解决方案之关键是什么？"></a>Q5 论文中提到的解决方案之关键是什么？</h3><p>这篇论文提出的解决方案的关键要素包括：</p>
<ol>
<li><strong>剪枝滤波器</strong>：该方法专注于从CNN中剪除对输出精度影响较小的滤波器。</li>
<li><strong>减少计算成本</strong>：通过一起移除整个滤波器及其连接的特征映射，可以显著降低计算成本。</li>
<li><strong>避免稀疏连接</strong>：与剪枝权重不同，此方法不会导致稀疏连接模式。因此，它不需要稀疏卷积库的支持，可以与现有的用于密集矩阵乘法的高效BLAS库一起工作。</li>
<li><strong>简单的剪枝技术</strong>：通过重新训练网络，即使是简单的滤波器剪枝技术也可以在几乎恢复到原始精度的同时，为VGG-16和ResNet-110等模型减少多达34%和38%的推理成本。</li>
</ol>
<p>这些关键要素共同实现了在保持精度的同时提高卷积神经网络的计算效率。</p>
<h3 id="Q6-论文中的实验是如何设计的？-1"><a href="#Q6-论文中的实验是如何设计的？-1" class="headerlink" title="Q6 论文中的实验是如何设计的？"></a>Q6 论文中的实验是如何设计的？</h3><p>实验部分描述了论文中进行的实验设计：</p>
<ol>
<li><strong>剪枝的网络类型</strong>：实验中剪枝了两种类型的网络：简单的CNN（例如VGG-16在CIFAR-10上）和残差网络（例如ResNet-56&#x2F;110在CIFAR-10上，ResNet-34在ImageNet上）。</li>
<li><strong>挑战</strong>：与常用于演示模型压缩的AlexNet或VGG（在ImageNet上）不同，VGG（在CIFAR-10上）和残差网络在全连接层中的参数较少。因此，从这些网络中剪去大量参数是具有挑战性的。</li>
<li><strong>实现</strong>：滤波器剪枝方法在Torch7中实现。剪枝滤波器后，将创建一个具有较少滤波器的新模型，并将修改层的剩余参数以及未受影响层的参数复制到新模型中。</li>
<li><strong>后续处理</strong>：如果剪除了卷积层，则后续批量归一化层的权重也会被移除。</li>
<li><strong>基线精度</strong>：为获得每个网络的基线精度，从头开始训练每个模型，并遵循与ResNet相同的预处理和超参数。</li>
<li><strong>重新训练</strong>：使用恒定学习速率进行重新训练，并减小剪枝后网络的学习速率。</li>
</ol>
<h3 id="Q7-用于定量评估的数据集是什么？代码有没有开源？-1"><a href="#Q7-用于定量评估的数据集是什么？代码有没有开源？-1" class="headerlink" title="Q7 用于定量评估的数据集是什么？代码有没有开源？"></a>Q7 用于定量评估的数据集是什么？代码有没有开源？</h3><p>用于定量评估的数据集包括CIFAR-10和ImageNet。开源</p>
<h3 id="Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？-1"><a href="#Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？-1" class="headerlink" title="Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？"></a>Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？</h3><p>实验结果部分给出了针对不同模型和剪枝方案的详细结果。以下是一些关键发现：</p>
<ul>
<li><strong>VGG-16剪枝</strong>：针对CIFAR-10数据集的VGG-16模型进行剪枝后，可以实现34.2%的FLOP（浮点操作数）剪减和64.0%的参数剪减，同时保持了类似的错误率（6.75%原始错误率与6.60%剪枝后错误率）。</li>
<li><strong>ResNet-56剪枝</strong>：针对CIFAR-10数据集的ResNet-56模型进行剪枝后，可以实现27.6%的FLOP剪减和13.7%的参数剪减，同时保持了相似的错误率。</li>
<li><strong>ResNet-110剪枝</strong>：针对CIFAR-10数据集的ResNet-110模型进行剪枝后，可以实现38.6%的FLOP剪减和32.4%的参数剪减，同时保持了相似的错误率。</li>
<li><strong>ResNet-34剪枝</strong>：针对ImageNet数据集的ResNet-34模型进行剪枝后，可以实现24.2%的FLOP剪减和10.8%的参数剪减，但错误率略有上升。</li>
</ul>
<p>这些实验结果支持了论文的科学假设，表明通过剪除对输出精度影响较小的滤波器，可以显著降低计算成本，同时基本保持原始精度。同时，该方法不导致稀疏连接模式，与现有的高效BLAS库兼容。</p>
<h3 id="Q9-这篇论文到底有什么贡献？-1"><a href="#Q9-这篇论文到底有什么贡献？-1" class="headerlink" title="Q9 这篇论文到底有什么贡献？"></a>Q9 这篇论文到底有什么贡献？</h3><p>这篇论文的主要贡献如下：</p>
<ol>
<li><strong>滤波器剪枝方法</strong>：提出了一种剪除CNN中滤波器的方法，以降低计算和存储成本，而不损失原始精度。</li>
<li><strong>有效的实现</strong>：该方法不引入稀疏连接模式，可以与现有的密集矩阵乘法库一起工作。</li>
<li><strong>广泛适用</strong>：该方法在VGG和ResNet等不同类型的网络上进行了验证。</li>
<li><strong>实验验证</strong>：通过广泛的实验，展示了该方法在不同网络和数据集上的有效性。</li>
</ol>
<h3 id="Q10-下一步呢？有什么工作可以继续深入？-1"><a href="#Q10-下一步呢？有什么工作可以继续深入？-1" class="headerlink" title="Q10 下一步呢？有什么工作可以继续深入？"></a>Q10 下一步呢？有什么工作可以继续深入？</h3><ol>
<li><strong>更复杂的剪枝策略</strong>：探索基于更复杂的标准和方法的剪枝策略，以实现更精确的滤波器选择。</li>
<li><strong>不同类型的网络</strong>：将方法扩展到更多种类和结构的卷积神经网络上。</li>
<li><strong>实时和嵌入式应用</strong>：针对实时和嵌入式应用的特定需求，优化和调整剪枝方法。</li>
<li><strong>与其他压缩技术结合</strong>：探索将滤波器剪枝与其他模型压缩和优化技术结合的可能性，以实现更全面的优化。</li>
</ol>
<h1 id="论文3：Filter-Pruning-via-Geometric-Median-for-Deep-Convolutional-Neural-Networks-Acceleration-基于几何中位数的深度卷积神经网络加速的滤波器剪枝"><a href="#论文3：Filter-Pruning-via-Geometric-Median-for-Deep-Convolutional-Neural-Networks-Acceleration-基于几何中位数的深度卷积神经网络加速的滤波器剪枝" class="headerlink" title="论文3：Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration (基于几何中位数的深度卷积神经网络加速的滤波器剪枝)"></a>论文3：Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration (基于几何中位数的深度卷积神经网络加速的滤波器剪枝)</h1><h1 id="Basic-Information-2"><a href="#Basic-Information-2" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration (基于几何中位数的深度卷积神经网络加速的滤波器剪枝)</li>
<li>Authors: Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, Yi Yang</li>
<li>Affiliation: CAI, University of Technology Sydney (悉尼科技大学)</li>
<li>Keywords: filter pruning, deep convolutional neural networks, acceleration, geometric median</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.00250">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/he-y/filter-pruning-geometric-median">GitHub</a></li>
<li>18年11月</li>
</ul>
<h1 id="论文简要-2"><a href="#论文简要-2" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125926446.png" alt="image-20230812125926446"></p>
<ul>
<li>上图1是典型的滤波器剪枝的方法。现有的方法大都是基于范数的标准进行剪枝。小范数不重要，认为滤波器的范数(p-norm)越小，携带的信息越少，于是对网络贡献越小，那么这些滤波器可以去掉而不会严重影响网络的性能。于是我们根据滤波器的p-norm进行重要性排序，规定一个阈值，并且删除p-norm小于阈值的滤波器。</li>
<li>这个浅蓝色区域是一种理想的滤波器范数分布，而基于这种范数标准的剪枝实际隐含了两个条件</li>
<li>:一是范数值的标准差尽可能的大，也就是分布相对稀疏，这样的目的是可以更容易的找到一个成值进行剪枝;</li>
<li>第二个条件是小范数的值应该尽可能的小，理想状况下是趋近于0，这样能保证小范数的滤波器对网络的贡献很小，去掉他对模型的影响不大。</li>
<li>本文提出了一种基于几何中位数的滤波器剪枝方法，用于压缩深度卷积神经网络模型，无需满足之前方法中的两个要求，通过在两个图像分类基准上的实验证明了其有效性和优势。</li>
</ul>
<h1 id="背景信息-2"><a href="#背景信息-2" class="headerlink" title="背景信息:"></a>背景信息:</h1><p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812130521561.png" alt="image-20230812130521561"></p>
<p>如这两幅图所示，浅蓝色区域仍是理想情况下的范数分布，绿色区域是实际的滤波器范数分布。a名我们看到绿色区域这种情况下的分布范数的标准差太小，这样出现的问题是很多滤波器有相似的重要性，很难确定一个合适的阈值(A smallnorm deviation leads to a small search space, which makes it diffcult to find an appropriate threshold to select fifilters toprune.)</p>
<p>b图我们看到最小范数的值仍然很大，也就是说他对网络的贡献度不能被忽视。</p>
<p>作者为了证明在有些场景中上面提到的两个条件并不能很好的满足，做了非常充分的实验证明，分别用resnet110在cifar10和resnet18在ILSVRC2012数据集上进行实验:</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812130609383.png" alt="image-20230812130609383"></p>
<p>绿色小竖线表示范数的观察值，蓝色线表示核的分布预测。b图我们看到在resnet110的第一层有大量的滤波器，它们的范数集中在10x6的量级上。c图展示滤波器的范数范围跨度只有0.3，远远小于a图第一层范数1.7的跨度，这说明同一个网络不同层的滤波器范数有的标差较大，有的就偏小。同样的看g图，resnet18最后一层的滤波器最小范数大约是0.8，与e图展示的第一层比起来并没有接近0，通过分析实际的预训练网络，发现这些网络确实出现了上述的情况。</p>
<ul>
<li>论文背景: 深度卷积神经网络的深度和宽度使得计算成本昂贵，难以在移动设备上部署。因此，有必要降低计算成本但保持高准确性的深度卷积神经网络模型。</li>
<li>过去方案: 过去的滤波器剪枝方法通常使用“较小范数-较不重要”准则来剪枝具有较小范数值的滤波器。然而，这种准则的有效性取决于两个要求：滤波器范数的偏差应该很大，滤波器的最小范数应该很小。然而，这两个要求并不总是满足的。</li>
<li>论文的Motivation: 鉴于过去方法的局限性，本文提出了一种新的滤波器剪枝方法，名为Filter Pruning via Geometric Median (FPGM)，通过剪枝具有冗余信息的滤波器来压缩CNN模型。与之前的方法不同，FPGM选择具有最可替代贡献的滤波器进行剪枝，而不是剪枝相对不重要的滤波器。通过计算同一层中滤波器的几何中位数，FPGM可以用其余滤波器来表示附近的滤波器，因此剪枝这些滤波器不会对模型性能产生重大负面影响。与基于范数的准则不同，FPGM的性能不会因为无法满足范数准则的要求而下降。</li>
</ul>
<h1 id="方法-2"><a href="#方法-2" class="headerlink" title="方法:"></a>方法:</h1><h3 id="基于几何中位数的滤波器剪枝方法"><a href="#基于几何中位数的滤波器剪枝方法" class="headerlink" title="基于几何中位数的滤波器剪枝方法"></a>基于几何中位数的滤波器剪枝方法</h3><p>作者摒弃了这种基于范数标准剪枝的方法，提出了一种基于几何中心的滤波器评价指标。在某一层中，从全局考虑所有filter的关系。<br>这种方法的提出灵感来源于几何中心的思想。什么是几何中心?就是在一个d维的空间中，给定一个点集，a1.a2.a3.….an，在该空间中找到一个点x*，使得该点到各个点的距离和最小，就是这个d维空间的几何中心。</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131322763.png" alt="image-20230812131322763"></p>
<p>基本思想:几何中心是对于欧几里得空间的点的中心的一个估计。受此启发，作者把滤波器抽象成欧氏空间中的点，对于网络中的每一层，在该层的滤波器空间中，计算GM，也就是第1层的数据中心。如果某个滤波器接近于这个GM，可以认为这个源波器的信息跟其他源波器重合，甚至是几余的，于是可以去掉这个源波器而不对网络产生大的影响。去掉它后它的功能可以被其他滤波器代替。沿用求取普通几何中心的思想，</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131413844.png" alt="image-20230812131413844"></p>
<p>归根结底，作者是想在某一层中找到与几何中心xGM相近的滤波器，然后将他们剪枝。为了方便求解，作者假设第层的几何中心就位于第层的滤波器中，因为真实目的是要找到与几何中心最近的滤波器。此时(2)的优化问题就变为，从第层中所有的滤波器中寻找一个filter,确保其与剩余的filters的距离之和最小。</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131533751.png" alt="image-20230812131533751"></p>
<p>公式5-9看不懂</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131557560.png" alt="image-20230812131557560"></p>
<p>算法流程如下：</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131517584.png" alt="image-20230812131517584"></p>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ul>
<li><strong>训练数据</strong>：( X )。</li>
<li><strong>剪枝率</strong>：( P_i )。剪枝率确定了每一层应剪去的滤波器数量的百分比。</li>
</ul>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><ol>
<li><strong>模型参数初始化</strong>：模型的权重参数被初始化为( W &#x3D; {W(i), 0 \leq i \leq L} )，其中( L )是网络的层数。</li>
</ol>
<h3 id="训练和剪枝过程"><a href="#训练和剪枝过程" class="headerlink" title="训练和剪枝过程"></a>训练和剪枝过程</h3><ol start="2">
<li><strong>开始训练周期</strong>：对于每个训练周期（从1到最大训练周期），执行以下步骤：<ul>
<li><strong>更新模型参数</strong>：根据训练数据( X )更新模型参数( W )。这一步通常涉及正向传播、损失计算、反向传播和权重更新。</li>
<li><strong>逐层剪枝</strong>：对于每一层（从1到( L )），执行以下剪枝步骤：<ul>
<li><strong>找到要剪去的滤波器</strong>：找到满足方程4的( N_{i+1} \times P_i )个滤波器。方程4通常涉及使用几何中位数来确定要剪去的滤波器。</li>
<li><strong>零化选定的滤波器</strong>：将选定的滤波器设置为零，从而实现剪枝。</li>
</ul>
</li>
<li><strong>重复以上步骤</strong>：继续训练和剪枝，直到达到最大训练周期。</li>
</ul>
</li>
</ol>
<h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><ol start="3">
<li><strong>获取紧凑模型</strong>：从最终的权重参数( W )中获取紧凑模型( W^* )。紧凑模型不包括已剪去（零化）的滤波器。</li>
</ol>
<h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><ul>
<li><strong>输出紧凑模型及其参数</strong>：最终剪枝后的模型及其参数( W^* )。</li>
</ul>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>基于几何中位数的滤波器剪枝（FPGM）是一个迭代过程，其中训练和剪枝在每个训练周期内交替进行。通过这种方式，算法可以在训练过程中逐渐剪去滤波器，从而在保持网络性能的同时减少模型大小。最终的紧凑模型包括剪枝后保留的所有滤波器和参数。</p>
<ul>
<li>a. 理论背景:<ul>
<li>本文提出了一种新的滤波器修剪方法，称为几何中位数滤波器修剪（FPGM），用于压缩深度卷积神经网络模型。与以往基于范数值修剪滤波器的方法不同，FPGM通过修剪冗余的滤波器来实现。在两个图像分类基准测试上的实验结果表明，FPGM在最小精度损失的情况下实现了显著的模型压缩。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>本文提出的FPGM方法通过计算同一层内滤波器的几何中位数（GM），并修剪与几何中位数最远的滤波器。与以往使用范数值作为标准的方法不同，FPGM不依赖于滤波器范数来选择修剪的滤波器。</li>
</ul>
</li>
</ul>
<h1 id="结果-2"><a href="#结果-2" class="headerlink" title="结果:"></a>结果:</h1><p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125737860.png" alt="image-20230812125737860"></p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125805978.png" alt="image-20230812125805978"></p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125815070.png" alt="image-20230812125815070"></p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>本文在两个基准测试（CIFAR-10和ILSVRC-2012）上评估了FPGM算法。CIFAR-10数据集包含10个不同类别的60,000个32x32彩色图像，其中包括50,000个训练图像和10,000个测试图像。ILSVRC-2012数据集是一个大规模数据集，包含1.28百万个训练图像和50,000个验证图像，共有1,000个类别。实验设置包括指定两个数据集的训练设置，包括参数设置和训练计划。修剪设置涉及为每个层设置修剪率，确定要修剪的滤波器数量。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>实验结果显示了FPGM的有效性和效率。在应用于CIFAR-10和ILSVRC-2012两个图像分类基准测试时，FPGM在显著减少模型FLOPs的同时，几乎没有损失精度。例如，在CIFAR-10上，FPGM在ResNet-110上减少了超过52%的FLOPs，并且相对精度提高了2.69%。在ILSVRC-2012上，FPGM在ResNet-101上减少了超过42%的FLOPs，而top-5精度没有下降。</li>
</ul>
</li>
</ul>
<h3 id="Q1-论文试图解决什么问题？-2"><a href="#Q1-论文试图解决什么问题？-2" class="headerlink" title="Q1 论文试图解决什么问题？"></a>Q1 论文试图解决什么问题？</h3><p>这篇论文试图解决深度卷积神经网络（CNN）的滤波器剪枝问题。以前的工作使用了“小范数-次要重要”的准则来剪去具有较小范数值的滤波器。本文分析了基于范数的准则，并指出其有效性取决于两个并不总是满足的要求：滤波器的范数偏差应该大，滤波器的最小范数应该小。为了解决这个问题，论文提出了一种名为“基于几何中位数的滤波器剪枝”（FPGM）的新方法。</p>
<h3 id="Q2-这是否是一个新的问题？-2"><a href="#Q2-这是否是一个新的问题？-2" class="headerlink" title="Q2 这是否是一个新的问题？"></a>Q2 这是否是一个新的问题？</h3><p>滤波器剪枝并不是一个全新的问题，已有许多工作致力于深度神经网络的压缩和加速。然而，本文提出了一种新的剪枝方法，并分析了以前基于范数的剪枝准则的局限性。</p>
<h3 id="Q3-这篇文章要验证一个什么科学假设？-2"><a href="#Q3-这篇文章要验证一个什么科学假设？-2" class="headerlink" title="Q3 这篇文章要验证一个什么科学假设？"></a>Q3 这篇文章要验证一个什么科学假设？</h3><p>这篇文章的科学假设是，通过使用几何中位数作为剪枝准则，可以剪去冗余的滤波器，从而压缩CNN模型，无论滤波器的范数偏差大小或滤波器的最小范数大小如何。这种方法的目的是克服基于范数的剪枝方法的局限性。</p>
<h3 id="Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？-2"><a href="#Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？-2" class="headerlink" title="Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？"></a>Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h3><p>相关的研究主要集中在加速卷积神经网络（CNN）的方法上，可以大致分为四个类别：</p>
<ol>
<li><strong>矩阵分解</strong>：使用数学方法分解矩阵以降低计算复杂度。</li>
<li><strong>低精度权重</strong>：使用低精度数值表示权重以减少存储和计算需求。</li>
<li><strong>知识蒸馏</strong>：通过训练较小的模型来模仿大模型的行为。</li>
<li><strong>剪枝</strong>：移除神经网络中不必要的连接。剪枝方法可以进一步分为权重剪枝和滤波器剪枝。其中，滤波器剪枝不仅减少了设备上的存储使用，还降低了计算成本以加速推理。</li>
</ol>
<h3 id="Q5-论文中提到的解决方案之关键是什么？-2"><a href="#Q5-论文中提到的解决方案之关键是什么？-2" class="headerlink" title="Q5 论文中提到的解决方案之关键是什么？"></a>Q5 论文中提到的解决方案之关键是什么？</h3><p>论文提出了一种名为“基于几何中位数的滤波器剪枝”（FPGM）的新方法。关键之处在于：</p>
<ul>
<li><strong>基于几何中位数</strong>：使用几何中位数作为剪枝准则，可以剪去冗余的滤波器。</li>
<li><strong>克服范数局限性</strong>：与基于范数的方法不同，FPGM无需满足滤波器的范数偏差大或最小范数小的要求。</li>
<li><strong>压缩和加速</strong>：该方法旨在压缩CNN模型并加速推理过程。</li>
</ul>
<h3 id="Q6-论文中的实验是如何设计的？-2"><a href="#Q6-论文中的实验是如何设计的？-2" class="headerlink" title="Q6 论文中的实验是如何设计的？"></a>Q6 论文中的实验是如何设计的？</h3><p>论文的实验部分评估了基于几何中位数的滤波器剪枝（FPGM）方法在不同类型的网络和数据集上的效果。以下是实验设计的关键部分：</p>
<ul>
<li><p><strong>网络结构</strong>：对单分支网络（VGGNet）和多分支网络（ResNet）进行了评估。</p>
</li>
<li><p><strong>数据集</strong>：实验在两个基准数据集上进行，即CIFAR-10和ILSVRC-2012。</p>
</li>
<li><p>训练设置</p>
<p>：</p>
<ul>
<li>在CIFAR-10上，参数设置与先前的工作相同。</li>
<li>在ILSVRC-2012上，使用默认参数设置，并与PyTorch官方示例中的数据增强策略相同。</li>
</ul>
</li>
<li><p><strong>剪枝设置</strong>：分析了从头开始训练的模型与预训练模型的差异。对于从头开始的模型，使用正常的训练计划，无需额外的微调。</p>
</li>
</ul>
<h3 id="Q7-用于定量评估的数据集是什么？代码有没有开源？-2"><a href="#Q7-用于定量评估的数据集是什么？代码有没有开源？-2" class="headerlink" title="Q7 用于定量评估的数据集是什么？代码有没有开源？"></a>Q7 用于定量评估的数据集是什么？代码有没有开源？</h3><p>用于定量评估的数据集是CIFAR-10和ILSVRC-2012。开源</p>
<h3 id="Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？-2"><a href="#Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？-2" class="headerlink" title="Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？"></a>Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？</h3><p>成功评估基于几何中位数的滤波器剪枝方法的有效性和效率</p>
<h3 id="Q9-这篇论文到底有什么贡献？-2"><a href="#Q9-这篇论文到底有什么贡献？-2" class="headerlink" title="Q9 这篇论文到底有什么贡献？"></a>Q9 这篇论文到底有什么贡献？</h3><p>这篇论文的主要贡献如下：</p>
<ol>
<li><p><strong>分析现有方法的局限性</strong>：分析了基于范数的剪枝准则的局限性，并提出了两个不总是满足的要求。</p>
</li>
<li><p><strong>提出新的剪枝方法</strong>：介绍了一种新的滤波器剪枝方法，即基于几何中位数的滤波器剪枝（FPGM），旨在压缩CNN模型并加速推理。</p>
</li>
<li><p><strong>实验验证</strong>：通过在多个网络结构和数据集上的实验，评估了所提方法的有效性和效率。</p>
<h3 id="Q10-下一步呢？有什么工作可以继续深入？-2"><a href="#Q10-下一步呢？有什么工作可以继续深入？-2" class="headerlink" title="Q10 下一步呢？有什么工作可以继续深入？"></a>Q10 下一步呢？有什么工作可以继续深入？</h3><ol>
<li><strong>扩展到其他网络结构</strong>：将FPGM方法扩展到其他复杂的网络结构，并评估其在不同任务和领域中的适用性。</li>
<li><strong>与其他压缩技术的结合</strong>：探讨如何将FPGM与其他模型压缩和加速技术（如量化、知识蒸馏等）结合使用。</li>
</ol>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3/" data-id="cllkmcmf5000gksu7f0li9ao8" data-title="8月第二周剪枝相关" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AF%86%E9%9B%86%E8%BF%9E%E6%8E%A5%E3%80%81%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%89%AA%E6%9E%9D/" rel="tag">密集连接、滤波器剪枝</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%89%E5%91%A8/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          8月第三周HomoDistil
        
      </div>
    </a>
  
  
    <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%92%B8%E9%A6%8F/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">8月第一周自然语言大模型蒸馏</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/QLORA-%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/" rel="tag">QLORA 剪枝 lomo全参数微调</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dbgpt-kagnet/" rel="tag">dbgpt kagnet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BD%8D%E7%BD%AE%E6%8F%92%E5%80%BC-yarn%E6%8F%92%E5%80%BC/" rel="tag">位置插值&#x2F;yarn插值</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%89%AA%E6%9E%9D-%E8%92%B8%E9%A6%8F/" rel="tag">剪枝+蒸馏</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E9%9B%86%E8%BF%9E%E6%8E%A5%E3%80%81%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%89%AA%E6%9E%9D/" rel="tag">密集连接、滤波器剪枝</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%8Cpromot%EF%BC%8Clora/" rel="tag">线性注意力，promot，lora</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B7%A8%E8%AF%AD%E8%A8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" rel="tag">跨语言多模态知识蒸馏</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" rel="tag">通用剪枝</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/QLORA-%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/" style="font-size: 10px;">QLORA 剪枝 lomo全参数微调</a> <a href="/tags/dbgpt-kagnet/" style="font-size: 10px;">dbgpt kagnet</a> <a href="/tags/%E4%BD%8D%E7%BD%AE%E6%8F%92%E5%80%BC-yarn%E6%8F%92%E5%80%BC/" style="font-size: 10px;">位置插值/yarn插值</a> <a href="/tags/%E5%89%AA%E6%9E%9D-%E8%92%B8%E9%A6%8F/" style="font-size: 10px;">剪枝+蒸馏</a> <a href="/tags/%E5%AF%86%E9%9B%86%E8%BF%9E%E6%8E%A5%E3%80%81%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%89%AA%E6%9E%9D/" style="font-size: 10px;">密集连接、滤波器剪枝</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%8Cpromot%EF%BC%8Clora/" style="font-size: 10px;">线性注意力，promot，lora</a> <a href="/tags/%E8%B7%A8%E8%AF%AD%E8%A8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" style="font-size: 10px;">跨语言多模态知识蒸馏</a> <a href="/tags/%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" style="font-size: 10px;">通用剪枝</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/10/08/10%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8chatdb/">10月第一周chatdb</a>
          </li>
        
          <li>
            <a href="/2023/10/08/9%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%91%A8%E6%8A%A5/">9月第二周周报</a>
          </li>
        
          <li>
            <a href="/2023/10/08/9%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E4%BD%8D%E7%BD%AE%E6%8F%92%E5%80%BC/">9月第一周</a>
          </li>
        
          <li>
            <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8cvil%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">7月第二周cvil多模态知识蒸馏</a>
          </li>
        
          <li>
            <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%89%E5%91%A8/">8月第三周HomoDistil</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>