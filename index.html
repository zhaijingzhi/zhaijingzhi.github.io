<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-7月第二周cvil多模态知识蒸馏" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8cvil%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T08:16:36.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8cvil%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">7月第二周cvil多模态知识蒸馏</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="7月第二周cViL跨语言多模态知识蒸馏-论文1：cViL-Cross-Lingual-Training-of-Vision-Language"><a href="#7月第二周cViL跨语言多模态知识蒸馏-论文1：cViL-Cross-Lingual-Training-of-Vision-Language" class="headerlink" title="7月第二周cViL跨语言多模态知识蒸馏 论文1：cViL: Cross-Lingual Training of Vision-Language"></a>7月第二周cViL跨语言多模态知识蒸馏 论文1：cViL: Cross-Lingual Training of Vision-Language</h1><p>Models using Knowledge Distillation</p>
<h2 id="自然语言蒸馏"><a href="#自然语言蒸馏" class="headerlink" title="??自然语言蒸馏"></a>??自然语言蒸馏</h2><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: cViL: Cross-Lingual Training of Vision-Language Models using Knowledge Distillation (cViL: 跨语言视觉-语言模型的知识蒸馏训练)</li>
<li>Authors: Kshitij Gupta, Devansh Gautam, Radhika Mamidi</li>
<li>Affiliation: IIIT Hyderabad (印度国际信息技术研究所)</li>
<li>Keywords: Vision-Language Models, Cross-Lingual Training, Knowledge Distillation</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.03354v2">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/kshitij98/cViL">GitHub</a></li>
</ul>
<h1 id="论文简要"><a href="#论文简要" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>本文提出了一种使用知识蒸馏的方法，通过跨语言训练英语视觉-语言模型来训练目标语言的单语模型，并在日语和印地语的视觉问答任务中取得了相对提升4.4%和13.4%的最新成果。</li>
</ul>
<h1 id="创新点总结："><a href="#创新点总结：" class="headerlink" title="创新点总结："></a>创新点总结：</h1><blockquote>
<ol>
<li><p><strong>使用mBERT初始化模型和分词器</strong>：作者选择了mBERT而非传统的BERT来初始化模型和分词器。mBERT是一个多语言版本的BERT，能处理多种语言，这使得模型有能力从一开始就处理多语言数据。</p>
</li>
<li><p><strong>数据增强的应用</strong>：作者通过将大量的英语数据集翻译为其他语言，从而扩大了非英语语言的数据集。这有助于改善数据稀缺问题，提高模型的性能。</p>
</li>
<li><p><strong>多方面的知识蒸馏</strong>：作者提出了一套全面的知识蒸馏方案，包括CLS标记蒸馏、图像标记蒸馏、对象标签蒸馏、混合代码蒸馏和中间层蒸馏。这些不同的蒸馏方法都试图最小化教师模型和学生模型在相应部分的差异，从而更好地将教师模型的知识迁移到学生模型中。</p>
</li>
<li><p><strong>引入混合代码蒸馏</strong>：作者提出了一个创新的混合代码蒸馏方法，通过生成具有上下文信息的双语词对齐，然后在目标语言中随机替换一部分词为英语对齐词，创建了混合语言的数据集。</p>
</li>
<li><p><strong>中间层蒸馏</strong>：作者试验了在模型的中间层应用蒸馏损失，这是一种新的尝试，可以帮助模型更好地理解语言。</p>
</li>
</ol>
</blockquote>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 视觉-语言任务在研究界越来越受关注，但主要集中在英语上。本文旨在利用仅使用英语的视觉-语言模型来训练目标语言的单语模型。</li>
<li>过去方案: 过去的方法主要是通过在预训练数据中使用多种语言来训练多语言模型，但初始训练成本高，不适合训练多种语言的多种架构的多语言模型。</li>
<li>论文的Motivation: 为了解决这些问题，本文提出了一种直接在目标语言中使用英语模型作为监督来训练单语多模态模型的方法，并通过知识蒸馏技术将高资源英语模型的知识转移到其他语言，从而减少资源消耗。</li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307151433262.png" alt="image-20230715143350223" style="zoom:150%;" />

<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307151639382.png" alt="image-20230715163918321"></p>
<ol>
<li><p><strong>CLS标记蒸馏</strong>：公式为 </p>
<p>$$<br>L_{CLS} &#x3D; MSE(C_S, C_T)<br>$$</p>
<p>这里的 (C_S) 和 (C_T) 分别是学生网络和教师网络的CLS标记嵌入。MSE() 是均方误差损失函数，它计算的是学生网络和教师网络的CLS标记嵌入之间的均方误差。</p>
</li>
<li><p>？？什么技巧，具体怎么用 <strong>图像标记蒸馏</strong>：公式为 </p>
<p>$$L_{img} &#x3D; \frac{1}{p}\sum_{i&#x3D;1}^{p}MSE(I_{S_i}, I_{T_i})$$</p>
<p>这里的 (I_{S_i}) 和 (I_{T_i}) 分别是学生网络和教师网络的第i个图像标记嵌入，(p) 是输入中图像标记的数量。</p>
</li>
<li><p><strong>对象标签蒸馏</strong>：公式为 </p>
</li>
<li><p>$$<br>L_{tag} &#x3D; \frac{1}{t^2}\sum_{i&#x3D;1}^{t}\sum_{j&#x3D;1}^{t}A_{ij}MSE(O_{S_i}, O_{T_j})<br>$$</p>
<p>这里的 (O_{S_i}) 和 (O_{T_j}) 分别是学生网络和教师网络的第i个和第j个对象标签嵌入，(A_{ij}) 是一个二值矩阵，表示第i个和第j个对象标签是否匹配和对齐，(t) 是输入中对象标签的数量。</p>
</li>
<li><p><strong>混合代码蒸馏</strong>：公式为<br>$$<br>L_{CM} &#x3D; \frac{1}{n^2}\sum_{i&#x3D;1}^{n}\sum_{j&#x3D;1}^{n}B_{ij}MSE(H_{S_i}, H_{T_j})<br>$$</p>
<p>这里的 (H_{S_i}) 和 (H_{T_j}) 分别是学生网络和教师网络的第i个和第j个文本标记嵌入，(B_{ij}) 是一个二值矩阵，表示第i个和第j个文本标记是否匹配和对齐，(n) 是输入中文本标记的数量。</p>
</li>
<li><p><strong>中间层蒸馏</strong>：对于选定的每一层 (m)，公式为<br>$$<br>L_{distil} &#x3D; \sum_{m \in L}\lambda_m(L_{CLS} + L_{img} + L_{tag} + L_{CM})<br>$$</p>
<p>这里的 (L) 是需要应用损失的层的集合，(\lambda_m) 是第m层的权重，(L_{CLS})、(L_{img})、(L_{tag}) 和 (L_{CM}) 分别是CLS标记蒸馏、图像标记蒸馏、对象标签蒸馏和混合代码蒸馏的损失。</p>
</li>
</ol>
<ul>
<li>a. 理论背景:<ul>
<li>本文提出了一种在目标语言中使用仅英语模型训练单语视觉语言模型的流程。作者扩展了OSCAR+模型，该模型使用对象标签进行图像-文本对齐，并在不同语言的视觉问答数据集上进行训练。作者引入了一种新颖的知识蒸馏方法，使用平行句子将知识从英语模型转移到资源较少的目标语言模型。作者还发布了一个日语和印地语的视觉问答数据集。该流程在日语和印地语的准确性上分别比最先进的模型提高了4.4%和13.4%。模型架构基于OSCAR方法，并使用mBERT处理非英语语言。通过将大型英语数据集进行翻译来进行数据增强，以减轻数据稀缺和语言偏差。知识蒸馏过程旨在将图像-问题对映射到跨语言的语义空间中的相同位置。知识蒸馏的目标函数被最小化以训练学生模型。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>该方法涉及使用知识蒸馏技术训练学生模型。实验中使用的教师模型是预训练于OSCAR+语料库并针对英语视觉问答进行微调的OSCAR+B检查点。学生模型使用不同类型的蒸馏，包括分类令牌蒸馏、图像令牌蒸馏、对象标签蒸馏、混合代码蒸馏和中间层蒸馏。<strong>每种类型的蒸馏都涉及最小化特定的目标函数，以对齐学生模型和教师模型的嵌入。</strong>蒸馏损失应用于学生模型的一部分层。提出了两种训练方法：CVILAUG，涉及数据增强技术；CVILKD，涉及知识蒸馏技术。模型在英语的任务特定数据集上进行训练，然后翻译到目标语言进行评估。</li>
</ul>
</li>
</ul>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><ul>
<li><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307151442421.png" alt="image-20230715144236380"></p>
</li>
<li><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307151446190.png" alt="image-20230715144639144"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307151443893.png" alt="image-20230715144336857"></p>
</li>
<li><blockquote>
<ol>
<li><strong>日语视觉问题回答</strong>：作者的模型（CVILAUG和CVILKD）在日语Visual Genome VQA数据集上的性能超过了当前的最佳模型，包括PCATT、UNITERCC和UC2。作者还注意到，在覆盖数据集的情况下，模型的正确分类率为48.02%，但由于数据集覆盖率只有74.39%，最终的准确率降低到了35.72%。</li>
<li><strong>印地语视觉问题回答</strong>：作者的模型（CVILKD）在印地语VQA v2.0数据集上的性能显著优于没有使用知识蒸馏的基线模型，准确率相对提高了7.3%。在印地语VQA v1.0数据集上，作者的系统也显著优于当前的最佳模型，包括BUTD、Bi-linear Attention和Multimodal Fusion。</li>
<li><strong>训练成本</strong>：作者的模型能够在17小时内使用知识蒸馏从零开始训练，这明显比预训练步骤需要的资源要少。</li>
<li><strong>消融研究</strong>：通过移除每一个蒸馏目标并比较模型在日语Visual Genome VQA数据集上的性能，作者发现所有的蒸馏目标都对系统的性能有所贡献。中间层蒸馏对系统的性能影响最大，而对象标签蒸馏也起到了重要的作用。</li>
</ol>
</blockquote>
</li>
<li><p>. 详细的实验设置:</p>
<ul>
<li>模型在4个Nvidia GeForce RTX 2080 Ti GPU上进行训练，每个GPU的批量大小为32个问题-答案对。使用了具体参数的AdamW优化器，以及dropout和attention dropout。在将文本输入模型之前，将其转换为小写。采用了混合精度训练以提高效率。对于CVILAUG和CVILKD两个系统，详细描述了训练方法。</li>
</ul>
</li>
<li><p>b. 详细的实验结果:</p>
<ul>
<li>研究的主要结果是对系统在最先进模型上的性能进行比较。模型在日语和印地语视觉问答数据集上的准确性优于基线，并取得了比以前模型更好的准确性。还讨论了系统在不同问题类型上的性能。通过t-SNE可视化了图像-问题对的学习特征。比较了模型生成的图像和单词令牌的嵌入，显示嵌入对齐良好，并形成了每个对象类别的可区分聚类。进行了消融研究，以调查不同蒸馏目标的贡献。移除了每个蒸馏目标，并比较了系统在日语视觉基因组数据集的测试集上的性能。根据提供的表格，不同模型在VQA数据集上的准确性显示。具有所有蒸馏目标的CVILKD模型达到了最高的35.72的准确性。消融研究表明，每个蒸馏目标对系统的性能都很重要。最关键的蒸馏目标是中间层蒸馏，其次是对象标签蒸馏。基线模型的准确性为33.75。</li>
</ul>
</li>
</ul>
<h1 id="论文2：MMLU-Measuring-massive-multitask-language-understanding-in-Chinese"><a href="#论文2：MMLU-Measuring-massive-multitask-language-understanding-in-Chinese" class="headerlink" title="论文2：MMLU: Measuring massive multitask language understanding in Chinese"></a>论文2：MMLU: Measuring massive multitask language understanding in Chinese</h1><h1 id="Basic-Information-1"><a href="#Basic-Information-1" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: CMMLU: Measuring massive multitask language understanding in Chinese (CMMLU：测量中文大规模多任务语言理解)</li>
<li>Authors: Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin</li>
<li>Affiliation: MBZUAI (Mohamed bin Zayed University of Artificial Intelligence) (MBZUAI)</li>
<li>Keywords: large language models, Chinese benchmark, language understanding, performance evaluation, knowledge and reasoning abilities</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.09212v1">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/haonan-li/CMMLU">GitHub</a></li>
</ul>
<h1 id="论文简要-1"><a href="#论文简要-1" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>本文介绍了CMMLU，这是一个全面的中文评估基准，旨在评估大规模语言模型在不同主题和环境下的性能。通过对18个先进的多语言和中文导向的语言模型进行评估，发现大多数模型在给定上下文示例和思维链提示的情况下，难以达到50%的平均准确率，而随机基准为25%。此外，通过广泛的实验，还分析了影响模型性能的因素，并提出了增强语言模型的方向。</li>
</ul>
<h1 id="背景信息-1"><a href="#背景信息-1" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 随着大型语言模型（LLMs）的能力不断提升，评估其性能变得越来越重要和具有挑战性。然而，评估这些模型中编码的知识和推理能力变得越来越具有挑战性，尤其是对于生成流畅和合理回答的LLMs的普及。</li>
<li>过去方案: 为了解决这个问题，研究人员从不同的角度创建了各种基准，如MMLU和CLUE。然而，这些基准主要针对英语，限制了对其他语言的LLMs进行评估的能力。</li>
<li>论文的Motivation: 鉴于中文是全球使用人数最多的语言，为了评估中文语言模型，需要提出一个全面的中文评估套件。本文的动机就是设计和介绍了CMMLU，这是一个专门针对中文语言和文化背景的综合评估套件，旨在评估LLMs的高级知识和推理能力。CMMLU涵盖了广泛的主题，包括自然科学、社会科学、工程和人文学科，并通过对ChatGPT和其他先进的中文导向LLMs进行评估，揭示了LLMs在中文知识和语言理解方面的改进空间。</li>
</ul>
<h1 id="方法-1"><a href="#方法-1" class="headerlink" title="方法:"></a>方法:</h1><ul>
<li>a. 理论背景:<ul>
<li>本文介绍了CMMLU，这是一个综合性的中文基准，用于评估大型语言模型（LLMs）在各个学科中的性能。评估了18个先进的多语言和中文导向的LLMs，结果显示大多数模型难以达到50%的平均准确率，表明LLMs需要改进。本文还进行了实验，以确定影响模型性能的因素，并提出了增强LLMs的方向。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>本文提出了CMMLU作为一个综合性的中文评估套件，涵盖了广泛的学科，并在中文语言和文化背景下评估LLMs。文中强调了LLMs在中文知识和语言理解方面的改进空间。</li>
</ul>
</li>
</ul>
<h1 id="结果-1"><a href="#结果-1" class="headerlink" title="结果:"></a>结果:</h1><ul>
<li>a. 详细的实验设置:<ul>
<li>本研究使用的数据集包含67个学科的11,528个多项选择题。这些问题由四名注释员手动从免费资源中收集，每小时50元人民币的费率。收集过程大约耗时250小时，并努力防止问题出现在语言模型的训练集中。数据集被分为包含5个问题的few-shot开发集和包含100多个问题的测试集。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>本研究评估了18个不同大小、语言导向和阶段（预训练或微调）的先进语言模型（LLMs），以评估它们在知识为中心的基准测试中的性能。评估包括零样本和少样本设置，并根据模型理解和利用知识的能力对模型进行评估。</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8cvil%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" data-id="cllkmcmex0005ksu72xzt0ecu" data-title="7月第二周cvil多模态知识蒸馏" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%B7%A8%E8%AF%AD%E8%A8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" rel="tag">跨语言多模态知识蒸馏</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-8月第三周" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%89%E5%91%A8/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T08:05:27.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%89%E5%91%A8/">8月第三周HomoDistil</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <ul>
<li><ul>
<li><h1 id="8月第三周周报"><a href="#8月第三周周报" class="headerlink" title="8月第三周周报"></a>8月第三周周报</h1><h2 id="论文1：HomoDistil-Homotopic-Task-Agnostic-Distillation-of-Pre-trained-Transformers"><a href="#论文1：HomoDistil-Homotopic-Task-Agnostic-Distillation-of-Pre-trained-Transformers" class="headerlink" title="论文1：HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers"></a>论文1：HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers</h2><p>base ICLR 2023</p>
<p>Chen Liang⋆∗, Haoming Jiang⋄, Zheng Li⋄, Xianfeng Tang⋄, Bin Yin⋄ &amp; Tuo Zhao⋆ ⋆Georgia Institute of Technology, ⋄ Amazon{cliang73,tourzhao}@gatech.edu,{jhaoming,amzzhe,xianft,alexbyin}@amazon.com</p>
<p>同主题蒸馏</p>
<h3 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h3><p>知识蒸馏已被证明是一个强大的模型压缩方法，以促进在实践中部署预训练的语言模型。本文侧重于与任务无关的蒸馏。它产生了一个紧凑的预训练模型，可以很容易地在计算成本和内存足迹较小的各种任务上进行微调。尽管具有实际的好处，但与任务无关的蒸馏具有挑战性。由于教师模型比学生模型具有更大的容量和更强的表示能力，因此学生很难在大量开放域训练数据上产生与教师匹配的预测。这种较大的预测差异通常会降低知识蒸馏的好处。为了应对这一挑战，我们提出了同主题蒸馏（HomoDistil），这是一种新的与任务无关的蒸馏方法，配备了迭代修剪。具体来说，我们从教师模型初始化学生模型，并迭代地修剪学生的神经元，直到达到目标宽度。这种方法在整个蒸馏过程中保持了教师和学生的预测之间的微小差异，保证了知识转移的有效性。大量实验表明，HomoDistil 在现有基线上取得了显着的改进。</p>
<h3 id="摘要总结："><a href="#摘要总结：" class="headerlink" title="摘要总结："></a>摘要总结：</h3><ul>
<li>(1): 本文介绍一种新型的迭代剪枝算法-Homotopic Distillation。</li>
<li>(2): 该算法采用结构剪枝技术，通过迭代剪枝来优化模型空间和时间效率。其实现步骤具体如下：</li>
<li>(3): 在优化模型时，每一次迭代会剪枝掉最不重要的神经元并对剪枝后的模型进行蒸馏。每个参数的重要性得分则可以根据公式进行计算。另外，我们采用SGD算法对学生模型进行权重更新，通过优化其知识蒸馏损失来提高模型的精度和泛化效果。</li>
<li>(4): 在BERT-base的自然语言理解和问答任务上，该算法在GLUE基准和SQuaD v1.1和v2.0数据集上都取得了优秀的结果，证明了Homotopic Distillation的有效性和可行性。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308201445125.png" alt="1"></p>
<p>HomoDistil 动机说明（用剪枝给知识蒸馏做初始化，并迭代式地获得最终的学生模型结构）</p>
<blockquote>
<p>随着预训练大模型规模的不断增加，任务特定蒸馏（下游微调蒸馏）的成本越来越高，任务无关蒸馏变得越来越重要。然而，由于教师模型的模型容量和表示能力远超学生模型，因此学生很难在大量开放域训练数据上模仿教师的预测。本文提出了同源蒸馏（Homotopic Distillation, HomoDistil）来缓解这一问题，该方法充分利用了蒸馏和剪枝的优势，将两者有机结合在了一起。</p>
<p>左图:在 HomoDistil 中，学生从老师初始化，并通过蒸馏过程迭代修剪。矩形的宽度表示层的宽度。颜色的深度代表训练的充分性。</p>
<p>右图：本文用教师模型初始化学生模型，以缓解两者在蒸馏过程中的容量和能力差异，并通过基于蒸馏损失的重要性得分的迭代剪枝，来逐步将学生模型修剪至最终想要的目标结构。在整个蒸馏+剪枝的过程中，教师和学生一直保持着较小的预测差异，这有助于知识更有效的传递。其核心动机如图 1 所示。</p>
</blockquote>
<p>该方法可从「蒸馏损失函数」和「迭代剪枝细节」两部分进行介绍。</p>
<h2 id="2-1-蒸馏损失函数"><a href="#2-1-蒸馏损失函数" class="headerlink" title="2.1 蒸馏损失函数"></a>2.1 蒸馏损失函数</h2><p>本文采用了与 TinyBERT [1] 的通用蒸馏阶段类似的蒸馏损失函数进行任务无关的蒸馏。整体损失函数可以分为三部分：</p>
<p>a）任务损失：设 是学生模型在开放域数据上预训练的任务损失（例如 BERT 的掩码语言建模损失 ）；</p>
<p>b）概率蒸馏损失：即 Hinton [2] 经典 KD 论文中的 KL 散度损失；</p>
<p>c）Transformer 蒸馏损失：具体包括教师和学生的中间层及嵌入层的隐层表示的差异损失，以及中间层的注意力得分差异损失。<br>当然，让我们详细解释上述段落中提到的公式和概念，并尝试回答可能的问题。我们这里讨论的是任务无关的知识蒸馏（Task-Agnostic Distillation），其目的是通过捕捉教师模型的某些内部表示来训练学生模型。</p>
<h3 id="1-知识蒸馏损失"><a href="#1-知识蒸馏损失" class="headerlink" title="1. 知识蒸馏损失 \"></a>1. 知识蒸馏损失 \</h3><p>$$<br>L_{\text{MLM}} + \alpha_1 D_{\text{KL}}<br>$$</p>
<h4 id="掩码语言建模损失-L-text-MLM"><a href="#掩码语言建模损失-L-text-MLM" class="headerlink" title="掩码语言建模损失 ( $L_{\text{MLM}} $)"></a>掩码语言建模损失 ( $L_{\text{MLM}} $)</h4><p>这部分损失与BERT等预训练模型的训练方式有关，其中模型试图预测句子中掩盖（或隐去）的词汇。它的目的是确保学生模型能够理解和生成自然语言。</p>
<h4 id="1-2-Kullback-Leibler-KL-散度损失-D-text-KL"><a href="#1-2-Kullback-Leibler-KL-散度损失-D-text-KL" class="headerlink" title="1.2 Kullback-Leibler (KL) 散度损失 ( $D_{\text{KL}}$ )"></a>1.2 Kullback-Leibler (KL) 散度损失 ( $D_{\text{KL}}$ )</h4><p>KL散度量化了教师模型和学生模型之间概率分布的差异。这有助于学生模型学习教师模型的概率预测行为。</p>
<h3 id="2-Transformer蒸馏损失"><a href="#2-Transformer蒸馏损失" class="headerlink" title="2. Transformer蒸馏损失"></a>2. Transformer蒸馏损失</h3><h4 id="2-1-隐藏表示的中间层蒸馏损失-L-text-hidn"><a href="#2-1-隐藏表示的中间层蒸馏损失-L-text-hidn" class="headerlink" title="2.1 隐藏表示的中间层蒸馏损失 ( $L_{\text{hidn}}$ )"></a>2.1 隐藏表示的中间层蒸馏损失 ( $L_{\text{hidn}}$ )</h4><p>学生模型和教师模型的隐藏表示在k-th中间层被比较。为了维度匹配，引入了一个可学习的线性投影矩阵( W_k^{\text{hidn}} )，使得学生模型的隐藏表示与教师模型在同一空间内。</p>
<ul>
<li>$ H_k^t  和 H_k^s $: 分别表示教师和学生模型在k-th中间层的隐藏表示。</li>
<li>${MSE}(H_k^t, H_k^s W_k^{\text{hidn}})$: 指的是教师和学生隐藏表示之间的均方误差。</li>
</ul>
<h4 id="2-2-嵌入层蒸馏损失-L-text-emb"><a href="#2-2-嵌入层蒸馏损失-L-text-emb" class="headerlink" title="2.2 嵌入层蒸馏损失 ( $L_{\text{emb}}$ )"></a>2.2 嵌入层蒸馏损失 ( $L_{\text{emb}}$ )</h4><p>嵌入层损失类似于中间层损失，但是作用在嵌入层。通过均方误差比较教师和学生模型的嵌入表示，并引入一个线性投影矩阵( $W_{\text{emb}}$ )来进行维度匹配。</p>
<h4 id="2-3-注意力蒸馏损失-L-text-attn"><a href="#2-3-注意力蒸馏损失-L-text-attn" class="headerlink" title="2.3 注意力蒸馏损失 ($ L_{\text{attn}}$ )"></a>2.3 注意力蒸馏损失 ($ L_{\text{attn}}$ )</h4><p>这一损失比较了教师和学生模型在k-th层的注意力分数矩阵。通过捕捉教师模型的注意力模式，有助于学生模型学习如何将注意力分配给不同的输入部分。</p>
<h3 id="3-总损失-L-text-total"><a href="#3-总损失-L-text-total" class="headerlink" title="3. 总损失 ($ L_{\text{total}}$ )"></a>3. 总损失 ($ L_{\text{total}}$ )</h3><p>所有这些损失项的加权和形成了总损失，通过优化这个总损失来训练学生模型。权重参数( \alpha_1, \alpha_2, \alpha_3, \alpha_4 ) 允许我们调整每个组件在总损失中的相对重要性。<strong>公式5</strong></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211622828.png" alt="image-20230819113820853"></p>
<h2 id="2-2-迭代剪枝细节"><a href="#2-2-迭代剪枝细节" class="headerlink" title="2.2 迭代剪枝细节"></a><strong>2.2 迭代剪枝细节</strong></h2><h3 id="1-学生模型的初始化"><a href="#1-学生模型的初始化" class="headerlink" title="1. 学生模型的初始化"></a>1. 学生模型的初始化</h3><p>学生模型从预训练的教师模型初始化，即$\theta_s^{(0)} &#x3D; \theta_t$。这意味着学生模型一开始的权重与教师模型相同。</p>
<h3 id="2-SGD型算法的更新"><a href="#2-SGD型算法的更新" class="headerlink" title="2. SGD型算法的更新"></a>2. SGD型算法的更新</h3><p>在第(t)次迭代中，学生模型根据总损失($L_{\text{total}}$)（由方程5定义）使用SGD（随机梯度下降）类型的算法进行更新。<strong>公式6</strong>更新规则为：</p>
<p>$\theta_s^{(t)} \leftarrow \theta_s^{(t-1)} - \eta \nabla_{\theta_s^{(t-1)}} L_{\text{total}}(\theta_s^{(t-1)}, \theta_t),$</p>
<p>其中($\eta$)是步长，控制每次迭代的更新幅度。</p>
<h3 id="3-计算重要性得分"><a href="#3-计算重要性得分" class="headerlink" title="3. 计算重要性得分"></a>3. 计算重要性得分</h3><p>接下来计算所有参数的重要性得分($S_j^{(t)}$)。重要性得分与参数的梯度和值有关，可以用来确定哪些神经元可以被剪枝，而不会显著影响模型性能。得分定义为：<br><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211622525.png" alt="image-20230819115152088"></p>
<h3 id="4-计算权重矩阵的重要性得分"><a href="#4-计算权重矩阵的重要性得分" class="headerlink" title="4. 计算权重矩阵的重要性得分"></a>4. 计算权重矩阵的重要性得分</h3><p>对于学生模型中的任何权重矩阵($W^{(t)}$)，其对应的重要性得分表示为($S^{(t)}_W$。然后定义每一列的重要性得分为$N^{(t)}_W$，<strong>公式7</strong> 其中</p>
<p>$N^{(t)}_{W_i} &#x3D; \left| S^{(t)}_W[:,i] \right|_1 \quad \forall i &#x3D; 1, \ldots, ds.$</p>
<p>这一步骤有助于确定哪些列（神经元）对模型最重要。</p>
<h3 id="5-计算二进制掩码"><a href="#5-计算二进制掩码" class="headerlink" title="5. 计算二进制掩码"></a>5. 计算二进制掩码</h3><p>接下来，计算与权重矩阵相关的二进制掩码($M^{(t)}_W$)，也就是<strong>公式8</strong> 其中</p>
<p>$M^{(t)}<em>{W[:,i]} &#x3D;<br>\begin{cases}<br>      1 &amp; \text{if } N^{(t)}</em>{W_i} \text{ is in the top } r^{(t)} \text{ of } N^{(t)}_W, \<br>      0 &amp; \text{otherwise}, , \forall i &#x3D; 1, \ldots, ds.<br>\end{cases}$</p>
<p>这个掩码用于确定哪些列保留，哪些列剪除。</p>
<h3 id="6-剪枝的时间表"><a href="#6-剪枝的时间表" class="headerlink" title="6. 剪枝的时间表"></a>6. 剪枝的时间表</h3><p>剪枝的速率(r(t))由一个三次递减函数控制，确保稀疏性缓慢增加，列逐渐剪除。参数(rf)代表最终稀疏性，(T)是总训练迭代次数，($0 \leq ti &lt; tf \leq T$)是超参数。</p>
<p>算法结果：</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211619521.png" alt="image-20230819135528828"></p>
<blockquote>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ol>
<li>($\theta_t$)：教师模型的参数。</li>
<li>($T, t_i, t_f, r_f, \alpha_1, \alpha_2, \alpha_3, \alpha_4$)：超参数。</li>
</ol>
<h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><ol>
<li>($\theta(T)_s$)：学生模型的参数。</li>
</ol>
<h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><ol>
<li><p><strong>初始化</strong>：学生模型的参数($\theta(0)_s$)被初始化为教师模型的参数($\theta_t$)。</p>
</li>
<li><p><strong>循环训练</strong>：算法进行(T)次迭代，其中每次迭代执行以下步骤：<br>a. <strong>计算总损失</strong>：根据等式5计算总损失($L_{\text{total}}$)。<br>b. <strong>参数更新</strong>：使用梯度下降法更新学生模型的参数。更新规则为($\theta(t)_s \leftarrow \theta(t-1)_s - \eta \nabla \theta(t-1)<em>s L</em>{\text{total}}$)，其中($\eta$)是学习率。<br>c. <strong>计算重要性得分</strong>：根据等式6计算重要性得分(S(t))。<br>d. <strong>计算权重和掩码</strong>：对于学生模型的每个权重矩阵(W(t))：</p>
<ul>
<li>计算各列的重要性得分(N(t)_W)，遵循等式7。</li>
<li>计算二进制掩码(M(t)_W)，遵循等式8。</li>
<li>使用掩码更新权重矩阵：($W(t) \leftarrow W(t) \odot M(t)_W$)。</li>
</ul>
</li>
</ol>
</blockquote>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>数据:<strong>语料库</strong>：</p>
<ul>
<li><strong>Wikipedia 2</strong>：包含2500M单词的英语维基百科语料库。</li>
<li><strong>BookCorpus</strong>（Zhu et al., 2015）：包含800M单词。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211619940.png" alt="image-20230819135306819"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211619689.png" alt="image-20230819142515987"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211619638.png" alt="image-20230819142537042"></p>
<h1 id="论文2-HOW-I-LEARNED-TO-STOP-WORRYING-AND-LOVERETRAINING"><a href="#论文2-HOW-I-LEARNED-TO-STOP-WORRYING-AND-LOVERETRAINING" class="headerlink" title="论文2 HOW I LEARNED TO STOP WORRYING AND LOVERETRAINING"></a>论文2 HOW I LEARNED TO STOP WORRYING AND LOVERETRAINING</h1><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: How I Learned to Stop Worrying and Love Retraining (我如何学会不再担心并热爱重新训练)</li>
<li>Authors: Max Zimmer, Christoph Spiegel, Sebastian Pokutta</li>
<li>Affiliation: Department for AI in Society, Science, and Technology, Zuse Institute Berlin, Germany (柏林祖斯研究所，德国)</li>
<li>Keywords: Neural Network Pruning, Retraining, Learning Rate Schedule, Budgeted Training</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=xxx">Paper</a> , [GitHub: None]</li>
<li>ICLR 2023</li>
</ul>
<h1 id="论文简要"><a href="#论文简要" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>本研究提出了一种简单而高效的方法，通过使用线性学习率调度来大幅缩短神经网络修剪后的重新训练阶段，并在初始稠密训练阶段上实施预算控制，从而在质量和速度上超越了现有的复杂方法。</li>
</ul>
<p>论文动机：</p>
<ol>
<li><strong>剪枝的必要性和挑战</strong>：由于现代神经网络通常具有大量的参数，因此需要高内存和计算资源。通过剪枝可以显著减少这些需求，但同时也带来了许多挑战，例如剪枝后的性能下降以及剪枝过程的计算复杂性。</li>
<li><strong>迭代大小剪枝（IMP）的问题</strong>：虽然IMP是一种有效的剪枝方法，但其计算复杂性和潜在的次优状态引起了关注。作者意图重新审视并改进这种方法，使其更加实用和有效。</li>
<li><strong>重新思考剪枝和再训练的方法</strong>：作者试图通过在预算训练的背景下重新思考IMP的再训练阶段，找到一种可以大大缩短运行时间并保持性能的方法。</li>
<li><strong>寻找更有效的学习率计划</strong>：通过提出自适应线性学习速率重启（ALLR），作者试图解决选择线性计划的初始值的问题，并考虑剪枝的影响和整体再训练时间。</li>
<li><strong>挑战现有观点</strong>：作者挑战了一般认为剪枝稳定方法优于依赖“硬”剪枝和再训练的方法的观点，通过实验和新方法来证明他们的观点。</li>
<li><strong>提供实用的解决方案</strong>：通过提出预算IMP（BIMP）和其他改进，作者意图提供一种实际、高效的解决方案，以便在不牺牲性能的情况下更有效地剪枝神经网络。</li>
</ol>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 近年来，神经网络修剪已成为压缩模型的重要方法，但修剪后的网络需要经过重新训练来恢复性能，而这一过程通常非常耗时。</li>
<li>过去方案: 传统的修剪方法通常需要多次迭代的重新训练，计算资源消耗较大，而现有的稳定修剪方法在训练过程中引入了复杂的参数化，也无法很好地解决这个问题。</li>
<li>论文的Motivation: 针对修剪后的重新训练阶段的学习率调度问题，以及初始稠密训练阶段的预算控制问题，本研究提出了一种简单而高效的方法，通过线性学习率调度和预算控制，实现了修剪后网络的高质量和快速训练。</li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211150540.png" alt="image-20230821115029487"></p>
<p>提出了几种不同的学习率调度策略来改进迭代式裁剪-重训循环(IMP)中的重训阶段,包括:</p>
<ul>
<li>LRW:学习率回绕(Learning Rate Rewinding),使用原训练的后几个学习率进行重训</li>
<li>SLR:学习率重新启动(Restarting),使用缩放后的原学习率调度进行重训</li>
<li>CLR:周期性学习率重新启动,使用基于余弦的周期学习率进行重训</li>
<li>LLR:线性学习率重新启动,使用线性衰减的学习率进行重训</li>
<li>ALLR:自适应线性学习率重新启动,根据裁剪影响自适应地确定初始学习率</li>
</ul>
<h3 id="裁剪稳定性"><a href="#裁剪稳定性" class="headerlink" title="裁剪稳定性"></a>裁剪稳定性</h3><p>提出了 Budgeted IMP (BIMP) 方法,在给定训练epoch次数budget内,首先dense训练,然后进行IMP裁剪和重训,避免额外的隐性偏置,与传统裁剪稳定性方法进行公平比较。</p>
<h3 id="裁剪方式"><a href="#裁剪方式" class="headerlink" title="裁剪方式"></a>裁剪方式</h3><p>简单采用基于权重大小的全局裁剪方式。</p>
<p>总体来说,主要贡献在于提出了重新思考重训的学习率调度策略,并在公平的基础上比较了裁剪稳定性方法,为神经网络裁剪的高效训练提供了新思路。</p>
<ul>
<li>a. 理论背景:<ul>
<li>本文讨论了现代神经网络架构中过参数化的问题以及修剪模型以压缩这些模型的潜在好处。介绍了迭代幅度修剪（IMP）的概念以及与重新训练阶段相关的挑战。作者提出重新思考重新训练阶段，将其置于预算训练的背景下，并证明可以使用简单的线性学习率调度显著缩短重新训练时间。他们还提出了一种自适应选择学习率初始值的方法。此外，他们建议在IMP的初始密集训练阶段施加预算，以高效地生成稀疏的训练网络。作者强调他们的研究结果质疑了修剪稳定方法通常优于依赖“硬”修剪和重新训练方法的观点。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>本文讨论了修剪神经网络的不同重新训练调度方法。作者提出了几种方法，包括速率回退（LRW）、缩放学习率重启（SLR）、循环学习率重启（CLR）、线性学习率重启（LLR）和自适应线性学习率重启（ALLR）。LRW使用每个周期的最后学习率重新训练修剪后的网络。SLR将原始调度压缩到重新训练时间框架内，并包含一个短暖身阶段。CLR基于1周期学习率调度，并包含一个短暖身阶段。LLR在重新训练期间使用线性学习率调度，而ALLR根据修剪的影响和可用的重新训练时间动态调整重新训练调度的初始值。作者根据Li等人（2020）关于在固定迭代预算内训练神经网络的研究结果提出了这些方法。他们认为重新训练应该考虑预算训练的方面，并且从预算训练中得出的经验通常适用于修剪的背景下。</li>
</ul>
</li>
</ul>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211154782.png" alt="image-20230821115453710"></p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>实验结果显示，修剪后的重新训练类似于预算训练。线性和余弦学习率调度优于常数和阶梯调度，其中LLR略优于CLR。然而，对于较短的重新训练时间和中等稀疏度，CLR和LLR等固定重启调度与FT和LRW相比不太竞争。ALLR在小的重新训练预算范围内始终改进了以前的方法。实验结果还展示了ALLR在不同重新训练周期下实现高测试准确率的有效性。此外，对于CIFAR-10上的ResNet-56和ImageNet上的ResNet-50的结果显示了BIMP与修剪稳定方法在不同稀疏度水平上的比较。BIMP在准确性、加速比和稀疏度方面与其他方法相比具有竞争力。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>实验结果显示，BIMP方法在测试性能、理论加速比和实现的稀疏度方面优于许多其他修剪稳定方法。对于ImageNet，DNW方法与BIMP相当或更好，但训练时间大约是BIMP的两倍。大多数其他方法与BIMP相比似乎处于劣势，而DPF是一个强有力的竞争对手。BIMP在相同数量的训练周期内实现了这些结果，并且没有一些其他方法的计算开销。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%89%E5%91%A8/" data-id="cllkmcmf4000eksu756mw56ms" data-title="8月第三周HomoDistil" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%89%AA%E6%9E%9D-%E8%92%B8%E9%A6%8F/" rel="tag">剪枝+蒸馏</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-8月第二周剪枝相关" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T08:03:58.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3/">8月第二周剪枝相关</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="8月第二周-以前的剪枝相关论文"><a href="#8月第二周-以前的剪枝相关论文" class="headerlink" title="8月第二周 以前的剪枝相关论文"></a>8月第二周 以前的剪枝相关论文</h1><h1 id="论文1：Densely-Connected-Convolutional-Networks-密集连接卷积网络"><a href="#论文1：Densely-Connected-Convolutional-Networks-密集连接卷积网络" class="headerlink" title="论文1：Densely Connected Convolutional Networks (密集连接卷积网络)"></a>论文1：Densely Connected Convolutional Networks (密集连接卷积网络)</h1><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Densely Connected Convolutional Networks (密集连接卷积网络)</li>
<li>Authors: Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger</li>
<li>Affiliation: First author’s affiliation: Cornell University (第一作者所属机构：康奈尔大学)</li>
<li>Keywords: convolutional networks, deep learning, dense connectivity, vanishing-gradient problem, feature reuse (卷积网络，深度学习，密集连接，梯度消失问题，特征重用)</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.06993">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/liuzhuang13/DenseNet">GitHub Code</a></li>
<li>16年8月</li>
</ul>
<h1 id="论文简要"><a href="#论文简要" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604156.png" alt="b8624ba6215bcbdf5fbd0071bbd3eccf_0_Figure_1"></p>
<p>图1：DenseNet在ResNet的基础上(ResNet介绍)，进一步扩展网络连接，该层前面所有层的feature map都是这层的输入，该层的feature map是后面所有层的输入。网络结构如上图:</p>
<ul>
<li>动机：随着CNN网络层数的不断增加，gradient vanishing和modeldegradation问题出现在了人们面前，BatchNormalization的广泛使用在一定程度上缓解了gradient vanishing的问题，而ResNet和HighwayNetworks通过构造恒等映射设置旁路，进一步减少了gradient vanishing和model degradation的产生。</li>
</ul>
<p>创新点总结:</p>
<p>(1)相比ResNet拥有更少的参数数量<br>(2)旁路加强了特征的重用.<br>(3)网络更易于训练，并具有一定的正则效果<br>(4)缓解了gradient vanishing和model degradation的问题</p>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 卷积神经网络（CNNs）已成为视觉目标识别的主要机器学习方法。然而，随着网络层数的增加，信息和梯度在网络中传递时可能会消失，这成为一个新的研究问题。</li>
<li>过去方案: 过去的研究提出了一些解决梯度消失问题的方法，如ResNets和Highway Networks，通过建立早期层与后期层之间的短路径来保留信息。然而，这些方法仍存在一些问题，如参数冗余和训练困难。</li>
<li>CNN史上的一个里程碑事件是ResNet模型的出现，ResNet可以训练出更深的CNN模型，从而实现更高的准确度。ResNet模型的核心是通过建立前面层与后面层之间的“短路连接”（shortcuts，skip connection），这有助于训练过程中梯度的反向传播，从而能训练出更深的CNN网络。</li>
<li>DenseNet模型，它的基本思路与ResNet一致，但是它建立的是前面所有层与后面层的密集连接（dense connection），它的名称也是由此而来。DenseNet的另一大特色是通过特征在channel上的连接来实现特征重用（feature reuse）。这些特点让DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能，DenseNet也因此斩获CVPR 2017的最佳论文奖。</li>
<li>论文的Motivation: 基于过去方法的观察，本文提出了一种新的网络架构，即DenseNet，通过密集连接每一层，实现了信息的最大流动和特征的重用。这种架构具有更好的参数效率和训练性能。</li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><p>Residual Connection 能够让模型趋向于宽网络，Densenet论文核心思想:对每一层的前面所有层都加一个单独的 shortcut到该层，使得任意两层网络都可以直接”沟通”。即下图：</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604162.png" alt="b8624ba6215bcbdf5fbd0071bbd3eccf_2_Figure_2"></p>
<h3 id="1-密集块（Dense-Block）受到GoogleNet的启发，论文提出了Dense-Block，即在每个Block内，所有layer都保持dense-connectivity，而在Block之间是没有dense-connectivity，而是通过transition-layer连接的"><a href="#1-密集块（Dense-Block）受到GoogleNet的启发，论文提出了Dense-Block，即在每个Block内，所有layer都保持dense-connectivity，而在Block之间是没有dense-connectivity，而是通过transition-layer连接的" class="headerlink" title="1. 密集块（Dense Block）受到GoogleNet的启发，论文提出了Dense Block，即在每个Block内，所有layer都保持dense connectivity，而在Block之间是没有dense connectivity，而是通过transition layer连接的"></a>1. 密集块（Dense Block）受到GoogleNet的启发，论文提出了Dense Block，即在每个Block内，所有layer都保持dense connectivity，而在Block之间是没有dense connectivity，而是通过transition layer连接的</h3><p>DenseNet的核心是一种名为“密集块”的结构。在密集块中，每一层都与之前所有层直接连接。这意味着第 ( i ) 层的输入不仅是第 ( i-1 ) 层的输出，还包括第 ( i-2 ) 层、第 ( i-3 ) 层等的输出。每一层的输出又成为其后所有层的输入。</p>
<p>这种密集连接的数学表达可以用下式表示：</p>
<p>$$<br>x_{l} &#x3D; H_{l}([x_{0}, x_{1}, \ldots, x_{l-1}])<br>$$</p>
<p>其中，( x_{l} ) 是第 ( l ) 层的输出，( H_{l} ) 是第 ( l ) 层的转换函数（如卷积、激活函数等），方括号表示将所有输入连接在一起。</p>
<p>H()就是Composite function，,每个Composite function的结构如下:<br>即单个Block内，层与层之间的非线性转换函数：</p>
<p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812120140992.png" alt="image-20230812120140992"></p>
<h3 id="2-过渡层（Transition-Layer）"><a href="#2-过渡层（Transition-Layer）" class="headerlink" title="2. 过渡层（Transition Layer）"></a>2. 过渡层（Transition Layer）</h3><p>由于密集连接可能导致特征图尺寸和通道数急剧增加，DenseNet引入了“过渡层”来控制网络的复杂性。过渡层通常包括卷积层和池化层，用于减少特征图的尺寸和通道数。结构如下</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604167.png" alt="image-20230812120040464"></p>
<h3 id="3-增长率（Growth-Rate）"><a href="#3-增长率（Growth-Rate）" class="headerlink" title="3. 增长率（Growth Rate）"></a>3. 增长率（Growth Rate）</h3><p>增长率是DenseNet中的一个关键参数，用于控制每一层添加的特征数量。假设增长率为 ( k )，则每一层产生 ( k ) 个新特征图，并将其与之前的特征图连接在一起。增长率有助于平衡网络的容量和计算复杂性。</p>
<p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812120217759.png" alt="image-20230812120217759"></p>
<h3 id="4-Bottleneck-Layer"><a href="#4-Bottleneck-Layer" class="headerlink" title="4. Bottleneck Layer"></a>4. Bottleneck Layer</h3><p>为了进一步减少计算负担（即使是每层只产生k个feature maps，但还是很多），DenseNet通常在每个卷积层之前添加一个“瓶颈层”（Bottleneck Layer）。瓶颈层包括1x1的卷积操作，用于减少特征图的通道数，然后再传递到更复杂的卷积层。</p>
<p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812120300514.png" alt="image-20230812120300514"></p>
<h3 id="5-合适的损失函数和优化器"><a href="#5-合适的损失函数和优化器" class="headerlink" title="5. 合适的损失函数和优化器"></a>5. 合适的损失函数和优化器</h3><p>与其他深度学习模型一样，DenseNet也需要选择合适的损失函数和优化器来进行训练。常用的损失函数如交叉熵损失，优化器如Adam或SGD等。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>DenseNet通过引入密集连接实现了特征的有效传播和重用，减轻了梯度消失问题，降低了参数数量。其独特的密集块、过渡层、增长率和瓶颈层等设计使其在多项计算机视觉任务上表现出色。</p>
<ul>
<li>a. 理论背景:<ul>
<li>本文介绍了密集卷积网络（DenseNets），它以前馈方式将每一层与其他每一层相连接。这种密集连接模式具有几个优点，包括缓解梯度消失问题、增强特征传播、鼓励特征重用和减少参数数量。DenseNets在目标识别基准任务上取得了显著的改进，同时需要更少的计算量。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>DenseNets通过特征重用来利用网络的潜力，从而得到易于训练和参数高效的紧凑模型。它们将任意一层与所有后续层直接连接，改善了层之间的信息流动。每一层的复合函数包括批归一化、修正线性单元和卷积。为了便于下采样，网络被划分为具有过渡层的密集连接的密集块。网络的增长率决定了每一层对网络全局状态的贡献程度。还使用瓶颈层和压缩因子来提高计算效率和模型紧凑性。</li>
</ul>
</li>
</ul>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果:"></a>实验结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604154.png" alt="image-20230812121120350"></p>
<ul>
<li><p>使用的是DenseNet-BC</p>
</li>
<li><p>使用4个Dense Block</p>
</li>
<li><p>在送入第一个Dense Block前，会先送到一个7x7x2k的stride&#x3D;2的卷积层</p>
</li>
<li><p>o所有的layers的feature map都设置为k</p>
</li>
<li><p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812121225490.png" alt="image-20230812121225490"></p>
</li>
<li><p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812121245625.png" alt="image-20230812121245625"></p>
</li>
<li><p>a. 详细的实验设置:</p>
<ul>
<li>DenseNet-BC在CIFAR数据集上取得了3.46%的错误率（C10+）和17.18%的错误率（C100+），优于现有的最先进结果。</li>
<li>DenseNet-BC在SVHN数据集上取得了比wide ResNet更好的结果，L&#x3D;100，k&#x3D;24，使用了dropout。</li>
</ul>
</li>
<li><p>b. 详细的实验结果:</p>
<ul>
<li>250层的DenseNet-BC在SVHN上的性能与较短的对应模型相比没有进一步提高。</li>
<li>DenseNet-BC在ImageNet分类任务上与最先进的ResNet架构相比，性能相当，但需要更少的参数和计算量。</li>
</ul>
</li>
</ul>
<h3 id="Q1-论文试图解决什么问题？"><a href="#Q1-论文试图解决什么问题？" class="headerlink" title="Q1 论文试图解决什么问题？"></a>Q1 论文试图解决什么问题？</h3><p>论文介绍了一种名为密集连接卷积网络（Dense Convolutional Network，简称 DenseNet）的新型结构。这一结构旨在解决卷积神经网络（CNN）中的一些核心问题，如梯度消失问题、特征传播的困难、特征重用的不足以及参数数量的增多。</p>
<h3 id="Q2-这是否是一个新的问题？"><a href="#Q2-这是否是一个新的问题？" class="headerlink" title="Q2 这是否是一个新的问题？"></a>Q2 这是否是一个新的问题？</h3><p>DenseNet提供了一种新的解决方案，通过密集连接每一层到其它所有层来解决这些问题。</p>
<h3 id="Q3-这篇文章要验证一个什么科学假设？"><a href="#Q3-这篇文章要验证一个什么科学假设？" class="headerlink" title="Q3 这篇文章要验证一个什么科学假设？"></a>Q3 这篇文章要验证一个什么科学假设？</h3><p>文章的科学假设是，通过在网络中引入密集连接，可以增强特征传播，减轻梯度消失问题，增强特征重用，并显著减少参数数量。这种连接方式被认为可以提高准确性并降低训练成本。</p>
<h3 id="Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？"><a href="#Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？" class="headerlink" title="Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？"></a>Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h3><p>相关研究包括不同类型的卷积神经网络结构，如LeNet、VGG等。这些研究可以归类为深度学习和计算机视觉领域内的网络架构研究。此论文的作者，如Gao Huang、Zhuang Liu、Laurens van der Maaten和Kilian Q. Weinberger等，都是该领域值得关注的研究员。</p>
<h3 id="Q5-论文中提到的解决方案之关键是什么？"><a href="#Q5-论文中提到的解决方案之关键是什么？" class="headerlink" title="Q5 论文中提到的解决方案之关键是什么？"></a>Q5 论文中提到的解决方案之关键是什么？</h3><p>解决方案的关键在于每一层与其它所有层之间的密集连接。这种连接方式使得每一层的特征图都作为其后每一层的输入，并且自身的特征图也作为所有后续层的输入。这增强了特征传播和重用，降低了参数数量。</p>
<h3 id="Q6-论文中的实验是如何设计的？"><a href="#Q6-论文中的实验是如何设计的？" class="headerlink" title="Q6 论文中的实验是如何设计的？"></a>Q6 论文中的实验是如何设计的？</h3><p>论文评估了DenseNet在四个竞争激烈的对象识别基准任务（CIFAR-10、CIFAR-100、SVHN和ImageNet）上的性能。实验设计用于比较DenseNet与现有最先进方法的性能，以证明其优越性和效率。</p>
<h3 id="Q7-用于定量评估的数据集是什么？代码有没有开源？"><a href="#Q7-用于定量评估的数据集是什么？代码有没有开源？" class="headerlink" title="Q7 用于定量评估的数据集是什么？代码有没有开源？"></a>Q7 用于定量评估的数据集是什么？代码有没有开源？</h3><p>用于定量评估的数据集包括CIFAR-10、CIFAR-100、SVHN和ImageNet。论文中提到了代码和预训练模型已在GitHub上开源。</p>
<h3 id="Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？"><a href="#Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？" class="headerlink" title="Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？"></a>Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？</h3><p>是的，论文中的实验结果显示DenseNets在多个基准任务上实现了显著改进，证明了其科学假设的有效性，即密集连接可以提高性能并降低计算成本。</p>
<h3 id="Q9-这篇论文到底有什么贡献？"><a href="#Q9-这篇论文到底有什么贡献？" class="headerlink" title="Q9 这篇论文到底有什么贡献？"></a>Q9 这篇论文到底有什么贡献？</h3><p>这篇论文的主要贡献是引入了DenseNet结构，这一新型结构通过密集连接解决了传统卷积神经网络中的一些问题，提高了准确性并降低了训练成本。它为深度学习和计算机视觉领域提供了一种新的研究方向。</p>
<h3 id="Q10-下一步呢？有什么工作可以继续深入？"><a href="#Q10-下一步呢？有什么工作可以继续深入？" class="headerlink" title="Q10 下一步呢？有什么工作可以继续深入？"></a>Q10 下一步呢？有什么工作可以继续深入？</h3><p>下一步的工作可能包括将DenseNet结构应用于更多类型的任务和领域，如自然语言处理、医学图像分析等。此外，也可以探索更多优化DenseNet结构的方法，如更高效的训练策略、更精细的连接模式等，以进一步提高其性能和通用性。</p>
<h3 id="密集连接放到transformer"><a href="#密集连接放到transformer" class="headerlink" title="密集连接放到transformer"></a>密集连接放到transformer</h3><h1 id="论文2-Pruning-Filters-for-Efficient-ConvNets-高效卷积神经网络的滤波器剪枝"><a href="#论文2-Pruning-Filters-for-Efficient-ConvNets-高效卷积神经网络的滤波器剪枝" class="headerlink" title="论文2 Pruning Filters for Efficient ConvNets (高效卷积神经网络的滤波器剪枝)"></a>论文2 Pruning Filters for Efficient ConvNets (高效卷积神经网络的滤波器剪枝)</h1><h1 id="Basic-Information-1"><a href="#Basic-Information-1" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Pruning Filters for Efficient ConvNets (高效卷积神经网络的滤波器剪枝)</li>
<li>Authors: Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf</li>
<li>Affiliation: University of Maryland (马里兰大学), NEC Labs America (NEC美国实验室)</li>
<li>Keywords: CNNs, computation costs, parameter storage costs, pruning filters, convolutional layers, accuracy, acceleration, sparsity, BLAS libraries</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.08710">Paper</a>, [GitHub: None]</li>
<li>16年8月</li>
</ul>
<h2 id="核心思想：用weight值的大小来评判filter的重要性，对于一个filter，对所有weight的绝对值求和-求L1范数-，作为该filter的评价指标，排序结果并将一层中值低的filter裁掉。属于Filters剪枝"><a href="#核心思想：用weight值的大小来评判filter的重要性，对于一个filter，对所有weight的绝对值求和-求L1范数-，作为该filter的评价指标，排序结果并将一层中值低的filter裁掉。属于Filters剪枝" class="headerlink" title="核心思想：用weight值的大小来评判filter的重要性，对于一个filter，对所有weight的绝对值求和(求L1范数)，作为该filter的评价指标，排序结果并将一层中值低的filter裁掉。属于Filters剪枝"></a>核心思想：用weight值的大小来评判filter的重要性，对于一个filter，对所有weight的绝对值求和(求L1范数)，作为该filter的评价指标，排序结果并将一层中值低的filter裁掉。属于Filters剪枝</h2><h1 id="论文简要-1"><a href="#论文简要-1" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>本研究提出了一种用于卷积神经网络（CNNs）的加速方法，通过剪枝对输出准确性影响较小的滤波器，从而显著降低计算成本。</li>
</ul>
<h1 id="背景信息-1"><a href="#背景信息-1" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: CNNs在各种应用中取得了成功，但计算和参数存储成本也显著增加。过去的研究主要集中在减少这些开销，包括剪枝和压缩各层的权重，以减少计算和存储成本，同时保持原始准确性。</li>
<li>过去方案: 过去的剪枝方法主要集中在剪枝权重，但这种方法在卷积层中可能无法充分减少计算成本，因为剪枝网络中存在不规则的稀疏性。此外，这些方法通常需要支持稀疏卷积库，而本研究提出的方法不需要，并且可以与现有的高效BLAS库一起使用。</li>
<li>论文的Motivation: 鉴于过去的剪枝方法存在的问题，本研究提出了一种新的滤波器剪枝方法，通过剪枝对输出准确性影响较小的滤波器，从而显著降低计算成本。与剪枝权重不同，这种方法不会导致稀疏连接模式，因此不需要稀疏卷积库的支持，并且可以与现有的高效BLAS库一起使用。通过实验证明，即使是简单的滤波器剪枝技术也可以在保持准确性的同时，将VGG-16的推理成本降低高达34%，将ResNet-110的推理成本降低高达38%。</li>
</ul>
<h1 id="方法-1"><a href="#方法-1" class="headerlink" title="方法:"></a>方法:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604181.png" alt="003b8469928619fec974c021af486830_2_Figure_1"></p>
<p>1.PRUNING FILTERS AND FEATURE MAPS<br>n表示的是第个卷积层的输入通道数，ni+1表示的是第i+1个filter，(hwi)表示的是输入featuremap的尺寸大小，单个filter(3D)的大<br>小表示为(k，k，ni)，总的filters表示为(ni+1，k，k，ni)，那么可以得到:一个feature map经过总的filters得到输出feature map后，需要进<br>行的计算量是ni<em>ni+1 <em>k</em>k</em>hi+1 <em>Wi+1，对应下图的kernel matrix。<br>那么，根据图片所示，<br>如果剪掉一个 feature map，可以直接减少ni</em>k<em>hi+1</em>Wi+1个运算(总的有ni+1个卷积核，输出通道数少了一个)。<br>输出 feature map 的减少一个，就会导致后面附加移除ni+2<em>k”</em>hi+2*wi+2个运算(输入通道数少一个，共有ni+1个输入通道<br>数)。所以减少m个 feature maps 可以减少m&#x2F;ni+1的计算量。图中的 kernel matrix每一列表示一个3Dfilter，一个3Dfilter会有一个feature map输出。</p>
<ol start="2">
<li>DETERMINING WHICH FILTERS TO PRUNE WITHIN A SINGLE LAYER<br>第i个卷积层剪掉m个卷积核的过程算法流程如下:<br>1.对每个卷积核，计算它的权重绝对值之和s。<br>2.根据s结果排序。<br>3.将m个权重绝对值之和最小的卷积核以及对应的 feature maps 剪掉。下一个卷积层中与剪掉的 feature maps 相关的核也要移除。<br>4.一个对干策i层和第i+1层的新的权重矩阵被创建，并目剩下的权重参数被复制到新模型中。 <img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604492.png" alt="image-20230812124443795"></li>
<li>DETERMINING SINGLE LAYER’S SENSITIVITY TO PRUNING<br>为了弄清楚每层的敏感度，论文对每一层独立剪枝并在验证集上对剪枝后的网络进行评估，Fiqure2(b)展示了结果，很明显斜率比较平缓的层对剪枝的敏感度更高，我们根据经验来决定对每一层的卷积核进行剪枝，对于深度网络(如VGG-16或ResNets)，我们观察到同一stage(相同尺寸的特征图)对应的层对剪枝的敏感度相似，为了避免引入layer-wise meta-parameters，我们对于同一stage的所有层使用相同的剪枝比例。对于那些敏感度高的层，我们剪枝时比例很小，甚至完全不进行剪枝。</li>
<li>Puring filters across multiple layers<br>之前的工作是逐层剪枝，然后重复训练来恢复精度。然而，理解如何能一次性对多层进行剪枝是非常有必要的:<br>1.对于深度网络，逐层剪枝再训练太耗时;<br>2.整体剪枝的方法提供给网络稳健性的一个全面视野，从而导致产生一个更小的网络;<br>3.对于复杂的网络，一个整体的方法很有必要，比如对于ResNet，对恒等映射特征图或者每个残差模块的第二个层剪枝会导致额外层的修剪:为了对多层同时剪枝，我们考虑了两个策略:<br>  4.每一层独立剪枝，即在计算(求权重绝对值之和)时不考虑上一层的修剪情况，所以计算时下图中的黄点仍然参与计算;<br>   5.贪心策略，计算时不计算已经修剪过的，即黄点不参与计算;</li>
</ol>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604736.png" alt="image-20230812124802841"></p>
<h3 id="1-连续层的剪枝："><a href="#1-连续层的剪枝：" class="headerlink" title="1. 连续层的剪枝："></a>1. <strong>连续层的剪枝</strong>：</h3><ul>
<li>这里涉及的是一种剪枝策略，考虑了卷积神经网络中连续层之间的关系。剪枝一个层的滤波器可能会影响下一个层的计算，因为滤波器之间存在依赖关系。</li>
</ul>
<h3 id="2-独立剪枝策略："><a href="#2-独立剪枝策略：" class="headerlink" title="2. 独立剪枝策略："></a>2. <strong>独立剪枝策略</strong>：</h3><ul>
<li>这个策略在计算每个滤波器的重要性时不考虑前一层已经被剪枝的滤波器。</li>
<li>例如，假设我们正在查看三个连续的层，用 ��,��+1,��+2<em>n**i</em>,<em>n**i</em>+1,<em>n**i</em>+2 来表示每层的滤波器数量。在计算第 �+1<em>i</em>+1 层的滤波器重要性时，这个策略不考虑第 �<em>i</em> 层哪些滤波器被剪除（蓝色标记）。</li>
<li>由于不考虑前一层的剪枝，所以某些卷积核权重（黄色标记）仍然会被包括在计算中。</li>
</ul>
<h3 id="3-贪婪剪枝策略："><a href="#3-贪婪剪枝策略：" class="headerlink" title="3. 贪婪剪枝策略："></a>3. <strong>贪婪剪枝策略</strong>：</h3><ul>
<li>与独立剪枝策略不同，贪婪剪枝策略在计算滤波器重要性时会考虑前一层已经剪除的滤波器。</li>
<li>这意味着如果前一层的某个特征映射已经被剪除，那么在下一层的计算中，与该特征映射相对应的卷积核不会被计算。</li>
</ul>
<h3 id="4-结果的卷积核矩阵："><a href="#4-结果的卷积核矩阵：" class="headerlink" title="4. 结果的卷积核矩阵："></a>4. <strong>结果的卷积核矩阵</strong>：</h3><ul>
<li>无论采用哪种策略，剪枝过程都将导致新的卷积核矩阵大小为 (��+1−1)×(��+2−1)(<em>n**i</em>+1−1)×(<em>n**i</em>+2−1)。实测第二种策略精度会更高</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604371.png" alt="image-20230812125105617"></p>
<p>这段话讨论了在卷积神经网络中剪枝残差块的特殊情况。残差块是一种特定的网络结构，它包括一个或多个卷积层和一个称为“快捷路径”或“投影快捷路径”的特殊连接。我们来详细解释这段文字的含义：</p>
<h3 id="1-残差块和投影快捷路径："><a href="#1-残差块和投影快捷路径：" class="headerlink" title="1. 残差块和投影快捷路径："></a>1. <strong>残差块和投影快捷路径</strong>：</h3><ul>
<li>残差块包括一系列卷积层，这些层的输出与输入相加，形成“残差”连接。</li>
<li>投影快捷路径是一种特殊的连接，它可以改变输入的维度以匹配残差块内层的输出维度。通常通过卷积操作实现。</li>
</ul>
<h3 id="2-剪枝残差块的第一层："><a href="#2-剪枝残差块的第一层：" class="headerlink" title="2. 剪枝残差块的第一层："></a>2. <strong>剪枝残差块的第一层</strong>：</h3><ul>
<li>这段文字指出，残差块的第一层可以“无限制”地剪枝。也就是说，您可以自由选择要剪除的滤波器，而不必担心会影响其他部分的网络。</li>
</ul>
<h3 id="3-剪枝残差块的第二层："><a href="#3-剪枝残差块的第二层：" class="headerlink" title="3. 剪枝残差块的第二层："></a>3. <strong>剪枝残差块的第二层</strong>：</h3><ul>
<li><p>残差块的第二层的剪枝与投影快捷路径的剪枝结果有关。</p>
</li>
<li><p>这里的逻辑是，由于投影快捷路径直接与残差块的第二层连接，因此如果投影快捷路径中的某个滤波器被剪除（例如，由于被认为不重要），那么残差块的第二层中与该滤波器相对应的滤波器也应被剪除。</p>
</li>
<li><p>这些要剪除的滤波器被标记为绿色。</p>
</li>
<li><p>a. 理论背景:</p>
<ul>
<li>本文讨论了卷积神经网络（CNNs）的计算和参数存储成本的增加以及需要高效和小型网络尺寸的需求。还强调了减少处理大量图像的网络服务的推理时间的重要性。</li>
</ul>
</li>
<li><p>b. 技术路线:</p>
<ul>
<li>本文提到了减少CNNs中计算成本的模型压缩和技术的先前研究。其中包括权重修剪、低秩逼近、基于FFT的卷积和量化等技术。还提到了其他方法，这些方法专注于去除冗余的特征图或使用稀疏约束训练紧凑的CNNs。</li>
</ul>
</li>
</ul>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812124123590.png" alt="image-20230812124123590"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604674.png" alt="image-20230812124135465"></p>
<ul>
<li>b. 详细的实验结果:<ul>
<li>本文提出的滤波器修剪技术在VGG-16和ResNet-110模型上的性能进行了实验。结果显示，该技术在CIFAR10数据集上可以将推理成本降低高达34%（对于VGG-16）和38%（对于ResNet-110），同时保持接近原始准确性。即使是相对于AlexNet或VGGNet而言参数和推理成本较少的ResNets，也可以在不牺牲太多准确性的情况下实现30%的浮点运算（FLOP）的减少。此外，本文还发现，修剪第一层残差块比修剪第二层更有效地减少总体FLOP。最小滤波器修剪优于随机滤波器修剪和具有最大ℓ1范数的滤波器修剪。最小滤波器修剪还优于使用各种标准（如σmean-mean、σmean-ℓ1、σmean-ℓ2和σvar-ℓ2）的基于激活的特征图修剪。综上所述，本文提出的方法可以在不显著损失准确性的情况下，将VGGNet和深度ResNets的FLOP减少约30%。</li>
</ul>
</li>
</ul>
<h3 id="Q1-论文试图解决什么问题？-1"><a href="#Q1-论文试图解决什么问题？-1" class="headerlink" title="Q1 论文试图解决什么问题？"></a>Q1 论文试图解决什么问题？</h3><p>论文关注卷积神经网络（CNN）的计算和参数存储成本的增加问题。作者提出了一种剪枝方法，通过从CNN中剪除滤波器来加速计算，同时保持原始精度。</p>
<h3 id="Q2-这是否是一个新的问题？-1"><a href="#Q2-这是否是一个新的问题？-1" class="headerlink" title="Q2 这是否是一个新的问题？"></a>Q2 这是否是一个新的问题？</h3><p>该问题不是全新的。在卷积神经网络的训练和部署中，计算和存储效率一直是一个关键问题。过去已有研究人员尝试通过剪枝和压缩权重来减轻这些开销。</p>
<h3 id="Q3-这篇文章要验证一个什么科学假设？-1"><a href="#Q3-这篇文章要验证一个什么科学假设？-1" class="headerlink" title="Q3 这篇文章要验证一个什么科学假设？"></a>Q3 这篇文章要验证一个什么科学假设？</h3><p>文章的科学假设是，与基于权重大小的剪枝方法相比，通过剪除滤波器来减少计算成本更为有效，因为后者可以减少卷积层的计算成本，而不仅仅是全连接层。</p>
<h3 id="Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？-1"><a href="#Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？-1" class="headerlink" title="Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？"></a>Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h3><ul>
<li>早期的工作由Le Cun等人于1989年引入，提出了名为“Optimal Brain Damage”的剪枝方法，采用理论支持的显着性度量来剪枝权重。</li>
<li>Hassibi和Stork于1993年提出了“Optimal Brain Surgeon”，以根据二阶导数信息确定并去除不重要的权重。</li>
<li>Mariet和Sra（2016）通过识别不需要重新训练的神经元的子集来减少网络冗余。但是，这种方法仅在全连接层上操作，并引入稀疏连接。</li>
<li>过去的工作还提出了通过将权重矩阵表示为两个较小矩阵的低秩乘积来近似卷积操作，以降低卷积层的计算成本（参见Denil et al. (2013); Jaderberg et al. (2014); Zhang et al. (2015b;a); Tai et al. (2016); Ioannou et al. (2016)）。</li>
<li>其他减少卷积开销的方法包括使用基于FFT的卷积（Mathieu等人）。</li>
</ul>
<p>这些研究可以归类为：</p>
<ul>
<li>基于理论的权重剪枝方法</li>
<li>网络冗余减少</li>
<li>低秩近似和特殊卷积方法</li>
</ul>
<h3 id="Q5-论文中提到的解决方案之关键是什么？-1"><a href="#Q5-论文中提到的解决方案之关键是什么？-1" class="headerlink" title="Q5 论文中提到的解决方案之关键是什么？"></a>Q5 论文中提到的解决方案之关键是什么？</h3><p>这篇论文提出的解决方案的关键要素包括：</p>
<ol>
<li><strong>剪枝滤波器</strong>：该方法专注于从CNN中剪除对输出精度影响较小的滤波器。</li>
<li><strong>减少计算成本</strong>：通过一起移除整个滤波器及其连接的特征映射，可以显著降低计算成本。</li>
<li><strong>避免稀疏连接</strong>：与剪枝权重不同，此方法不会导致稀疏连接模式。因此，它不需要稀疏卷积库的支持，可以与现有的用于密集矩阵乘法的高效BLAS库一起工作。</li>
<li><strong>简单的剪枝技术</strong>：通过重新训练网络，即使是简单的滤波器剪枝技术也可以在几乎恢复到原始精度的同时，为VGG-16和ResNet-110等模型减少多达34%和38%的推理成本。</li>
</ol>
<p>这些关键要素共同实现了在保持精度的同时提高卷积神经网络的计算效率。</p>
<h3 id="Q6-论文中的实验是如何设计的？-1"><a href="#Q6-论文中的实验是如何设计的？-1" class="headerlink" title="Q6 论文中的实验是如何设计的？"></a>Q6 论文中的实验是如何设计的？</h3><p>实验部分描述了论文中进行的实验设计：</p>
<ol>
<li><strong>剪枝的网络类型</strong>：实验中剪枝了两种类型的网络：简单的CNN（例如VGG-16在CIFAR-10上）和残差网络（例如ResNet-56&#x2F;110在CIFAR-10上，ResNet-34在ImageNet上）。</li>
<li><strong>挑战</strong>：与常用于演示模型压缩的AlexNet或VGG（在ImageNet上）不同，VGG（在CIFAR-10上）和残差网络在全连接层中的参数较少。因此，从这些网络中剪去大量参数是具有挑战性的。</li>
<li><strong>实现</strong>：滤波器剪枝方法在Torch7中实现。剪枝滤波器后，将创建一个具有较少滤波器的新模型，并将修改层的剩余参数以及未受影响层的参数复制到新模型中。</li>
<li><strong>后续处理</strong>：如果剪除了卷积层，则后续批量归一化层的权重也会被移除。</li>
<li><strong>基线精度</strong>：为获得每个网络的基线精度，从头开始训练每个模型，并遵循与ResNet相同的预处理和超参数。</li>
<li><strong>重新训练</strong>：使用恒定学习速率进行重新训练，并减小剪枝后网络的学习速率。</li>
</ol>
<h3 id="Q7-用于定量评估的数据集是什么？代码有没有开源？-1"><a href="#Q7-用于定量评估的数据集是什么？代码有没有开源？-1" class="headerlink" title="Q7 用于定量评估的数据集是什么？代码有没有开源？"></a>Q7 用于定量评估的数据集是什么？代码有没有开源？</h3><p>用于定量评估的数据集包括CIFAR-10和ImageNet。开源</p>
<h3 id="Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？-1"><a href="#Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？-1" class="headerlink" title="Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？"></a>Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？</h3><p>实验结果部分给出了针对不同模型和剪枝方案的详细结果。以下是一些关键发现：</p>
<ul>
<li><strong>VGG-16剪枝</strong>：针对CIFAR-10数据集的VGG-16模型进行剪枝后，可以实现34.2%的FLOP（浮点操作数）剪减和64.0%的参数剪减，同时保持了类似的错误率（6.75%原始错误率与6.60%剪枝后错误率）。</li>
<li><strong>ResNet-56剪枝</strong>：针对CIFAR-10数据集的ResNet-56模型进行剪枝后，可以实现27.6%的FLOP剪减和13.7%的参数剪减，同时保持了相似的错误率。</li>
<li><strong>ResNet-110剪枝</strong>：针对CIFAR-10数据集的ResNet-110模型进行剪枝后，可以实现38.6%的FLOP剪减和32.4%的参数剪减，同时保持了相似的错误率。</li>
<li><strong>ResNet-34剪枝</strong>：针对ImageNet数据集的ResNet-34模型进行剪枝后，可以实现24.2%的FLOP剪减和10.8%的参数剪减，但错误率略有上升。</li>
</ul>
<p>这些实验结果支持了论文的科学假设，表明通过剪除对输出精度影响较小的滤波器，可以显著降低计算成本，同时基本保持原始精度。同时，该方法不导致稀疏连接模式，与现有的高效BLAS库兼容。</p>
<h3 id="Q9-这篇论文到底有什么贡献？-1"><a href="#Q9-这篇论文到底有什么贡献？-1" class="headerlink" title="Q9 这篇论文到底有什么贡献？"></a>Q9 这篇论文到底有什么贡献？</h3><p>这篇论文的主要贡献如下：</p>
<ol>
<li><strong>滤波器剪枝方法</strong>：提出了一种剪除CNN中滤波器的方法，以降低计算和存储成本，而不损失原始精度。</li>
<li><strong>有效的实现</strong>：该方法不引入稀疏连接模式，可以与现有的密集矩阵乘法库一起工作。</li>
<li><strong>广泛适用</strong>：该方法在VGG和ResNet等不同类型的网络上进行了验证。</li>
<li><strong>实验验证</strong>：通过广泛的实验，展示了该方法在不同网络和数据集上的有效性。</li>
</ol>
<h3 id="Q10-下一步呢？有什么工作可以继续深入？-1"><a href="#Q10-下一步呢？有什么工作可以继续深入？-1" class="headerlink" title="Q10 下一步呢？有什么工作可以继续深入？"></a>Q10 下一步呢？有什么工作可以继续深入？</h3><ol>
<li><strong>更复杂的剪枝策略</strong>：探索基于更复杂的标准和方法的剪枝策略，以实现更精确的滤波器选择。</li>
<li><strong>不同类型的网络</strong>：将方法扩展到更多种类和结构的卷积神经网络上。</li>
<li><strong>实时和嵌入式应用</strong>：针对实时和嵌入式应用的特定需求，优化和调整剪枝方法。</li>
<li><strong>与其他压缩技术结合</strong>：探索将滤波器剪枝与其他模型压缩和优化技术结合的可能性，以实现更全面的优化。</li>
</ol>
<h1 id="论文3：Filter-Pruning-via-Geometric-Median-for-Deep-Convolutional-Neural-Networks-Acceleration-基于几何中位数的深度卷积神经网络加速的滤波器剪枝"><a href="#论文3：Filter-Pruning-via-Geometric-Median-for-Deep-Convolutional-Neural-Networks-Acceleration-基于几何中位数的深度卷积神经网络加速的滤波器剪枝" class="headerlink" title="论文3：Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration (基于几何中位数的深度卷积神经网络加速的滤波器剪枝)"></a>论文3：Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration (基于几何中位数的深度卷积神经网络加速的滤波器剪枝)</h1><h1 id="Basic-Information-2"><a href="#Basic-Information-2" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration (基于几何中位数的深度卷积神经网络加速的滤波器剪枝)</li>
<li>Authors: Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, Yi Yang</li>
<li>Affiliation: CAI, University of Technology Sydney (悉尼科技大学)</li>
<li>Keywords: filter pruning, deep convolutional neural networks, acceleration, geometric median</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.00250">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/he-y/filter-pruning-geometric-median">GitHub</a></li>
<li>18年11月</li>
</ul>
<h1 id="论文简要-2"><a href="#论文简要-2" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125926446.png" alt="image-20230812125926446"></p>
<ul>
<li>上图1是典型的滤波器剪枝的方法。现有的方法大都是基于范数的标准进行剪枝。小范数不重要，认为滤波器的范数(p-norm)越小，携带的信息越少，于是对网络贡献越小，那么这些滤波器可以去掉而不会严重影响网络的性能。于是我们根据滤波器的p-norm进行重要性排序，规定一个阈值，并且删除p-norm小于阈值的滤波器。</li>
<li>这个浅蓝色区域是一种理想的滤波器范数分布，而基于这种范数标准的剪枝实际隐含了两个条件</li>
<li>:一是范数值的标准差尽可能的大，也就是分布相对稀疏，这样的目的是可以更容易的找到一个成值进行剪枝;</li>
<li>第二个条件是小范数的值应该尽可能的小，理想状况下是趋近于0，这样能保证小范数的滤波器对网络的贡献很小，去掉他对模型的影响不大。</li>
<li>本文提出了一种基于几何中位数的滤波器剪枝方法，用于压缩深度卷积神经网络模型，无需满足之前方法中的两个要求，通过在两个图像分类基准上的实验证明了其有效性和优势。</li>
</ul>
<h1 id="背景信息-2"><a href="#背景信息-2" class="headerlink" title="背景信息:"></a>背景信息:</h1><p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812130521561.png" alt="image-20230812130521561"></p>
<p>如这两幅图所示，浅蓝色区域仍是理想情况下的范数分布，绿色区域是实际的滤波器范数分布。a名我们看到绿色区域这种情况下的分布范数的标准差太小，这样出现的问题是很多滤波器有相似的重要性，很难确定一个合适的阈值(A smallnorm deviation leads to a small search space, which makes it diffcult to find an appropriate threshold to select fifilters toprune.)</p>
<p>b图我们看到最小范数的值仍然很大，也就是说他对网络的贡献度不能被忽视。</p>
<p>作者为了证明在有些场景中上面提到的两个条件并不能很好的满足，做了非常充分的实验证明，分别用resnet110在cifar10和resnet18在ILSVRC2012数据集上进行实验:</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812130609383.png" alt="image-20230812130609383"></p>
<p>绿色小竖线表示范数的观察值，蓝色线表示核的分布预测。b图我们看到在resnet110的第一层有大量的滤波器，它们的范数集中在10x6的量级上。c图展示滤波器的范数范围跨度只有0.3，远远小于a图第一层范数1.7的跨度，这说明同一个网络不同层的滤波器范数有的标差较大，有的就偏小。同样的看g图，resnet18最后一层的滤波器最小范数大约是0.8，与e图展示的第一层比起来并没有接近0，通过分析实际的预训练网络，发现这些网络确实出现了上述的情况。</p>
<ul>
<li>论文背景: 深度卷积神经网络的深度和宽度使得计算成本昂贵，难以在移动设备上部署。因此，有必要降低计算成本但保持高准确性的深度卷积神经网络模型。</li>
<li>过去方案: 过去的滤波器剪枝方法通常使用“较小范数-较不重要”准则来剪枝具有较小范数值的滤波器。然而，这种准则的有效性取决于两个要求：滤波器范数的偏差应该很大，滤波器的最小范数应该很小。然而，这两个要求并不总是满足的。</li>
<li>论文的Motivation: 鉴于过去方法的局限性，本文提出了一种新的滤波器剪枝方法，名为Filter Pruning via Geometric Median (FPGM)，通过剪枝具有冗余信息的滤波器来压缩CNN模型。与之前的方法不同，FPGM选择具有最可替代贡献的滤波器进行剪枝，而不是剪枝相对不重要的滤波器。通过计算同一层中滤波器的几何中位数，FPGM可以用其余滤波器来表示附近的滤波器，因此剪枝这些滤波器不会对模型性能产生重大负面影响。与基于范数的准则不同，FPGM的性能不会因为无法满足范数准则的要求而下降。</li>
</ul>
<h1 id="方法-2"><a href="#方法-2" class="headerlink" title="方法:"></a>方法:</h1><h3 id="基于几何中位数的滤波器剪枝方法"><a href="#基于几何中位数的滤波器剪枝方法" class="headerlink" title="基于几何中位数的滤波器剪枝方法"></a>基于几何中位数的滤波器剪枝方法</h3><p>作者摒弃了这种基于范数标准剪枝的方法，提出了一种基于几何中心的滤波器评价指标。在某一层中，从全局考虑所有filter的关系。<br>这种方法的提出灵感来源于几何中心的思想。什么是几何中心?就是在一个d维的空间中，给定一个点集，a1.a2.a3.….an，在该空间中找到一个点x*，使得该点到各个点的距离和最小，就是这个d维空间的几何中心。</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131322763.png" alt="image-20230812131322763"></p>
<p>基本思想:几何中心是对于欧几里得空间的点的中心的一个估计。受此启发，作者把滤波器抽象成欧氏空间中的点，对于网络中的每一层，在该层的滤波器空间中，计算GM，也就是第1层的数据中心。如果某个滤波器接近于这个GM，可以认为这个源波器的信息跟其他源波器重合，甚至是几余的，于是可以去掉这个源波器而不对网络产生大的影响。去掉它后它的功能可以被其他滤波器代替。沿用求取普通几何中心的思想，</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131413844.png" alt="image-20230812131413844"></p>
<p>归根结底，作者是想在某一层中找到与几何中心xGM相近的滤波器，然后将他们剪枝。为了方便求解，作者假设第层的几何中心就位于第层的滤波器中，因为真实目的是要找到与几何中心最近的滤波器。此时(2)的优化问题就变为，从第层中所有的滤波器中寻找一个filter,确保其与剩余的filters的距离之和最小。</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131533751.png" alt="image-20230812131533751"></p>
<p>公式5-9看不懂</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131557560.png" alt="image-20230812131557560"></p>
<p>算法流程如下：</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131517584.png" alt="image-20230812131517584"></p>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ul>
<li><strong>训练数据</strong>：( X )。</li>
<li><strong>剪枝率</strong>：( P_i )。剪枝率确定了每一层应剪去的滤波器数量的百分比。</li>
</ul>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><ol>
<li><strong>模型参数初始化</strong>：模型的权重参数被初始化为( W &#x3D; {W(i), 0 \leq i \leq L} )，其中( L )是网络的层数。</li>
</ol>
<h3 id="训练和剪枝过程"><a href="#训练和剪枝过程" class="headerlink" title="训练和剪枝过程"></a>训练和剪枝过程</h3><ol start="2">
<li><strong>开始训练周期</strong>：对于每个训练周期（从1到最大训练周期），执行以下步骤：<ul>
<li><strong>更新模型参数</strong>：根据训练数据( X )更新模型参数( W )。这一步通常涉及正向传播、损失计算、反向传播和权重更新。</li>
<li><strong>逐层剪枝</strong>：对于每一层（从1到( L )），执行以下剪枝步骤：<ul>
<li><strong>找到要剪去的滤波器</strong>：找到满足方程4的( N_{i+1} \times P_i )个滤波器。方程4通常涉及使用几何中位数来确定要剪去的滤波器。</li>
<li><strong>零化选定的滤波器</strong>：将选定的滤波器设置为零，从而实现剪枝。</li>
</ul>
</li>
<li><strong>重复以上步骤</strong>：继续训练和剪枝，直到达到最大训练周期。</li>
</ul>
</li>
</ol>
<h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><ol start="3">
<li><strong>获取紧凑模型</strong>：从最终的权重参数( W )中获取紧凑模型( W^* )。紧凑模型不包括已剪去（零化）的滤波器。</li>
</ol>
<h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><ul>
<li><strong>输出紧凑模型及其参数</strong>：最终剪枝后的模型及其参数( W^* )。</li>
</ul>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>基于几何中位数的滤波器剪枝（FPGM）是一个迭代过程，其中训练和剪枝在每个训练周期内交替进行。通过这种方式，算法可以在训练过程中逐渐剪去滤波器，从而在保持网络性能的同时减少模型大小。最终的紧凑模型包括剪枝后保留的所有滤波器和参数。</p>
<ul>
<li>a. 理论背景:<ul>
<li>本文提出了一种新的滤波器修剪方法，称为几何中位数滤波器修剪（FPGM），用于压缩深度卷积神经网络模型。与以往基于范数值修剪滤波器的方法不同，FPGM通过修剪冗余的滤波器来实现。在两个图像分类基准测试上的实验结果表明，FPGM在最小精度损失的情况下实现了显著的模型压缩。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>本文提出的FPGM方法通过计算同一层内滤波器的几何中位数（GM），并修剪与几何中位数最远的滤波器。与以往使用范数值作为标准的方法不同，FPGM不依赖于滤波器范数来选择修剪的滤波器。</li>
</ul>
</li>
</ul>
<h1 id="结果-2"><a href="#结果-2" class="headerlink" title="结果:"></a>结果:</h1><p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125737860.png" alt="image-20230812125737860"></p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125805978.png" alt="image-20230812125805978"></p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125815070.png" alt="image-20230812125815070"></p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>本文在两个基准测试（CIFAR-10和ILSVRC-2012）上评估了FPGM算法。CIFAR-10数据集包含10个不同类别的60,000个32x32彩色图像，其中包括50,000个训练图像和10,000个测试图像。ILSVRC-2012数据集是一个大规模数据集，包含1.28百万个训练图像和50,000个验证图像，共有1,000个类别。实验设置包括指定两个数据集的训练设置，包括参数设置和训练计划。修剪设置涉及为每个层设置修剪率，确定要修剪的滤波器数量。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>实验结果显示了FPGM的有效性和效率。在应用于CIFAR-10和ILSVRC-2012两个图像分类基准测试时，FPGM在显著减少模型FLOPs的同时，几乎没有损失精度。例如，在CIFAR-10上，FPGM在ResNet-110上减少了超过52%的FLOPs，并且相对精度提高了2.69%。在ILSVRC-2012上，FPGM在ResNet-101上减少了超过42%的FLOPs，而top-5精度没有下降。</li>
</ul>
</li>
</ul>
<h3 id="Q1-论文试图解决什么问题？-2"><a href="#Q1-论文试图解决什么问题？-2" class="headerlink" title="Q1 论文试图解决什么问题？"></a>Q1 论文试图解决什么问题？</h3><p>这篇论文试图解决深度卷积神经网络（CNN）的滤波器剪枝问题。以前的工作使用了“小范数-次要重要”的准则来剪去具有较小范数值的滤波器。本文分析了基于范数的准则，并指出其有效性取决于两个并不总是满足的要求：滤波器的范数偏差应该大，滤波器的最小范数应该小。为了解决这个问题，论文提出了一种名为“基于几何中位数的滤波器剪枝”（FPGM）的新方法。</p>
<h3 id="Q2-这是否是一个新的问题？-2"><a href="#Q2-这是否是一个新的问题？-2" class="headerlink" title="Q2 这是否是一个新的问题？"></a>Q2 这是否是一个新的问题？</h3><p>滤波器剪枝并不是一个全新的问题，已有许多工作致力于深度神经网络的压缩和加速。然而，本文提出了一种新的剪枝方法，并分析了以前基于范数的剪枝准则的局限性。</p>
<h3 id="Q3-这篇文章要验证一个什么科学假设？-2"><a href="#Q3-这篇文章要验证一个什么科学假设？-2" class="headerlink" title="Q3 这篇文章要验证一个什么科学假设？"></a>Q3 这篇文章要验证一个什么科学假设？</h3><p>这篇文章的科学假设是，通过使用几何中位数作为剪枝准则，可以剪去冗余的滤波器，从而压缩CNN模型，无论滤波器的范数偏差大小或滤波器的最小范数大小如何。这种方法的目的是克服基于范数的剪枝方法的局限性。</p>
<h3 id="Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？-2"><a href="#Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？-2" class="headerlink" title="Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？"></a>Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h3><p>相关的研究主要集中在加速卷积神经网络（CNN）的方法上，可以大致分为四个类别：</p>
<ol>
<li><strong>矩阵分解</strong>：使用数学方法分解矩阵以降低计算复杂度。</li>
<li><strong>低精度权重</strong>：使用低精度数值表示权重以减少存储和计算需求。</li>
<li><strong>知识蒸馏</strong>：通过训练较小的模型来模仿大模型的行为。</li>
<li><strong>剪枝</strong>：移除神经网络中不必要的连接。剪枝方法可以进一步分为权重剪枝和滤波器剪枝。其中，滤波器剪枝不仅减少了设备上的存储使用，还降低了计算成本以加速推理。</li>
</ol>
<h3 id="Q5-论文中提到的解决方案之关键是什么？-2"><a href="#Q5-论文中提到的解决方案之关键是什么？-2" class="headerlink" title="Q5 论文中提到的解决方案之关键是什么？"></a>Q5 论文中提到的解决方案之关键是什么？</h3><p>论文提出了一种名为“基于几何中位数的滤波器剪枝”（FPGM）的新方法。关键之处在于：</p>
<ul>
<li><strong>基于几何中位数</strong>：使用几何中位数作为剪枝准则，可以剪去冗余的滤波器。</li>
<li><strong>克服范数局限性</strong>：与基于范数的方法不同，FPGM无需满足滤波器的范数偏差大或最小范数小的要求。</li>
<li><strong>压缩和加速</strong>：该方法旨在压缩CNN模型并加速推理过程。</li>
</ul>
<h3 id="Q6-论文中的实验是如何设计的？-2"><a href="#Q6-论文中的实验是如何设计的？-2" class="headerlink" title="Q6 论文中的实验是如何设计的？"></a>Q6 论文中的实验是如何设计的？</h3><p>论文的实验部分评估了基于几何中位数的滤波器剪枝（FPGM）方法在不同类型的网络和数据集上的效果。以下是实验设计的关键部分：</p>
<ul>
<li><p><strong>网络结构</strong>：对单分支网络（VGGNet）和多分支网络（ResNet）进行了评估。</p>
</li>
<li><p><strong>数据集</strong>：实验在两个基准数据集上进行，即CIFAR-10和ILSVRC-2012。</p>
</li>
<li><p>训练设置</p>
<p>：</p>
<ul>
<li>在CIFAR-10上，参数设置与先前的工作相同。</li>
<li>在ILSVRC-2012上，使用默认参数设置，并与PyTorch官方示例中的数据增强策略相同。</li>
</ul>
</li>
<li><p><strong>剪枝设置</strong>：分析了从头开始训练的模型与预训练模型的差异。对于从头开始的模型，使用正常的训练计划，无需额外的微调。</p>
</li>
</ul>
<h3 id="Q7-用于定量评估的数据集是什么？代码有没有开源？-2"><a href="#Q7-用于定量评估的数据集是什么？代码有没有开源？-2" class="headerlink" title="Q7 用于定量评估的数据集是什么？代码有没有开源？"></a>Q7 用于定量评估的数据集是什么？代码有没有开源？</h3><p>用于定量评估的数据集是CIFAR-10和ILSVRC-2012。开源</p>
<h3 id="Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？-2"><a href="#Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？-2" class="headerlink" title="Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？"></a>Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？</h3><p>成功评估基于几何中位数的滤波器剪枝方法的有效性和效率</p>
<h3 id="Q9-这篇论文到底有什么贡献？-2"><a href="#Q9-这篇论文到底有什么贡献？-2" class="headerlink" title="Q9 这篇论文到底有什么贡献？"></a>Q9 这篇论文到底有什么贡献？</h3><p>这篇论文的主要贡献如下：</p>
<ol>
<li><p><strong>分析现有方法的局限性</strong>：分析了基于范数的剪枝准则的局限性，并提出了两个不总是满足的要求。</p>
</li>
<li><p><strong>提出新的剪枝方法</strong>：介绍了一种新的滤波器剪枝方法，即基于几何中位数的滤波器剪枝（FPGM），旨在压缩CNN模型并加速推理。</p>
</li>
<li><p><strong>实验验证</strong>：通过在多个网络结构和数据集上的实验，评估了所提方法的有效性和效率。</p>
<h3 id="Q10-下一步呢？有什么工作可以继续深入？-2"><a href="#Q10-下一步呢？有什么工作可以继续深入？-2" class="headerlink" title="Q10 下一步呢？有什么工作可以继续深入？"></a>Q10 下一步呢？有什么工作可以继续深入？</h3><ol>
<li><strong>扩展到其他网络结构</strong>：将FPGM方法扩展到其他复杂的网络结构，并评估其在不同任务和领域中的适用性。</li>
<li><strong>与其他压缩技术的结合</strong>：探讨如何将FPGM与其他模型压缩和加速技术（如量化、知识蒸馏等）结合使用。</li>
</ol>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3/" data-id="cllkmcmf5000gksu7f0li9ao8" data-title="8月第二周剪枝相关" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AF%86%E9%9B%86%E8%BF%9E%E6%8E%A5%E3%80%81%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%89%AA%E6%9E%9D/" rel="tag">密集连接、滤波器剪枝</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-8月第一周自然语言蒸馏" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%92%B8%E9%A6%8F/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T07:58:35.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%92%B8%E9%A6%8F/">8月第一周自然语言大模型蒸馏</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="8月第一周自然语言蒸馏论文"><a href="#8月第一周自然语言蒸馏论文" class="headerlink" title="8月第一周自然语言蒸馏论文"></a>8月第一周自然语言蒸馏论文</h1><h1 id="Title-Knowledge-Distillation-of-Large-Language-Models-大型语言模型的知识蒸馏"><a href="#Title-Knowledge-Distillation-of-Large-Language-Models-大型语言模型的知识蒸馏" class="headerlink" title="Title: Knowledge Distillation of Large Language Models (大型语言模型的知识蒸馏)"></a>Title: Knowledge Distillation of Large Language Models (大型语言模型的知识蒸馏)</h1><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Knowledge Distillation of Large Language Models (大型语言模型的知识蒸馏)</li>
<li>Authors: Yuxian Gu, Li Dong, Furu Wei, Minlie Huang</li>
<li>Affiliation: The CoAI Group, Tsinghua University (清华大学)</li>
<li>Keywords: Knowledge Distillation, Large Language Models, Generative Language Models, Reverse Kullback-Leibler Divergence, Optimization Approach</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.08543v1">Paper</a>, <a target="_blank" rel="noopener" href="https://aka.ms/MiniLLM">GitHub Code</a></li>
</ul>
<h1 id="论文简要-第一篇自然语言蒸馏-bert蒸馏？"><a href="#论文简要-第一篇自然语言蒸馏-bert蒸馏？" class="headerlink" title="论文简要 :第一篇自然语言蒸馏 bert蒸馏？"></a>论文简要 :第一篇自然语言蒸馏 bert蒸馏？</h1><ul>
<li>本文提出了一种名为MINILLM的方法，用于从生成型大型语言模型中蒸馏出较小的语言模型。通过使用逆向Kullback-Leibler散度作为目标函数，并引入有效的优化方法，MINILLM模型在指令跟随任务中表现出更高的生成质量、更低的暴露偏差、更好的校准性和更高的长文本生成性能。</li>
</ul>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 随着大型语言模型的快速发展，知识蒸馏（KD）成为减少计算资源需求的一种常用技术。然而，过去的知识蒸馏方法主要应用于白盒分类模型或训练小模型以模仿ChatGPT等黑盒模型API。如何有效地从白盒生成型语言模型中蒸馏知识仍然未被充分探索，而随着生成型语言模型的繁荣，这变得越来越重要。</li>
<li>过去方案: 过去的知识蒸馏方法主要应用于白盒分类模型或训练小模型以模仿黑盒模型API。白盒知识蒸馏方法主要研究了小型语言理解模型，而对于生成型语言模型的白盒知识蒸馏尚未被探索。</li>
<li>论文的Motivation: 随着生成型语言模型的兴起，白盒知识蒸馏对于研究社区和行业部门变得更有价值，因为学生模型可以从白盒教师模型中获得更好的信号，从而潜在地实现更高的性能。然而，白盒知识蒸馏方法主要研究了小型语言理解模型，而对于生成型语言模型的白盒知识蒸馏尚未被探索。因此，本文旨在研究生成型语言模型的白盒知识蒸馏，并提出了一种新的方法MINILLM，通过使用逆向Kullback-Leibler散度作为目标函数，实现了更好的生成质量和性能。</li>
</ul>
<h1 id="方法-创新点1：正则化整体样本"><a href="#方法-创新点1：正则化整体样本" class="headerlink" title="方法: 创新点1：正则化整体样本"></a>方法: 创新点1：正则化整体样本</h1><h2 id="可能的工作：在视觉蒸馏上使用正则化减少过拟合增加泛化能力，针对序列蒸馏改进"><a href="#可能的工作：在视觉蒸馏上使用正则化减少过拟合增加泛化能力，针对序列蒸馏改进" class="headerlink" title="可能的工作：在视觉蒸馏上使用正则化减少过拟合增加泛化能力，针对序列蒸馏改进"></a>可能的工作：在视觉蒸馏上使用正则化减少过拟合增加泛化能力，针对序列蒸馏改进</h2><p>可能的工作2：法学大模型对垂直领域大模型建设，提出了一个新的方法，包括微调，提示词，langchain，本文提到的蒸馏</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308061123535.png" alt="image-20230806112311078"></p>
<blockquote>
<ol>
<li>序列级知识蒸馏（KD）(左图): 这种方法强迫学生模型直接从教师模型生成的序列中学习。前向 KLD (Kullback-Leibler Divergence，库尔巴克-莱布勒散度)用作衡量教师输出和学生输出分布之间的不相似度。<strong>这种方法就像是死记硬背，学生模型本质上是试图复制教师的确切输出序列。</strong>提示 𝒙 输入到两个模型中，然后计算教师输出 𝒚 (从 𝑝 中采样)和学生预测输出之间的差异，以调整学生模型参数（𝜃）。然而，这种方法可能导致学生在语言生成任务中过度估计某些区域，特别是当学生模型缺乏捕获所有教师模型输出分布细微差异的能力时。</li>
<li>MINILLM (右图): 相比之下，这种方法允许学生模型通过教师模型的反馈来改善其生成能力。与单纯模仿教师输出不同，这种方法旨在帮助学生模型更好地泛化。这是通过最小化反向 KLD 来实现的，本质上是使学生模型的输出分布尽可能地接近教师的，但这样可以使学生模型也能从自己的错误中学习。<strong>提示 𝒙 输入到两个模型中，然后计算教师输出 (从 𝑞! 中采样)和学生预测输出之间的差异，为学生模型提供学习的机会。</strong></li>
</ol>
</blockquote>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308061238004.png" alt="image-20230806123825947"></p>
<blockquote>
<p>输入包括以下元素：</p>
<ul>
<li>条件生成数据集D，包含提示和真实响应</li>
<li>预训练语料库DPT，包含长文档纯文本</li>
<li>教师模型，其输出分布为p</li>
<li>初始学生模型，其输出分布为qθ0</li>
</ul>
<p>输出为一个学生模型，其输出分布为qθ。</p>
<p>这个算法的步骤如下：</p>
<ol>
<li>首先在数据集D上对学生模型进行微调，并选择具有最低验证损失的模型θ。</li>
<li>重复以下步骤直到收敛：<ul>
<li>从数据集D中抽取一批提示，并从其中收集响应以得到S &#x3D; {(xm, ym)}M m&#x3D;1。</li>
<li>从DPT中抽取一个小批量D’PT &#x3D; {dm}M m&#x3D;1。</li>
<li>计算主损失梯度(∇J)Main，这个值取决于学生模型qθ和教师模型pe之间的概率分布差异，详见等式5和等式6。</li>
<li>计算正则化损失梯度(∇J)Reg，这个值是在所有可能的输出yt中，学生模型qθ和教师模型p的分布差异的梯度，详见等式3。</li>
<li>计算预训练损失的梯度∇LPT，这个值是学生模型qθ在预训练语料库D’PT上的对数概率的梯度。</li>
<li>更新模型参数θ，根据计算出的梯度值，来调整模型的参数。</li>
</ul>
</li>
</ol>
<p>这个算法的核心思想是在模型参数更新过程中引入了一个正则化项，旨在减小学生模型和教师模型在相同输入下的输出分布差异，从而使学生模型更好地模仿教师模型。此外，为了保持模型的泛化能力，还计算了在预训练语料库上的损失，这有助于防止模型过度拟合教师模型的输出。</p>
</blockquote>
<p>步骤：</p>
<p>在这篇论文的实验设置中，研究者们首先在指令-响应数据集 D 上对一个大模型进行微调，将其作为教师模型 p。然后，他们比较了不同的知识蒸馏方法，以在教师模型的指导下对学生模型进行蒸馏，并通过评估蒸馏模型的指令执行性能来进行比较。</p>
<p>他们蒸馏了三种不同大小的模型：GPT-2，OPT和LLaMA，并分别使用GPT-2-1.5B，OPT-13B和LLaMA-13B作为各模型类型的教师模型。他们还使用GPT-J作为教师模型的结果。</p>
<p>他们从 databricks-dolly-15k 数据集中构建了训练数据，该数据集由 15K个人工编写的指令-响应对组成。他们随机分配了 14K个样本作为训练集 D，并留下了 500个样本用于验证和测试。对于 DPT，他们为 GPT-2 系列使用了 OpenWebText，而对于其他模型，他们使用了 RoBERTa 训练语料库。</p>
<p>实验中使用了两个指标来评估模型生成的响应：Rouge-L 分数和GPT-4反馈。</p>
<p>他们在主要实验中考虑了三个基线：</p>
<ol>
<li>SFT w&#x2F;o KD 直接在 D 上对学生模型进行微调，使用金色响应进行监督。</li>
<li>KD 使用教师分布作为每个标记步骤的监督，对 D 上的学生模型进行微调，也称为单词级别的 KD。</li>
<li>SeqKD 在教师生成的数据上对学生模型进行微调。</li>
</ol>
<p>总结实验步骤：</p>
<ol>
<li>在指令-响应数据集上微调大模型作为教师模型。</li>
<li>对多种大小的学生模型进行知识蒸馏，并评估其在教师模型指导下在数据集上的性能。</li>
<li>从 databricks-dolly-15k 数据集中构建训练数据。</li>
<li>使用 OpenWebText 和 RoBERTa 训练语料库进行预训练。</li>
<li>使用 Rouge-L 和 GPT-4 反馈对模型生成的响应进行评估。</li>
<li>比较与直接微调学生模型（SFT w&#x2F;o KD）、单词级别知识蒸馏（KD）以及在教师生成数据上微调学生模型（SeqKD）等基线方法的性能。</li>
<li>报告结果并进行分析。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308061243956.png" alt="image-20230806124343892"></p>
<ul>
<li>a. 理论背景:<ul>
<li>本文讨论了使用知识蒸馏（KD）来减少大型语言模型（LLM）的计算需求。先前的KD方法主要集中在白盒分类模型或训练小模型以模仿黑盒模型API，而作者提出了一种名为MINILLM的方法，从生成性更大的语言模型中提炼出较小的语言模型。作者认为标准的KD目标对于生成性LLM来说是次优的，并提出了最小化反向Kullback-Leibler散度（KLD）的方法。他们还引入了一种优化方法来学习这个目标。实验结果表明，MINILLM模型生成的响应更精确，整体质量更高，暴露偏差更低，校准性更好，并具有更高的长文本生成性能。该方法适用于不同模型家族，参数大小范围从120M到13B。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>MINILLM方法用于语言模型（LLM）中的知识蒸馏（KD）。与序列级KD不同，MINILLM专注于最小化反向Kullback-Leibler散度（KLD），并鼓励学生模型在其自身能力范围内生成教师模型偏好的样本。使用策略梯度定理进行优化，通过从学生模型中进行采样来计算目标函数的梯度。为了解决高方差和奖励欺骗等问题，提出了三种策略：单步正则化、教师混合采样和长度归一化。提供了MINILLM的训练算法，包括在数据集上微调学生模型、采样提示和响应以及计算梯度。还描述了实验设置和评估指标。</li>
</ul>
</li>
</ul>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><ul>
<li>a. 详细的实验设置:<ul>
<li>表1中的评估结果显示，标准的知识蒸馏（KD）方法，SFT w&#x2F;o KD、KD和SeqKD，成功地从教师模型中提取了知识，实现了更好的Rouge-L和GPT-4反馈分数。然而，MINILLM方法在几乎所有情况下都优于基线方法，表明其在提取具有高整体性能的小模型方面的有效性。MINILLM还生成了与真实值具有高重叠度的更精确的响应，并展示了良好的超出分布的泛化能力。MINILLM的改进在不同的模型大小和家族中保持一致，证明了其在大型语言模型（LLM）时代的可扩展性和泛化能力。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>实验结果显示，微调模型的ExAccErr在生成过程中不断增加，而MiniLLM具有较低的ExAccErr，并且错误在长文本生成中停止累积。在两个文本分类数据集SST2和BoolQ上的校准测试显示，使用KD和SeqKD训练的模型与教师模型相比校准性较差。另一方面，MINILLM缩小了学生和教师之间ECE分数的差距。模型的性能根据真实响应的长度而变化，在期望短响应的提示上，所有方法都获得了较低的分数。MINILLM通过保持生成响应中不同的4-gram比例来保持生成多样性，并且不会在测试集上导致语言建模损失的显著增加。消融研究表明，教师混合采样和长度归一化对于稳定训练至关重要，而单步正则化减少了训练过程的方差。在教师混合探索中，α值的选择会影响性能，一般而言，α &#x3D; 0.2是合适的。添加预训练损失有助于在规范NLP任务上保持能力，而不会对指令遵循任务的性能产生显著影响。</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%92%B8%E9%A6%8F/" data-id="cllkmcmf6000iksu7egm425wk" data-title="8月第一周自然语言大模型蒸馏" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-7月第四周周报依赖图通用剪枝" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%91%A8%E6%8A%A5%E4%BE%9D%E8%B5%96%E5%9B%BE%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T07:57:10.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%91%A8%E6%8A%A5%E4%BE%9D%E8%B5%96%E5%9B%BE%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/">7月第四周周报依赖图通用剪枝</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="7月第四周依赖图论文通用剪枝"><a href="#7月第四周依赖图论文通用剪枝" class="headerlink" title="7月第四周依赖图论文通用剪枝"></a>7月第四周依赖图论文通用剪枝</h1><h1 id="论文1-DepGraph-Towards-Any-Structural-Pruning"><a href="#论文1-DepGraph-Towards-Any-Structural-Pruning" class="headerlink" title="论文1 DepGraph: Towards Any Structural Pruning"></a>论文1 DepGraph: Towards Any Structural Pruning</h1><h1 id="DepGraph：面向任何结构修剪"><a href="#DepGraph：面向任何结构修剪" class="headerlink" title="DepGraph：面向任何结构修剪"></a>DepGraph：面向任何结构修剪</h1><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: DepGraph: Towards Any Structural Pruning (DepGraph: 通向任意结构剪枝)</li>
<li>Authors: Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, Xinchao Wang</li>
<li>Affiliation: National University of Singapore, Zhejiang University, Huawei Technologies Ltd.</li>
<li>Keywords: structural pruning, neural networks, dependency graph, automatic method, generalizability</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.12900v2">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/VainF/Torch-Pruning">GitHub</a></li>
</ul>
<h1 id="论文简要"><a href="#论文简要" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li><p>提出了一种非深度图算法DepGraph，实现了架构通用的结构化剪枝，适用于CNNs, Transformers, RNNs, GNNs等网络。DepGraph能够自动地分析复杂的结构耦合，从而正确地移除参数实现网络加速。基于DepGraph算法，我们开发了PyTorch结构化剪枝框架 <a href="https://link.zhihu.com/?target=https://github.com/VainF/Torch-Pruning">Torch-Pruning</a>。不同于依赖Masking实现的“模拟剪枝”，该框架能够实际地移除参数和通道，降低模型推理成本。在DepGraph的帮助下，研究者和工程师无需再与复杂的网络结构斗智斗勇，可以轻松完成复杂模型的一键剪枝。</p>
<p>论文标题：DepGraph: Towards Any Structural Pruning<br>论文链接：<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2301.12900">https://arxiv.org/abs/2301.12900</a><br>项目地址：<a href="https://link.zhihu.com/?target=https://github.com/VainF/Torch-Pruning">https://github.com/VainF/Torch-</a></p>
</li>
</ul>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 近年来，边缘计算应用的出现使得深度神经网络压缩变得必要。剪枝作为一种网络压缩方法已被证明是高效且实用的。然而，现有的剪枝方法往往依赖于手动设计的分组方案，限制了其在新的网络架构上的通用性。</li>
<li>过去方案: 传统的剪枝方法可以大致分为结构剪枝和非结构剪枝两种。结构剪枝通过物理上移除参数来改变神经网络的结构，而非结构剪枝则通过将特定权重置零来实现。然而，现有的结构剪枝方法往往需要针对不同网络进行个案分析，且无法直接推广到其他网络架构，限制了其在实际应用中的使用。</li>
<li>论文的Motivation: 鉴于现有结构剪枝方法的局限性，本文旨在提出一种通用的剪枝方案，能够适用于任意网络架构。为了解决结构剪枝中的依赖性问题，作者引入了Dependency Graph (DepGraph)方法，通过显式建模不同层之间的依赖关系，实现了全自动的剪枝过程。通过在多个网络架构上的实验证明了DepGraph的有效性和通用性。</li>
</ul>
<h1 id="论文摘要"><a href="#论文摘要" class="headerlink" title="论文摘要"></a>论文摘要</h1><blockquote>
<p>结构剪枝能够通过从神经网络中移除结构性分组的参数来加速模型。然而，不同模型中的参数分组模式各不相同，这使得依赖于手动设计的分组方案的特定架构的剪枝器无法泛化到新的架构。在这项工作中，我们研究了一个极具挑战性但鲜少被探索的任务，即<strong>任意结构剪枝</strong>，以解决诸如CNNs、RNNs、GNNs和Transformers等任意架构的通用结构剪枝。实现这个目标的最大难题在于<strong>结构耦合</strong>，它不仅强制不同的层同时被剪枝，还期望所有被移除的参数在重要性上保持一致，从而避免剪枝后出现结构问题和显著的性能下降。为解决这个问题，我们提出了一种通用且全自动的方法，依赖图（Dependency Graph，DepGraph），用以明确地建模层与层之间的依赖关系，以及为剪枝全面地组合耦合参数。在这项工作中，我们对多种架构和任务进行了广泛的评估，包括用于图像的ResNe(X)t、DenseNet、MobileNet和Vision transformer，用于图的GAT，用于3D点云的DGCNN，以及用于语言的LSTM，并表明，即使采用简单的基于规范的判别准则，所提出的方法也能够持续产生令人满意的性能。</p>
</blockquote>
<blockquote>
<ol>
<li>什么是结构耦合（structural coupling），并且它如何影响神经网络的剪枝？</li>
<li>结构耦合是指<strong>神经网络中不同层之间的关联性</strong>，这种关联性强制同时剪枝不同的层，同时也期望所有被剪枝的参数都一致地不重要，这样可以避免剪枝后出现结构问题和明显的性能下降。具体来说，如果一个参数被剪枝，那么它的耦合参数也应该被剪枝，以保持网络的结构完整性。</li>
</ol>
</blockquote>
<h2 id="图片1：不同神经网络模型结构耦合和剪枝策略的对比"><a href="#图片1：不同神经网络模型结构耦合和剪枝策略的对比" class="headerlink" title="图片1：不同神经网络模型结构耦合和剪枝策略的对比"></a>图片1：不同神经网络模型结构耦合和剪枝策略的对比</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221403214.png" alt="image-20230722140353693"></p>
<blockquote>
<p>a: 在”a”部分（CNNs）中，例如，为了剪枝Conv2，块内的所有其他层（如Conv1, BN1, BN2）也必须被剪枝。这强调了在剪枝过程中，如果要剪枝某个层，则必须同时剪枝块内的所有其他层。</p>
<p>b: b部分中，如果要对Transform的MLP模块进行剪枝，也就必须要对Norm和多头注意力进行剪枝，这种情况的出现是由于神经网络中的结构耦合。在神经网络中，不同层的参数是彼此依赖的，这种依赖关系迫使一旦一个层（如MLP）被剪枝，那么它的耦合层（如Norm和多头注意力机制）也必须被剪枝。各层之间的信息流动是相互依赖的。如果你剪掉了一个层，那么它的输出就不再能够作为下一层的输入。因此，下一层也就失去了它的输入源，相当于也被”剪掉”了。而这些层可能又是其他层的输入源，因此，剪枝一个层可能会导致多个层被剪掉。这就是为什么在剪枝MLP的同时，必须剪掉Norm和多头注意力机制。</p>
<p>c 一样的道理 RNNS</p>
<p>d GNNS  进行剪枝时，不仅要考虑节点或边的重要性，还需要考虑剪枝操作对整个网络结构的影响。例如，如果剪枝的节点或边是信息传播路径的一部分，那么剪枝可能会阻断信息的流动，从而影响网络的性能。</p>
</blockquote>
<h1 id="相关工作部分"><a href="#相关工作部分" class="headerlink" title="相关工作部分"></a>相关工作部分</h1><blockquote>
<ol>
<li><strong>结构剪枝和非结构剪枝</strong>：结构剪枝的目标是物理上移除一组参数，从而减少神经网络的大小。相比之下，非结构剪枝涉及将特定的权重置零，但不改变网络结构。非结构剪枝在实践中易于实施，并且本质上能够适应各种网络。然而，它通常需要专门的AI加速器或软件进行模型加速。相反，结构剪枝通过从网络中物理上移除参数来改善推理开销，从而在更广泛的应用领域中找到应用。</li>
<li><strong>剪枝分组参数</strong>：在复杂的网络结构中，参数组之间可能存在依赖性，需要同时进行剪枝。例如，当剪枝两个连续的卷积层时，从第一层移除一个过滤器需要剪枝下一层的关联核。虽然手动分析参数依赖性是可行的，但当应用于复杂的网络时，这个过程可能非常耗费人力。此外，这样的手动方案本质上不能转移到新的架构，这严重限制了剪枝的应用。最近，一些初步的工作已经被提出来解密层与层之间的复杂关系。不幸的是，现有的技术仍然依赖于经验规则或预定义的架构模式，使它们对所有结构剪枝应用的通用性不足。</li>
<li>作者指出了他们的工作目标：提出一种通用的方法来解决这个挑战，证明有效地处理参数依赖性可以使结构剪枝在各种网络中通用，从而在多个任务上得到满意的性能。</li>
</ol>
<p>在卷积神经网络（CNNs）中，一层的输出通常作为下一层的输入，这种依赖关系也适用于卷积层。如果你移除了Conv2的一个过滤器，那么Conv2的输入（也就是Conv1的输出）就会少了一个特征映射。为了保持网络的结构完整性，你需要同时剪掉Conv1中对应的输出通道。</p>
<p><strong>具体来说，Conv1的每个过滤器都会输出一个特征映射，这些特征映射被堆叠在一起，形成Conv2的输入。每个Conv2的过滤器都会在这些输入特征映射上进行卷积操作。如果你移除了Conv2的一个过滤器，那么那个过滤器对应的输入特征映射就不再需要了。这个输入特征映射是Conv1的一个输出特征映射，所以对应的Conv1的过滤器也就可以被剪掉了。</strong></p>
<p>这就是为什么在剪枝Conv2的时候，也需要剪掉Conv1。这是由于Conv1和Conv2之间的结构耦合，也就是他们的输出和输入之间的依赖关系。</p>
</blockquote>
<h1 id="方法-21、29"><a href="#方法-21、29" class="headerlink" title="方法:21、29"></a>方法:21、29</h1><ol>
<li><h2 id="Dependency-in-Neural-Networks神经网络中的依赖"><a href="#Dependency-in-Neural-Networks神经网络中的依赖" class="headerlink" title="Dependency in Neural Networks神经网络中的依赖"></a>Dependency in Neural Networks神经网络中的依赖</h2></li>
<li><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221459077.png" alt="image-20230722145939026"></p>
</li>
<li><p>在不同结构中具有相互依赖性的分组参数。必须同时修剪所有高亮显示的参数。</p>
</li>
<li><blockquote>
<p>在许多神经网络优化任务中，结构修剪是一种常用的技术，通过移除不重要的神经元来使网络更加高效。在这里，作者使用了一个由三个连续层组成的线性神经网络作为例子，每一层都由二维权重矩阵（wl, wl+1和wl+2）参数化。</p>
<p>当通过修剪神经元来瘦身网络时，参数之间的依赖性就显现出来了。在这里，wl和wl+1的依赖性是这样的：如果你想修剪连接wl和wl+1的第k个神经元，那么你就必须同时移除wl[k, :] 和 wl+1[:, k]。这就是所谓的“依赖性”，因为这两个参数（即wl和wl+1）必须一起修剪，不能单独修剪。</p>
<p>在现有的文献中，研究人员通常使用手动设计的、模型特定的方案来处理层依赖性并对深度神经网络进行结构修剪。然而，这个过程中存在着各种类型的依赖性，如作者所示。手动分析所有的依赖性并不可行，尤其是当简单的依赖性可能会嵌套或组合形成更复杂的模式时。</p>
</blockquote>
</li>
</ol>
<h2 id="Dependency-Graph依赖图"><a href="#Dependency-Graph依赖图" class="headerlink" title="Dependency Graph依赖图"></a>Dependency Graph依赖图</h2><p>1.分组：为了执行结构修剪（结构修剪是一种优化神经网络的方法，通过删除不重要的神经元或连接来减少网络的复杂性），我们首先需要将网络层按照它们的依赖关系进行分组。在这里，他们提出了一个分组矩阵G，它是一个L×L的矩阵，其中L是网络层的数量。在这个矩阵中，如果第i层和第j层之间存在依赖关系，那么Gij就是1。</p>
<p>然而，现代的深度网络可能由成千上万个具有复杂连接的层组成，这使得从神经网络中获取分组模式变得非常困难。也就是说，得到的分组矩阵G可能会非常大而复杂，其中Gij的值不仅取决于第i层和第j层，还会受到它们之间所有中间层的影响。在大多数情况下，这种非局部的、隐式的关系不能用简单的规则来处理。</p>
<p>因此，作者提出了一个新的解决方案：<strong>他们不直接估计分组矩阵G，而是提出了一个更易于估计的依赖模型方法，即依赖图。依赖图是从分组矩阵G中有效地提取出来的，它能更好地处理和展示层之间复杂的依赖关系。</strong></p>
<p> 1 分组矩阵的定义：<br>$$<br>G \in {0,1}^{L \times L}<br>$$</p>
<ol start="2">
<li><p>如果第i层和第j层之间存在依赖关系，那么：<br>$$<br>G_{ij} &#x3D; 1<br>$$</p>
</li>
<li><p>如果不存在依赖关系，则：</p>
</li>
</ol>
<p>$$<br>G_{ij} &#x3D; 0<br>$$</p>
<ol start="4">
<li>对角线元素的设置，即自依赖性：</li>
</ol>
<p>$$<br>Diag(G) &#x3D; \mathbf{1}_{1 \times L}<br>$$</p>
<p>其中$\mathbf{1}_{1 \times L}$是长度为L的全1向量。</p>
<ol start="5">
<li>表示与第i层有依赖关系的所有层的集合：</li>
</ol>
<p>$$<br>g(i) &#x3D; {j | G_{ij} &#x3D; 1}<br>$$</p>
<p>2 Dependency Graph ：介绍了一种新的图形数据结构——依赖图Dependency Graph，这是一种有效的分组矩阵G的压缩方式。依赖图D记录了具有直接连接的相邻层之间的本地依赖关系，与分组矩阵G的区别在于，它仅记录了直接连接的相邻层之间的依赖关系。依赖图D可以被视为分组矩阵G的传递性简化，即<strong>包含与G相同的顶点，但尽可能少的边</strong>。</p>
<p>在介绍Dependency Graph之前，首先用一个例子解释了依赖关系的冗余。考虑一个依赖组g &#x3D; {w1, w2, w3}，它有依赖关系w1 ⇔ w2，w2 ⇔ w3，和w1 ⇔ w3。经过仔细观察，可以发现这个依赖模型存在一些冗余。例如，依赖关系w1 ⇔ w3可以通过递归过程从w1 ⇔ w2和w2 ⇔ w3推导出来。开始时，我们以w1为起点，检查它与其他层（如w1 ⇔ w2）的依赖关系。然后，w2提供了一个新的起点，用于递归地扩展依赖关系，这反过来又“触发”了w2 ⇔ w3。这个递归过程最终以一个传递关系w1 ⇔ w2 ⇔ w3结束。在这种情况下，我们只需要两个依赖关系就可以描述组g中的关系。</p>
<p>同样的，3.2节中讨论的分组矩阵G对于依赖性建模也是冗余的，因此可以被压缩为一个形式更紧凑，边更少，但保留相同信息的形式。这就引出了依赖图。</p>
<p>依赖图D的正式构造规则是：对于所有的Gij &#x3D; 1，在D中存在从顶点i到j的路径。因此，Gij可以通过检查D中顶点i和j之间是否存在路径来推导出来。</p>
<blockquote>
<p>依赖图Dependency Graph的构造过程分步骤解释：</p>
<ol>
<li><strong>定义所有节点</strong>：首先，我们在图中定义每一层网络为一个节点。这些节点相当于我们要考虑的网络层。</li>
<li><strong>检查并添加直接的边</strong>：然后，我们仔细观察网络中每一层之间的直接连接。如果第i层和第j层之间有直接连接（即第i层的输出是第j层的输入），我们就在依赖图中添加一条从i到j的边。这样，我们就得到了所有直接相邻层之间的依赖关系。</li>
</ol>
<p>现在我们来考虑一个更为复杂的依赖关系：如果有第i层→第j层→第k层的依赖关系。在原始的分组矩阵G中，我们会同时标记Gij和Gjk为1，还需要标记Gik为1，表示第i层和第k层也存在依赖关系。</p>
<p>然而，在依赖图D中，我们不需要直接标记Gik。只要有从节点i到节点k的路径（在这种情况下，路径就是i→j→k），就认为Gik存在。这样，依赖图D只需要记录直接的依赖关系，而所有间接的依赖关系可以通过检查路径的存在来推导。这大大减少了需要记录的依赖关系的数量。</p>
<p>依赖图D的主要目标就是将分组矩阵G压缩为更简洁的形式，便于处理复杂的依赖关系。这种压缩是有效的，因为我们只关心是否存在从一个节点到另一个节点的路径，而不关心路径的具体形状。这种方法可以在保留所有重要信息的同时，大大简化了依赖关系的表示。</p>
</blockquote>
<p>3 Network Decomposition 网络分解</p>
<blockquote>
<p>这一段的主要内容是说，在实际操作中，仅仅在网络层级别上建立依赖图可能存在一些问题，因为某些基础层（如全连接层）可能有两种不同的修剪方案，比如 w[k, :] 和 w[:, k]，它们分别压缩了输入和输出的维度。此外，网络还包含一些非参数化的操作，比如跳过连接（skip connections），它们也会影响层之间的依赖关系。</p>
<p>为了解决这些问题，作者提出了一种新的表示法，将网络 F(x; w) 分解为更细的基本组件，表示为 F &#x3D; {f1, f2, …, fL}，其中每个组件 f 可以是参数化的层（如卷积）或非参数化的操作（如残差加法）。他们不再专注于层级别的关系，而是集中于层的输入和输出之间的依赖关系。特别地，他们将组件 fi 的输入和输出分别表示为 f- i 和 f+ i。对于任何网络，最终的分解可以被形式化为 F &#x3D; {f- 1 , f+ 1 , …, f- L , f+ L }。这种表示法使得依赖关系的建模更为容易，并允许同一层有不同的修剪方案。</p>
</blockquote>
<p>4 Dependency Modeling 依赖模型</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221521004.png" alt="image-20230722152158933"></p>
<blockquote>
<p>层分组是通过从f+ 4开始在DepGraph上递归传播来实现的。在该示例中，由于上面所示的发散修剪方案，在卷积输入f-4和输出f+ 4之间不存在层内依赖性。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221523007.png" alt="image-20230722152330970"></p>
<p>算法1 解释</p>
<blockquote>
<p>用于构建所“依赖图”，它是一个二维矩阵，用于表示网络中的层之间的依赖关系。</p>
<p>以下是这个算法的步骤：</p>
<ol>
<li><p>输入一个神经网络F(x; w)。</p>
</li>
<li><p>将网络F分解为两组组件：f− 和 f+。这些组件分别代表每一层的输入和输出。</p>
</li>
<li><p>初始化依赖图DepGraph D，这是一个 2L x 2L 的零矩阵，其中 L 是网络中层的数量。</p>
</li>
<li><p>双重循环遍历所有的 i 和 j，对于每一对 (i, j)，根据公式计算 D(f- i, f+ j) 和 D(f+ j, f- i) 的值。</p>
<ul>
<li><p>如果 f- i 与 f+ j 是相连的，或者它们位于同一层并且有相同的剪枝方案（sch(f- i) &#x3D; sch(f+ j)），则值为1；</p>
</li>
<li><p>否则，值为0。</p>
</li>
</ul>
</li>
<li><p>返回依赖图D。</p>
</li>
</ol>
<p>简单来说，这个算法通过检查网络中每一层的输入和输出，建立了一个表征层间和层内依赖关系的二维矩阵。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221524708.png" alt="image-20230722152425671"></p>
<blockquote>
<p>这个算法是用来在神经网络中创建一组分组，每一组中的层有依赖关系。</p>
<p>以下是这个算法的步骤：</p>
<ol>
<li><p>输入依赖图DepGraph D。</p>
</li>
<li><p>初始化一个空的集合G，这将存储分组。</p>
</li>
<li><p>遍历所有层i。</p>
</li>
<li><p>对于每个i，初始化一个包含i的新组g。</p>
</li>
<li><p>在一个循环中，持续更新g，直到没有新的元素可以添加进g为止：</p>
<ul>
<li><p>创建一个未见过的层的集合UNSEEN，这些层还没有被添加到g中。</p>
</li>
<li><p>创建一个新的集合g’，包含那些在UNSEEN中并且与g中的某个层k有依赖关系的层（即Dkj &#x3D; 1）。</p>
</li>
<li><p>更新g，将g’中的所有层添加到g中。</p>
</li>
</ul>
</li>
<li><p>将g添加到G中。</p>
</li>
<li><p>返回所有的分组G。</p>
</li>
</ol>
<p>简单来说，这个算法是通过遍历网络中的所有层，并查看哪些层与已经在组中的层有依赖关系，从而创建一系列分组。每个分组中的所有层都有相互依赖关系。</p>
</blockquote>
<h2 id="3-3-组级剪枝"><a href="#3-3-组级剪枝" class="headerlink" title="3.3 组级剪枝"></a>3.3 组级剪枝</h2><p>这部分主要讨论了如何在神经网络中实现组级别的剪枝。</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221527044.png" alt="image-20230722152727989"></p>
<blockquote>
<p>比较了三种不同的剪枝方法，即如何选择神经网络中应该删除的权重（即神经元之间的连接）。每种方法都有自己的优点和缺点，主要的区别在于他们如何定义哪些权重是“重要的”。</p>
<ol>
<li>方法(a) 是非结构化剪枝：在这种方法中，每个权重都被独立地看待。换句话说，我们只关注每个单独权重的重要性，而不考虑它们之间的关系。这种方法的优点是简单直观，缺点是它可能忽略了权重之间的依赖关系。</li>
<li>方法(b) 是结构化剪枝：这种方法在评估权重重要性时，会考虑到一层内的结构关系，即在同一层内的权重将会一起被考虑。但是，它依然忽视了在不同层之间可能存在的关联关系。</li>
<li>方法(c) 是本文提出的组级剪枝：这种方法考虑到了权重之间的依赖关系，无论这些权重是在同一层还是在不同层。它的目标是将所有关联的参数（即一组）一起设置为零，这样就可以很容易地通过一个简单的幅度方法（magnitude method）来识别它们。</li>
</ol>
<p>这三种方法的主要区别在于他们处理权重之间关系的方式。在组级剪枝中，作者认为在神经网络中，权重之间的关系是非常重要的，应该被一同考虑，从而得到更好的剪枝效果。</p>
</blockquote>
<p>在前面的部分中，作者们已经开发了一种用于分析神经网络内部依赖性的通用方法，这自然引导出了组级别剪枝问题。评估组合参数的重要性对于剪枝来说是一个重大的挑战，因为它涉及到几个关联的层。在这个部分中，作者们利用一个简单的基于范数的标准来为组级别剪枝建立了一个实用的方法。</p>
<p>给定一个参数组g &#x3D; {w1, w2, …, w|g|}，现有的标准，如L2范数的重要性I(w) &#x3D; w2，可以为g中的每个w产生独立的评分。估计组的重要性的一个自然的方式是计算一个聚合的评分I(g) &#x3D; ∑w∈g I(w)。但不幸的是，独立地在不同层上估计的重要性评分可能由于分布和大小的差异而不可加，因此没有意义。为了使这个简单的聚合适用于重要性估计，<strong>作者们提出了一个稀疏训练方法</strong>，用于在组级别上稀疏化参数，这样，那些被零化的组就可以从网络中安全地移除。</p>
<blockquote>
<p>作者在这一部分讨论了如何对神经网络进行“组级别”的剪枝，这是一种对网络进行优化的方法，用以删除网络中不重要或者冗余的部分，以减少网络的复杂性。</p>
<p>在这里，他们提到的”组”指的是一组相关的神经网络参数，比如，这些参数可能来自同一层，或者在不同层间有一些依赖关系。这些参数被看作是一个整体，而不是独立处理，因为他们在网络中的作用是相互关联的。</p>
<p>那么，问题来了，我们应该如何决定哪个“组”重要，哪个不重要呢？作者在这里提出了一种方法，即通过计算组内所有参数的L2范数（一种衡量参数重要性的方法）的总和来判断。如果一个组的总重要性得分低，那么这个组可能就不太重要，可以被剪枝掉。</p>
<p>但是，这个方法有个问题，就是不同层的参数可能具有不同的规模或分布，所以我们不能简单地将他们的重要性得分加在一起。为了解决这个问题，作者提出了一种“稀疏训练”的方法。这种方法的目标是尽量让不重要的组的参数变为0，这样这些参数就不会对网络的输出产生影响，可以安全地被剪枝掉。</p>
<p>作者使用了一个特定的公式，通过对参数进行“惩罚”，使得不重要的参数趋向于0，这就是所谓的”正则化项”。这个公式涉及一些复杂的数学概念，简单来说，就是根据每个参数的重要性对其进行加权，重要性越低的参数，其“惩罚”越大。</p>
<p>在进行了这种“稀疏训练”后，作者再用一个简单的公式来决定哪些参数是不重要的，即他们的分数低于某个阈值，然后将这些参数删除。</p>
<p>最后，作者通过实验证明，这种方法在剪枝效果上能达到与其他现代方法相当的水平，即使它的原理比较简单。</p>
</blockquote>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><ul>
<li><p>a. 详细的实验设置:</p>
<ul>
<li><p>作者在CIFAR数据集上进行了实验，评估了他们修剪算法的性能。</p>
</li>
<li><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221531158.png" alt="image-20230722153111108"></p>
</li>
<li><blockquote>
<ul>
<li>“Model &#x2F; Data”: ResNet56和VGG19都是常见的深度学习模型，它们在许多任务上都表现出色。CIFAR-10和CIFAR-100是两个常用的图像分类数据集，分别包含10个和100个类别。</li>
<li>“Method”: 这是正在测试的剪枝方法。表中列出了一系列的剪枝方法，包括本文作者的方法(“Ours”)。</li>
<li>“Base”: 这是未进行剪枝的模型在测试数据集上的准确率。</li>
<li>“Pruned”: 这是使用特定剪枝方法后的模型在测试数据集上的准确率。</li>
<li>“∆ Acc.”: 这是剪枝前后模型准确率的变化，可以看作是剪枝对模型性能的影响。负值表示准确率下降，正值表示准确率提高。</li>
<li>“Speed Up”: 这是通过剪枝实现的加速比例，表示剪枝后的模型与原始模型相比，执行速度提高了多少倍。这是剪枝的主要目标之一，因为更快的模型可以更有效地处理数据和做出预测。</li>
</ul>
</blockquote>
</li>
<li></li>
<li><p>报告了修剪模型的准确性和理论加速比。</p>
</li>
</ul>
</li>
<li><p>b. 详细的实验结果:</p>
</li>
<li><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221534860.png" alt="image-20230722153445775"></p>
</li>
<li><blockquote>
<p>这个表格展示了对各种网络结构（如ResNet-56、VGG-19、DenseNet-121、MobileNetv2和GoogleNet）进行不同策略的剪枝时的精确度结果。策略包括随机剪枝、不进行分组剪枝、只对卷积层进行剪枝、以及全分组剪枝。并且分别对比了统一的剪枝和学习到的剪枝的效果。</p>
<p>每行的数据对应一种剪枝策略对应的结果。</p>
<ul>
<li>“Architecture”: 这是正在测试的模型的名称。</li>
<li>“Strategy”: 这是正在测试的剪枝策略。表中列出了一系列的剪枝策略，包括随机剪枝（”Random”）、不进行分组剪枝（”No grouping”）、只对卷积层进行剪枝（”Conv-only”）以及全分组剪枝（”Full Grouping”）。</li>
<li>“Pruned Accuracy with Uniform &#x2F; Learned Sparsity”: 这是剪枝后的模型在测试数据集上的准确率，其中包括了使用统一的剪枝策略和学习到的剪枝策略的结果。剪枝策略分别对应了不同的速度提升比例，例如1.5倍、3.0倍、6.0倍、12倍。</li>
<li>“Avg.”: 这是各个速度提升比例下的平均精确度。</li>
</ul>
<p>表格的主要目的是展示在使用不同的剪枝策略以及不同的速度提升比例时，各个网络模型的精确度的变化情况，以此来评估和比较不同剪枝策略的效果。</p>
</blockquote>
<ul>
<li>结果显示，他们的方法在所有分组层（包括卷积、批归一化和全连接层）上都能促进稀疏性，从而提高修剪模型的准确性。</li>
<li>作者还可视化了他们的方法学习到的分组参数的范数，显示了组内的稀疏性。</li>
<li>进行了消融研究以验证分组的有效性，结果表明忽略分组信息会显著降低他们方法的性能。</li>
<li>作者还比较了均匀稀疏性和学习稀疏性，发现学习稀疏性通常优于均匀稀疏性，尽管有时会导致过度修剪和准确性下降。</li>
<li>他们还将其框架应用于包括DenseNet和GoogleNet在内的各种卷积神经网络，展示了其框架的通用性。</li>
<li>提供了DepGraph可视化，显示了参数的分组，为大型神经网络的修剪过程提供了便利。</li>
</ul>
</li>
</ul>
<h1 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h1><p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1TRvELQDNj9PwM-EERWbF3IQOyxZeDepp?usp=sharing#scrollTo=yaoMwy86Vhxz">Torch-Pruning-Demo.ipynb - Colaboratory (google.com)</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model_dict = &#123;</span><br><span class="line">    <span class="string">&#x27;resnet50&#x27;</span>: torchvision.models.resnet50,</span><br><span class="line">    <span class="string">&#x27;resnet18&#x27;</span>: torchvision.models.resnet18,</span><br><span class="line">    <span class="string">&#x27;convnext&#x27;</span>: torchvision.models.convnext_base,</span><br><span class="line">    <span class="string">&#x27;vgg_19_bn&#x27;</span>: torchvision.models.vgg19_bn,</span><br><span class="line">    <span class="string">&#x27;regnet_x_1_6gf&#x27;</span>: torchvision.models.regnet_x_1_6gf,</span><br><span class="line">    <span class="string">&#x27;efficientnet_b4&#x27;</span>: torchvision.models.efficientnet_b4,</span><br><span class="line">    <span class="string">&#x27;densenet121&#x27;</span>: torchvision.models.densenet121,</span><br><span class="line">    <span class="string">&#x27;vit_b_32&#x27;</span>: torchvision.models.vit_b_32,</span><br><span class="line">    <span class="string">&#x27;mobilenet_v3_large&#x27;</span>: torchvision.models.mobilenet_v3_large,</span><br><span class="line">    <span class="comment"># Register your models here. This demo only covers classification models.</span></span><br><span class="line">    <span class="comment"># Swin Transformers are not supported.</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">model = model_dict[<span class="string">&#x27;vit_b_32&#x27;</span>](pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">imp = tp.importance.MagnitudeImportance(p=<span class="number">2</span>) </span><br><span class="line"><span class="comment"># 创建一个重要性估计器，这基于权重的L2范数来评估神经网络各部分的重要性。</span></span><br><span class="line"></span><br><span class="line">ignored_layers = []</span><br><span class="line"><span class="comment"># 创建一个空的列表，用于保存我们不想剪裁的神经网络层。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> model.modules():</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, torch.nn.Linear) <span class="keyword">and</span> m.out_features == <span class="number">1000</span>: </span><br><span class="line">  <span class="comment"># 遍历模型中的所有模块。如果模块是线性层且输出特征数为1000（即分类器），则忽略这一层，不进行剪裁。</span></span><br><span class="line">    ignored_layers.append(m)</span><br><span class="line"></span><br><span class="line">round_to = <span class="literal">None</span></span><br><span class="line"><span class="comment"># 创建一个变量，用于设定剪裁的粒度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>( model, torchvision.models.vision_transformer.VisionTransformer):</span><br><span class="line">  round_to = model.encoder.layers[<span class="number">0</span>].num_heads </span><br><span class="line">  <span class="comment"># 模型是视觉变换器，则将round_to设为变换器的头部数量，这是模型特定的剪裁限制。</span></span><br><span class="line"></span><br><span class="line">pruner = tp.pruner.MagnitudePruner(</span><br><span class="line">    model = model,</span><br><span class="line">    <span class="comment"># 待剪裁的模型</span></span><br><span class="line"></span><br><span class="line">    example_inputs = torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>),</span><br><span class="line">    <span class="comment"># 输入的样例，用于模型的前向传播</span></span><br><span class="line"></span><br><span class="line">    importance = imp,     </span><br><span class="line">    <span class="comment"># 重要性估计器，用于评估哪些部分可以剪裁</span></span><br><span class="line"></span><br><span class="line">    global_pruning=<span class="literal">False</span>, </span><br><span class="line">    <span class="comment"># 设置全局剪裁为False，这意味着不是对整个网络进行剪裁，而是对各层独立进行剪裁。</span></span><br><span class="line"></span><br><span class="line">    ch_sparsity = <span class="number">0.5</span>,    </span><br><span class="line">    <span class="comment"># 设置剪裁后的稀疏度为0.5，即剪裁后每层保留的神经元或通道的比例。</span></span><br><span class="line"></span><br><span class="line">    iterative_steps = <span class="number">1</span>,  </span><br><span class="line">    <span class="comment"># 设置迭代步骤数为1，即达到目标稀疏度所需的剪裁步骤数。</span></span><br><span class="line"></span><br><span class="line">    ignored_layers = ignored_layers,        </span><br><span class="line">    <span class="comment"># 指定要忽略的层，这些层在剪裁过程中不会被剪裁。</span></span><br><span class="line"></span><br><span class="line">    round_to = round_to,  </span><br><span class="line">    <span class="comment"># 设置剪裁的粒度，即剪裁后的通道数需要是这个数的倍数。</span></span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model size before pruning</span></span><br><span class="line">base_macs, base_nparams = tp.utils.count_ops_and_params(model, torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"><span class="comment"># 计算剪裁前的模型大小。使用的度量包括MACs（乘加操作数）和参数数量。输入数据是随机生成的张量。</span></span><br><span class="line"></span><br><span class="line">pruner.step()</span><br><span class="line"><span class="comment"># 执行剪裁操作。在之前的代码中，已经对pruner对象进行了初始化，并设置了剪裁参数。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># modify some inferece-related attributes if necessary</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(model, torchvision.models.vision_transformer.VisionTransformer):</span><br><span class="line"><span class="comment"># 如果模型是视觉变换器（ViT），那么在剪裁后需要修改一些与推理相关的属性。</span></span><br><span class="line">    model.hidden_dim = model.conv_proj.out_channels</span><br><span class="line"><span class="comment"># 对于视觉变换器，剪裁后其隐藏维度（hidden_dim）需要被修改为卷积投影层的输出通道数。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameter &amp; MACs Counter</span></span><br><span class="line">pruned_macs, pruned_nparams = tp.utils.count_ops_and_params(model, torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"><span class="comment"># 计算剪裁后的模型大小。使用的度量依然是MACs和参数数量。输入数据是随机生成的张量。</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The pruned model:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"><span class="comment"># 打印剪裁后的模型。</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Summary:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Params: &#123;:.2f&#125; M =&gt; &#123;:.2f&#125; M&quot;</span>.<span class="built_in">format</span>(base_nparams/<span class="number">1e6</span>, pruned_nparams/<span class="number">1e6</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MACs: &#123;:.2f&#125; G =&gt; &#123;:.2f&#125; G&quot;</span>.<span class="built_in">format</span>(base_macs/<span class="number">1e9</span>, pruned_macs/<span class="number">1e9</span>))</span><br><span class="line"><span class="comment"># 打印剪裁前后的模型大小，包括参数数量和MACs。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Forward</span></span><br><span class="line">output = model(torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"><span class="comment"># 测试剪裁后模型的正向传播。输入数据是随机生成的张量。</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output.shape: &quot;</span>, output.shape)</span><br><span class="line"><span class="comment"># 打印正向传播的输出形状。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Backward</span></span><br><span class="line">loss = torch.nn.functional.cross_entropy(output, torch.randint(<span class="number">1</span>, <span class="number">1000</span>, (<span class="number">1</span>,)))</span><br><span class="line"><span class="comment"># 计算反向传播的损失。使用的损失函数是交叉熵，目标值是随机生成的整数。</span></span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment"># 执行反向传播，计算梯度。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="论文2：Scaling-TransNormer-to-175-Billion-Parameters"><a href="#论文2：Scaling-TransNormer-to-175-Billion-Parameters" class="headerlink" title="论文2：Scaling TransNormer to 175 Billion Parameters"></a>论文2：Scaling TransNormer to 175 Billion Parameters</h1><p>Zhen Qin♯, 1,2Dong Li♯, 1,2Weigao Sun♯, 1,2Weixuan Sun♯, 1,2Xuyang Shen♯,<br>2Xiaodong Han, 2Yunshen Wei, 2Baohong Lv, 1Fei Yuan, 2Xiao Luo,<br>1Yu Qiao, 1,2Yiran Zhong∗<br>1Shanghai AI Laboratory, 2OpenNLPLab</p>
<h1 id="Basic-Information-1"><a href="#Basic-Information-1" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Scaling TransNormer to 175 Billion Parameters (将TransNormer扩展到1750亿参数)</li>
<li>Authors: Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, Yu Qiao, Yiran Zhong</li>
<li>Affiliation: Shanghai AI Laboratory, OpenNLPLab (上海人工智能实验室, OpenNLPLab)</li>
<li>Keywords: Large Language Model, TransNormerLLM, linear attention, efficiency, scalability (大型语言模型, TransNormerLLM, 线性注意力, 效率, 可扩展性)</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.14995v1">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/OpenNLPLab/TransnormerLLM">GitHub</a></li>
</ul>
<h1 id="论文简要-1"><a href="#论文简要-1" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>本文介绍了TransNormerLLM，这是第一个基于线性注意力的大型语言模型，它在准确性和效率方面优于传统的基于softmax注意力的模型。通过引入位置嵌入、线性注意力加速、门控机制、张量归一化和推理加速等先进修改，TransNormerLLM从之前的线性注意力架构TransNormer发展而来。通过一系列的实验验证，该模型在训练和推理阶段都表现出卓越的效率和性能。</li>
</ul>
<h1 id="背景和创新点"><a href="#背景和创新点" class="headerlink" title="背景和创新点:"></a>背景和创新点:</h1><blockquote>
<ol>
<li><p>尽管现有的 Transformer 模型在许多任务上表现优秀，但是当面对大规模参数时，它们往往会遇到一些问题。首先，计算效率较低，这主要是因为 Transformer 模型需要计算和存储所有 token 之间的注意力权重，这在处理长序列时会导致计算和存储开销极大。其次，Transformer 模型还存在注意力稀释问题，即模型可能无法有效地关注到距离较远的 token。</p>
<p>为了解决这些问题，论文作者提出了 TransNormerLLM。TransNormerLLM 是对 TransNormer 模型的一个改进，它通过一系列创新的设计来解决上述问题。以下是 TransNormerLLM 的一些主要创新点：</p>
<ol>
<li><strong>位置编码</strong>：在 TransNormerLLM 中，作者引入了一种名为 LRPE-d 的新方法，这是一种线性化相对位置编码（LRPE）与指数衰减相结合的方法。这种方法可以让模型对距离较远的 token 给予较小的注意力权重，从而有效地解决了注意力稀释问题。同时，由于 LRPE-d 可以被分解为关于每个输入元素的函数，因此它与线性注意力方法完全兼容。值得一提的是，LRPE-d 的参数可以通过训练进行学习，这为模型提供了更大的灵活性。</li>
<li><strong>门控机制</strong>：TransNormerLLM 引入了一种新的门控机制，被称为 SGLU（Simple Gated Linear Unit）。SGLU 是一种改进的激活函数，它引入了一个门控参数来控制信息的流动。不同于传统的 GLU，SGLU 去除了激活函数，因为门本身就可以引入非线性，从而使得模型更加高效。</li>
<li><strong>张量归一化</strong>：在 TransNormerLLM 中，作者引入了一种新的张量归一化方法，称为 SRMSNorm（Simple RMS Norm）。SRMSNorm 是一种更简单的归一化方法，它不依赖于特征维度，因此在处理大规模模型时更为高效。</li>
</ol>
</li>
</ol>
</blockquote>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291115870.png" alt="image-20230729111523716"></p>
<ul>
<li><blockquote>
<p>当然，下面是这些公式的整理以及对应的详细解释：</p>
<ol>
<li><p><strong>位置编码 (LRPE-d)</strong>:<br>$$<br>a_{st} &#x3D; q_s^T k_t \lambda^{s-t} \exp(i\theta(s-t))<br>$$</p>
<p>  这是 TransNormerLLM 使用的位置编码公式，其中 (q_s) 和 (k_t) 分别是位置 (s) 和 (t) 的查询和键，(\lambda) 是衰减率，(\theta) 是可学习的参数。这个公式表示，模型在计算位置 (s) 和 (t) 之间的注意力时，会考虑它们之间的相对位置，并通过 (\lambda^{s-t}) 和 (\exp(i\theta(s-t))) 来调整这个注意力。</p>
<ol start="2">
<li><strong>门控线性注意力 (GLA)</strong>:</li>
</ol>
</li>
</ol>
<p>$$<br>  O &#x3D; \text{Norm}(QK^TV) \odot U<br>$$</p>
<pre><code>  这是 TransNormerLLM 的门控机制，其中 \(Q\), \(K\), \(V\) 和 \(U\) 是模型的查询、键、值和门控参数，\(\odot\) 是元素级的乘法，\(\text&#123;Norm&#125;(\cdot)\) 是归一化函数。这个公式表示，模型在计算输出 \(O\) 时，会先计算 \(QK^TV\) 的归一化，然后与门控参数 \(U\) 进行元素级的乘法。
</code></pre>
<ol start="3">
<li><p><strong>简单门控线性单元 (SGLU)</strong>:</p>
<p>$$<br> O &#x3D; [V \odot U]W_o<br>$$</p>
<p>   这是 TransNormerLLM 的通道混合机制，其中 (V) 和 (U) 是模型的值和门控参数，(W_o) 是输出的权重矩阵。这个公式表示，模型在计算输出 (O) 时，会先计算 (V) 和 (U) 的元素级乘法，然后乘以权重矩阵 (W_o)。</p>
</li>
<li><p><strong>张量归一化 (SRMSNorm)</strong>:</p>
<p>$$<br> \text{SRMSNorm}(x) &#x3D; \frac{x}{|x|_2&#x2F;\sqrt{d}}<br>$$</p>
<p>   这是 TransNormerLLM 的张量归一化函数，其中 (x) 是输入的张量，(|x|_2) 是 (x) 的 L2 范数，(d) 是特征的维度。这个公式表示，模型在归一化 (x) 时，会先计算 (x) 的 L2 范数，然后除以 (\sqrt{d})，最后用 (x) 除以这个结果。</p>
</li>
</ol>
<p>以上四个公式是 TransNormerLLM 模型的关键部分，它们分别对应了模型的位置编码、门控机制、通道混合和张量归一化等重要功能。理解这些公式对于理解 TransNormerLLM 模型的工作原理是非常重要的。</p>
</blockquote>
</li>
<li><p>a. 理论背景:</p>
<ul>
<li>本文介绍了TransNormerLLM，这是一个基于线性注意力的大型语言模型（LLM），在准确性和效率方面超越了传统的softmax注意力模型。该模型建立在之前的线性注意力架构TransNormer的基础上，并引入了位置嵌入、线性注意力加速、门控机制、张量归一化和推理加速等先进的修改。本文强调了传统Transformer的局限性以及LLM中更高效的序列建模方法的需求。</li>
</ul>
</li>
<li><p>b. 技术路线:</p>
<ul>
<li>本文介绍了TransNormerLLM中的架构改进。它引入了将TransNormer的DiagAttention替换为线性注意力，以增强全局交互。本文还引入了具有指数衰减的LRPE来解决注意力稀释问题。在训练过程中，提出了Lightning Attention技术，显著加速线性注意力，将内存使用减少了四倍。本文简化了GLU和归一化，总体加速了20%。通过稳定的推理算法，确保了数值稳定性和恒定的推理速度。本文强调了TransNormerLLM的可扩展性以及在大规模集群上部署的能力。还提到计划开源预训练模型，促进LLM的社区驱动进展。</li>
</ul>
</li>
</ul>
<h1 id="结果-1"><a href="#结果-1" class="headerlink" title="结果:"></a>结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291141964.png" alt="image-20230729114150913"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291138381.png" alt="image-20230729113850322"></p>
<p>Transformer vs TransNormerLLM.TransNormerLLM在相同配置下，在385M和1B大小上的性能分别优于Transformer 5%和9%。</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291138203.png" alt="image-20230729113857153"></p>
<p>我们比较原始的TransNormer和改进的TransNormerLLM，结果示于表4中。TransNormerLLM分别表现出2%和1%的增强，同时显著更快。</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291142007.png" alt="image-20230729114258960"></p>
<p>位置编码在位置编码实验中，我们进行了一系列测试，比较了LRPE-d、APE（绝对位置编码）、LRPE和Exp-Decay（指数衰减）。从表5中可以明显看出，我们提出的增强已经显示出对原始模型的改进。此外，最终的方案表现出2%的LRPE方法的改进。</p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>本研究使用PyTorch和Triton在Metaseq框架中实现了TransNormerLLM模型。</li>
<li>使用Adam优化器进行训练，并使用FSDP将模型扩展到NVIDIA A100 80G集群。还采用了模型并行技术进行性能优化。模型在包含300B个标记的样本语料库上进行训练。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>架构消融实验表明，TransNormerLLM在大小为385M时比Transformer性能提高了5%，在大小为1B时提高了9%。</li>
<li>对比了不同的位置编码方法，LRPE+LRPE-d组合的效果最优，比LRPE提高了2%。</li>
<li>对于衰减温度的消融研究表明，添加衰减温度可以降低TransNormerLLM的困惑度。</li>
<li>TransNormerLLM中引入门控机制可以降低损失值。</li>
<li>对GLA激活函数进行了不同的测试，结果表明激活函数的选择对最终结果影响很小。</li>
<li>在Gated Linear Units（GLU）结构中去除激活函数对结果影响微乎其微。</li>
<li>对于归一化方法进行了多种测试，结果表明这些方法在应用于TransNormerLLM时几乎没有区别。然而，使用Triton实现的修改版SRMSNorm在计算速度上比PyTorch实现方法提供了显著的提升。</li>
<li>Lightning Attention的计算速度至少比NormAttention的PyTorch实现快2倍。</li>
<li>Lightning Attention的内存占用随序列长度线性增长，当序列长度为8192时，比基线模型的内存效率提高了4倍。</li>
<li>模型并行显著降低了内存消耗，当模型并行大小设置为8时，TransNormerLLM-7B模型在单个GPU上只需要24.1GB的内存，相比模型并行大小为1时，内存减少了62.3%。</li>
<li>TransNormerLLM在训练速度和内存消耗方面始终优于Transformer，即使启用了模型并行。</li>
<li>TransNormerLLM模型在计算速度上始终优于Transformer模型，即使模型规模更大。</li>
<li>TransNormerLLM能够以更长的上下文长度进行训练，实现更高的计算速度。</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/7%E6%9C%88%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%91%A8%E6%8A%A5%E4%BE%9D%E8%B5%96%E5%9B%BE%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" data-id="cllkmcmf2000dksu71rcabuj4" data-title="7月第四周周报依赖图通用剪枝" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" rel="tag">通用剪枝</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-7月第一周QLORA-有效剪枝-lomo全参数微调" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8QLORA-%E6%9C%89%E6%95%88%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T07:52:06.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8QLORA-%E6%9C%89%E6%95%88%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/">7月第一周QLORA/有效剪枝/lomo全参数微调</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="7月第一周QLORA-大模型有效剪枝-LOMO-全参数微调减少内存占用"><a href="#7月第一周QLORA-大模型有效剪枝-LOMO-全参数微调减少内存占用" class="headerlink" title="7月第一周QLORA&#x2F;大模型有效剪枝&#x2F;LOMO:全参数微调减少内存占用"></a>7月第一周QLORA&#x2F;大模型有效剪枝&#x2F;LOMO:全参数微调减少内存占用</h1><h1 id="论文1：QLORA：量化LLM的有效微调"><a href="#论文1：QLORA：量化LLM的有效微调" class="headerlink" title="论文1：QLORA：量化LLM的有效微调"></a>论文1：QLORA：量化LLM的有效微调</h1><h1 id="基本信息："><a href="#基本信息：" class="headerlink" title="基本信息："></a>基本信息：</h1><ul>
<li>题目：QLORA：量化LLM的有效微调 （QLORA：高效微调量化语言模型）</li>
<li>作者：蒂姆·德特默斯、阿蒂多罗·帕格诺尼、阿里·霍尔茨曼、卢克·泽特勒莫耶 （蒂姆·德特默斯、阿蒂多罗·帕格诺尼、阿里·霍尔茨曼、卢克·泽特尔莫耶）</li>
<li>隶属关系： 华盛顿大学</li>
<li>关键词：语言模型，微调，量化，低秩适配器（LLA），NormalFloat，双重量化，分页优化器，指令遵循，聊天机器人性能</li>
<li>网址：纸张：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14314v1%EF%BC%8CGitHub%EF%BC%9Ahttps://github.com/artidoro/qlora">https://arxiv.org/abs/2305.14314v1，GitHub：https://github.com/artidoro/qlora</a> 和 <a target="_blank" rel="noopener" href="https://github.com/TimDettmers/bitsandbytes">https://github.com/TimDettmers/bitsandbytes</a></li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><ul>
<li>a. 理论背景：<ul>
<li>本文讨论了QLORA和LoRA这两种参数高效的神经网络微调方法，以减少训练过程中的内存需求。LoRA通过分解投影来增强线性投影，而QLORA则使用4位NormalFloat量化和双量化来减少内存使用。QLORA还引入了分页优化器来防止梯度检查点过程中的内存溢出错误。</li>
</ul>
</li>
<li>b. 技术路线：<ul>
<li>QLORA使用LoRA方法在每个网络层引入适配器，以避免先前工作中出现的准确性折衷。QLORA使用4位NormalFloat（NF4）作为新的数据类型，该类型对于正态分布的权重是最优的。双量化用于量化量化常数，并使用分页优化器来管理内存峰值。</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927766.png" alt="image-20230625110446430"></p>
<blockquote>
<ol>
<li>LoRA（Low-Rank Adaptation）：这是一种微调方法，它通过低秩适应（low-rank adaptation）的技术来减少微调过程中的内存需求。具体而言，LoRA使用低秩矩阵分解的方法来近似和压缩原始的Transformer模型，以减少参数数量和内存占用。</li>
<li>QLORA（Quantized Low-Rank Adaptation）：这是QLORA对LoRA的改进。QLORA通过对Transformer模型进行量化，将模型参数的精度减少到4位，并使用分页式优化器（paged optimizers）来处理内存峰值，从而减少内存需求。</li>
</ol>
<p>关于QLORA相对于LoRA的改进，主要有两个方面：</p>
<ol>
<li>量化（Quantization）：QLORA使用较低的位精度（4位）来表示模型参数，从而降低了内存需求。通过减少每个参数的位数，可以大幅度减少存储参数所需的内存空间。</li>
<li>分页式优化器（Paged Optimizers）：QLORA使用分页式优化器来处理内存峰值问题。分页式优化器将模型参数分成多个页（pages），每次只加载一部分参数到内存中进行计算，以减少内存使用量。通过逐页加载和计算参数，可以处理大型模型在微调过程中可能产生的内存峰值问题。</li>
</ol>
</blockquote>
<h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><blockquote>
<p><strong>Block-wise k-bit Quantization</strong> 量化是将输入从保持更多信息的表示离散化到具有更少信息的表示的过程。这通常意味着将具有更多位的数据类型转换为更少位，例如从32位浮点数转换为8位整数。为了确保使用低位数据类型的整个范围，输入数据类型通常通过由输入元素的绝对最大值进行归一化而被重新缩放到目标数据类型范围中，输入元素通常被构造为张量。例如，将32位浮点（FP 32）张量量化为Int 8张量，范围为[-127，127]：<br>$$<br>\begin{aligned}<br>&amp; \<br>&amp;\mathbf{X}^{\mathrm{Int}}&amp; {}^{8}&#x3D;\mathrm{round}\left(\frac{127}{\mathrm{absmax}(\mathbf{X}^{\mathrm{FP32}})}\mathbf{X}^{\mathrm{FP32}}\right)&#x3D;\mathrm{round}(c^{\mathrm{FP32}}\cdot\mathbf{X}^{\mathrm{FP32}}),<br>\end{aligned}<br>$$</p>
<p>$$<br>\mathbf{dequant}(c^{\mathrm{FP32}},\mathbf{X}^{\mathrm{Int8}})&#x3D;{\frac{\mathbf{X}^{\mathrm{Int8}}}{c^{\mathrm{FP32}}}}&#x3D;\mathbf{X}^{\mathrm{FP32}}<br>$$</p>
<p><strong>Low-rank Adapters</strong>  低阶适配器（LoRA）微调是一种通过使用一小组可训练参数（通常称为适配器）来降低内存需求的方法，同时不更新保持固定的完整模型参数。随机梯度下降期间的梯度通过固定的预训练模型权重传递到适配器，该适配器被更新以优化损失函数。LoRA通过额外的因子分解投影来增强线性投影。</p>
<p>在参数高效微调（PEFT）方法中，LLM微调的大部分存储器占用来自激活梯度而不是来自学习的LoRA参数。对于在批量大小为1的FLAN v2上训练的7B LLaMA模型，LoRA权重相当于常用的原始模型权重的0.2%[28，37]，LoRA输入梯度的内存占用量为567 MB，而LoRA参数仅占用26 MB。使用梯度检查点[9]，输入梯度减少到每个序列平均18 MB，使其比所有LoRA权重组合更占用内存。相比之下，4位基本模型消耗5，048 MB内存。这突出了梯度检查点设置是重要的，但也突出了积极地减少LoRA参数的量仅产生较小的存储器益处。这意味着我们可以使用更多的适配器，而不会显著增加整体训练内存占用（详细的分类请参见附录G）。如后所述，这对于恢复完整的16位精度性能至关重要。</p>
</blockquote>
<h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>QLORA包含两个组件：4-bit NormalFloat量化和Double Quantization。其中：4-bit NormalFloat数据类型是基于Quantile Quantization技术开发的，通过估计输入张量的分位数来保证每个量化区间分配相等的值。Double Quantization是将额外的量化常数进行量化以减小内存开销的过程。</p>
<h3 id="4-bit-NormalFloat量化"><a href="#4-bit-NormalFloat量化" class="headerlink" title="4-bit NormalFloat量化"></a>4-bit NormalFloat量化</h3><p>NormalFloat（NF）数据类型建立在分位数量化[15]的基础上，这是一种信息理论上的最佳数据类型，可确保每个量化二进制具有从输入张量分配的相等数量的值。分位数量化通过经验累积分布函数估计输入张量的分位数来工作。分位数量化的主要限制是分位数估计的过程是昂贵的。因此，快速分位数近似算法，如SRAM分位数[15]，用于估计它们。由于这些分位数估计算法的近似性质，数据类型对于离群值具有较大的量化误差，离群值通常是最重要的值。当输入张量来自固定到量化常数的分布时，可以避免昂贵的分位数估计和近似误差。在这种情况下，输入张量具有相同的分位数，使得精确的分位数估计在计算上可行。</p>
<p>NF数据类型基于Quantile Quantization方法，它是一种信息论上最优的数据类型，可以确保将输入张量中的每个量化区间分配相等数量的值。Quantile quantization通过估计输入张量的分位数来实现，估计过程基于经验累积分布函数。Quantile quantization的主要限制在于分位数估计的过程比较昂贵。因此，需要使用快速的分位数近似算法（如SRAM quantiles）来进行估计。由于这些分位数估计算法是近似的，所以对于异常值（通常是最重要的值），数据类型存在较大的量化误差。当输入张量来自一个固定到量化常数的分布时，可以避免昂贵的分位数估计和近似误差。在这种情况下，输入张量具有相同的分位数，因此可以通过精确的分位数估计来降低计算成本。由于预训练的神经网络权重通常具有以零为中心的正态分布，标准差为σ（参见附录F），我们可以通过缩放σ来将所有权重转换为单个固定分布，使得该分布完全适配于我们的数据类型范围。对于我们的数据类型，我们将其范围设置为[-1, 1]。因此，数据类型的分位数和神经网络权重都需要被归一化到该范围内。对于零均值正态分布，标准差为σ且落在[-1, 1]范围内的情况，信息论上最优的数据类型计算步骤如下：（1）估计理论N(0, 1)分布的2k+1个分位数，以获得正态分布的k位量化数据类型；（2）将该数据类型的值归一化到[-1, 1]范围内；（3）通过绝对最大重缩放，将输入的权重张量归一化到[-1, 1]范围内。一旦权重范围和数据类型范围匹配，就可以像通常一样进行量化。步骤（3）等价于将权重张量的标准差重新缩放以匹配k位数据类型的标准差。<br>$$<br>q_i&#x3D;\frac{1}{2}\left(Q_X\left(\frac{i}{2^k+1}\right)+Q_X\left(\frac{i+1}{2^k+1}\right)\right),<br>$$</p>
<h3 id="Double-Quantization"><a href="#Double-Quantization" class="headerlink" title="Double Quantization"></a>Double Quantization</h3><p>这段文本介绍了一种称为Double Quantization (DQ) 的过程，用于对量化常数进行量化以实现更高的内存节省。精确的4位量化通常需要较小的块大小，但同时也会带来相当大的内存开销。例如，对于权重W，使用32位常数和块大小为64，平均每个参数需要增加32&#x2F;64 &#x3D; 0.5位的内存。Double Quantization可以帮助减少量化常数的内存占用。</p>
<p>具体而言，Double Quantization将第一次量化的量化常数cFP32作为第二次量化的输入。第二次量化得到了量化后的量化常数cFP8和第一层量化常数cFP32。我们在第二次量化中使用了8位浮点数（Floats），块大小为256，因为研究人员发现8位量化不会降低性能，这与Dettmers和Zettlemoyer的研究结果一致。</p>
<p>由于cFP32为正值，我们在量化之前从c2中减去均值，使值围绕零对称分布，以实现对称量化。平均而言，对于块大小为64，这种量化将每个参数的内存占用从32&#x2F;64 &#x3D; 0.5位减少到8&#x2F;64 + 32&#x2F;(64 · 256) &#x3D; 0.127位，每个参数减少了0.373位的内存占用。</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927895.png" alt="image-20230625141917832"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927319.png" alt="image-20230625142005506"></p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><ul>
<li>a. 详细的实验设置：<ul>
<li>使用QLORA，Guanaco模型系列在Vicuna基准测试中表现优于所有先前发布的模型，达到ChatGPT性能的99.3%，仅需要在单个GPU上进行24小时的微调。</li>
<li>QLORA用于微调超过1，000个模型，并对8个指令数据集、多个模型类型（LLaMA、T5）和模型规模进行了详细的指令跟随和聊天机器人性能分析。</li>
<li>结果表明，使用QLORA在小型高质量数据集上进行微调可以获得最先进的结果，即使使用比先前最先进的模型更小的模型。</li>
</ul>
</li>
<li>b. 详细的实验结果：<ul>
<li>QLORA在三种架构类型上的实验评估表明，其性能与完整模型微调相当，并优于16位适配器微调。</li>
<li>4位NormalFloat数据类型减少了量化误差，而双量化减少了内存开销，使QLORA成为一种有效且内存高效的微调方法。</li>
</ul>
</li>
</ul>
<h1 id="论文2：A-Simple-and-Effective-Pruning-Approach-for-Large-Language-Models"><a href="#论文2：A-Simple-and-Effective-Pruning-Approach-for-Large-Language-Models" class="headerlink" title="论文2：A Simple and Effective Pruning Approach for Large Language Models"></a>论文2：A Simple and Effective Pruning Approach for Large Language Models</h1><blockquote>
<ul>
<li>Title: A Simple and Effective Pruning Approach for Large Language Models (大型语言模型的简单有效修剪方法)</li>
<li>Authors: Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter</li>
<li>Affiliation: Carnegie Mellon University (卡内基梅隆大学)</li>
<li>Keywords: Large Language Models, Network pruning, Weight reconstruction, Sparsity, Magnitude pruning (大型语言模型，网络修剪，权重重构，稀疏性，幅度修剪)</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.11695v1">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/locuslab/wanda">GitHub</a></li>
<li>2023.06</li>
</ul>
</blockquote>
<h3 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h3><p>本研究提出了一种简单而有效的修剪方法，名为Wanda（基于权重和激活的修剪），用于大型语言模型（LLMs），通过在预训练的LLMs中修剪具有最小幅度的权重乘以相应的输入激活来诱导稀疏性。该方法无需重新训练或权重更新，并且修剪后的LLM可以直接使用。实验证明，Wanda在各种语言基准测试中明显优于幅度修剪，并与涉及大量权重更新的最新方法竞争激烈。</p>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 大型语言模型（LLMs）在复杂语言基准测试中表现出色，但由于其庞大的参数规模，需要大量的计算资源。为了降低LLMs的计算成本，研究人员一直在努力寻找有效的压缩方法。</li>
<li>过去方案: 以往的修剪方法要么需要重新训练，这对于规模庞大的LLMs来说很少可行，要么需要解决依赖于二阶信息的权重重构问题，这可能也会带来计算负担。这些方法在压缩LLMs方面的应用相对较少，这与在LLM之前的模型压缩领域的趋势相矛盾。</li>
<li>论文的Motivation: 鉴于以往方法的局限性，本研究旨在提出一种简单而有效的修剪方法，用于直接从预训练的LLMs中找到高效的稀疏网络，而无需重新训练或权重更新。通过观察LLMs中出现的大幅度特征，作者提出了一种基于权重和激活的修剪度量，并在每个输出上比较权重，以实现对LLMs的稀疏化。作者通过实验证明，该方法在性能上明显优于幅度修剪，并且在计算成本上要低于最新的LLMs修剪方法。</li>
<li>幅度修剪（Magnitude Pruning）：是一种剪枝技术，用于稀疏化神经网络中的权重参数。该技术通过保留模型中最重要的权重，将较小幅度的权重置为零或接近零的值，以达到减少模型复杂度和提高推理速度的目的。</li>
<li>Emergent Large Magnitude Features：（ELM特征）是指在训练神经网络时，由于网络的非线性特性和优化过程中的相互作用，某些具有较大幅度的特征或神经元在网络的上层逐渐变得重要。</li>
</ul>
<h1 id="方法-2"><a href="#方法-2" class="headerlink" title="方法:"></a>方法:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021429688.png" alt="image-20230702142920614"></p>
<blockquote>
<p>举个例子说明传统剪枝的额局限性：考虑一个具有两个输入和相应权重的神经元，其中y &#x3D; w1x1 + w2x2，且|w1| ≤ |w2|。传统的幅度剪枝方法总是会选择移除权重w1，但是在LLMs中，输入特征x1和x2的尺度可能存在显著差异。在某些情况下，可能有|x1| ≫ |x2|，导致|w1x1| ≫ |w2x2|。因此，在这种情况下，我们应该选择移除权重w2，因为相比于移除权重w1，它对神经元输出y的影响更小。</p>
<p>为了解决幅度剪枝方法的这个局限性，该段介绍了一种针对LLMs设计的剪枝度量。对于完全连接的线性层，考虑形状为(Cout, Cin)的权重W，输入激活为形状为(N × L, Cin)的X。对于每个权重，作者提出通过其幅度和相应输入特征范数的乘积来评估其重要性。具体地，当前权重Wij的得分由以下公式定义： Sij &#x3D; |Wij| · ∥Xj∥2 其中|·|表示绝对值运算符，∥Xj∥2计算了聚合在N × L个不同标记上的第j个特征的ℓ2范数，最终得分由这两个标量值的乘积计算而得。作者发现，ℓ2范数在衡量激活幅度时往往比其他范数函数（如ℓ1和ℓ∞范数）效果更好。</p>
<p>2范数（L2 norm），也称为欧几里德范数，用于计算向量的长度或矩阵的平方和的平方根。对于一个向量x&#x3D;(x1, x2, …, xn)，其ℓ2范数定义如下：</p>
<p>∥x∥2 &#x3D; sqrt(x1^2 + x2^2 + … + xn^2)</p>
</blockquote>
<ul>
<li>a. 理论背景:<ul>
<li>大型语言模型（LLMs）的重要性和由于其规模和计算要求而带来的挑战。</li>
<li>网络修剪方法的需求，以减小LLMs的规模。</li>
<li>现有方法要么需要重新训练，要么需要计算昂贵的权重重构。</li>
<li>引入一种名为Wanda的新型修剪方法，根据权重的大小乘以相应的输入激活来修剪权重。</li>
<li>Wanda不需要重新训练或权重更新，可以直接用于预训练的LLMs。</li>
<li>Wanda在性能和计算成本方面优于幅度修剪和其他最近的方法。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>Wanda是为修剪大型语言模型（LLMs）而设计的。</li>
<li>引入了一种修剪度量，根据输入特征激活的范数向量和权重的绝对值的点积来计算权重的重要性。</li>
<li>这种度量是稳健的，并且可以使用适量的校准样本进行估计。</li>
<li>Wanda建议使用更局部的修剪粒度级别，按输出逐个比较和删除权重。</li>
<li>修剪过程可以在LLM模型的单次前向传递中无缝实现，无需权重更新或进一步训练。</li>
<li>Wanda还可以扩展到结构化的N:M稀疏性，其中每M个连续权重中最多有N个非零。</li>
</ul>
</li>
</ul>
<h1 id="结果-1"><a href="#结果-1" class="headerlink" title="结果:"></a>结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021447017.png" alt="image-20230702144742941"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021459806.png" alt="image-20230702145942743"></p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>在LLaMA模型系列上评估了Wanda方法，该系列包括各种参数级别的Transformer语言模型。</li>
<li>将修剪方法应用于四个LLaMA模型：LLaMA-7B，LLaMA-13B，LLaMA-30B和LLaMA-65B。</li>
<li>通过在保留验证集上计算困惑度来评估修剪网络的性能。</li>
<li>使用128个从C4训练数据中采样的序列作为校准数据，用于估计输入统计信息。</li>
<li>Perplexity: 是一种用于评估语言模型性能的指标。它衡量模型对给定序列的预测能力和不确定性。较低的困惑度表示模型在给定序列上的预测更准确和更自信。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>将Wanda与两种先前的修剪方法（幅度修剪和SparseGPT）进行比较。</li>
<li>Wanda保持了幅度修剪的简单性，但在发现预训练LLMs中的稀疏网络方面非常有效。</li>
<li>Wanda比SparseGPT更快，因为它不涉及逆计算。</li>
<li>SparseGPT（稀疏GPT）是一种基于稀疏注意力机制的语言模型，它是对GPT（Generative Pre-trained Transformer）模型的改进和扩展。SparseGPT使用稀疏的注意力权重矩阵，其中只有少数非零元素，而其他元素为零。这种稀疏性允许SparseGPT在保持相对较低的计算成本的同时，仍能保持较好的模型性能。</li>
<li>提供了一个比较表，显示每种方法使用的时间复杂度和修剪度量。</li>
</ul>
</li>
</ul>
<h1 id="论文3：LOMO-Full-Parameter-Fine-tuning-for-Large-Language-Models-with-Limited-Resources"><a href="#论文3：LOMO-Full-Parameter-Fine-tuning-for-Large-Language-Models-with-Limited-Resources" class="headerlink" title="论文3：LOMO:Full Parameter Fine-tuning for Large Language Models with Limited Resources"></a>论文3：LOMO:Full Parameter Fine-tuning for Large Language Models with Limited Resources</h1><p><strong>论文：LOMO：利用有限的资源对大型语言模型进行全参数微调</strong></p>
<blockquote>
<p>ArXiv: <a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/2306.09782.pdf">https://arxiv.org/pdf/2306.09782.pdf</a><br>机构：复旦大学<br>时间：2023.6.16<br>Github：<a href="https://link.zhihu.com/?target=https://github.com/OpenLMLab/LOMO">https://github.com/OpenLMLab/LOMO</a></p>
</blockquote>
<h2 id="摘要：-1"><a href="#摘要：-1" class="headerlink" title="摘要："></a>摘要：</h2><p>大型语言模型（LLMs）已经彻底改变了自然语言处理（NLP），但是训练LLMs需要大量的GPU资源。降低LLMs训练的门槛将鼓励更多研究人员参与，从而使学术界和社会受益。虽然现有的方法着重于参数高效微调，即微调或添加少量参数，但很少有人解决了有限资源下调整LLMs的全部参数的挑战。在本文中，我们提出了一种新的优化器LOw-Memory Optimization（LOMO），它将梯度计算和参数更新融合在一步中以减少内存使用。通过将LOMO与现有的内存节省技术集成，我们将内存使用降低到10.8％，与标准方法（DeepSpeed解决方案）相比。因此，我们的方法使单台机器上的65B模型的全参数微调成为可能，该机器配有8×RTX 3090，每个显存为24GB。</p>
<h2 id="IDEA"><a href="#IDEA" class="headerlink" title="IDEA"></a>IDEA</h2><blockquote>
<p>分析了SGD可以finetune LLM的原因，不用Adam改用SGD，在SGD的基础上提出了一个LOw-Memory Optimization（LOMO）的优化器，来全参数finetune LLM，并在下游任务上获得了比lora等更好的效果。（可能因为资源问题没对比Adam的全参数finetune的结果，这个还不够有说服力）8张3090能微调65B的模型了</p>
</blockquote>
<h2 id="背景知识："><a href="#背景知识：" class="headerlink" title="背景知识："></a>背景知识：</h2><blockquote>
<h3 id="1-SGD代表随机梯度下降（Stochastic-Gradient-Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。"><a href="#1-SGD代表随机梯度下降（Stochastic-Gradient-Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。" class="headerlink" title="1.SGD代表随机梯度下降（Stochastic Gradient Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。"></a>1.SGD代表随机梯度下降（Stochastic Gradient Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。</h3><p>下面是SGD的详细解释：</p>
<ol>
<li><p>梯度下降：梯度下降是一种优化算法，用于找到函数的局部最小值或全局最小值。在机器学习中，我们使用梯度下降来最小化模型的损失函数。梯度表示函数在给定点的变化率方向，通过朝着梯度的反方向调整模型参数，我们可以逐步降低损失函数的值，从而找到最优参数。</p>
</li>
<li><p>随机梯度下降：随机梯度下降是梯度下降的一种变种，其目标是加速收敛过程。与传统的梯度下降一次性使用整个训练数据来计算损失函数的梯度相比，随机梯度下降每次迭代仅使用一个训练样本来估计梯度。这样做的好处是计算成本较低，特别是在大规模数据集上，同时它也使得算法更具随机性，有助于避免陷入局部最优解。</p>
</li>
<li><p>迭代过程：SGD的迭代过程如下：</p>
<ul>
<li>初始化模型参数，例如权重和偏置。</li>
<li>将训练数据打乱顺序，以随机顺序遍历样本。</li>
<li>对于每个训练样本，计算损失函数关于模型参数的梯度。</li>
<li>使用梯度来更新模型参数，通常是通过将梯度乘以一个学习率来确定更新步长。</li>
<li>重复以上步骤，直到达到指定的迭代次数或满足停止准则（例如损失函数的收敛）。</li>
</ul>
</li>
<li><p>学习率：学习率是SGD算法中的一个重要参数，用于控制参数更新的步长。较大的学习率可能导致参数更新过大，错过最优解；而较小的学习率可能导致收敛速度缓慢。因此，选择合适的学习率是使用SGD的关键之一。在实践中，可以使用学习率调度策略来逐步减小学习率，以获得更好的收敛性能。</p>
</li>
<li><p>批量大小：除了使用单个样本计算梯度外，SGD还支持使用多个样本计算梯</p>
</li>
</ol>
<p>度。将一批样本一起计算梯度称为小批量随机梯度下降（mini-batch SGD）。小批量SGD通常比单个样本的SGD更稳定，因为它可以减少参数更新的方差，并利用并行计算的优势。批量大小是一个需要调整的超参数，通常根据可用的计算资源和训练数据的规模进行选择。</p>
<p>总结来说，SGD是一种基于梯度下降的优化算法，通过每次迭代使用一个样本（或一小批样本）的梯度来更新模型参数。它是一种高效的训练算法，特别适用于大规模数据集和复杂模型。然而，SGD也有一些缺点，例如可能陷入局部最优解和对学习率的敏感性。因此，在使用SGD时需要进行适当的参数调整和正则化技术的应用。</p>
</blockquote>
<blockquote>
<p>2.Adam算法</p>
<p>Adam是一种优化算法，全称为自适应矩估计优化算法（Adaptive Moment Estimation）。它是一种基于梯度的优化算法，结合了梯度下降和动量优化的思想。Adam算法具有较快的收敛速度和良好的性能，在深度学习领域广泛使用。</p>
<p>下面是Adam算法的详细解释：</p>
<ol>
<li><p>梯度下降：梯度下降是一种基本的优化算法，用于最小化损失函数。它通过计算损失函数关于模型参数的梯度，并朝着梯度的反方向更新参数，以逐步降低损失函数的值。</p>
</li>
<li><p>动量优化：动量优化是一种改进的梯度下降算法，引入了动量（momentum）的概念。动量表示模型更新的惯性，使得参数更新在当前梯度方向的基础上，还考虑了历史梯度的影响。这有助于加速收敛，尤其在存在平坦区域或峡谷的情况下。</p>
</li>
<li><p>自适应学习率：Adam算法引入了自适应学习率的概念，通过自动调整学习率的大小来适应每个参数的变化情况。它基于梯度的一阶矩估计（mean）和二阶矩估计（variance）来自适应地调整学习率。</p>
</li>
<li><p>Adam算法的更新步骤：</p>
<ul>
<li>初始化模型参数和累计变量（一阶和二阶矩估计的初始值）。</li>
<li>在每次迭代中，计算当前的梯度。</li>
<li>更新一阶矩估计和二阶矩估计。</li>
<li>根据一阶和二阶矩估计计算参数更新的方向和大小。</li>
<li>更新模型参数。</li>
<li>重复以上步骤，直到达到指定的迭代次数或满足停止准则。</li>
</ul>
</li>
<li><p>Adam算法的优点：</p>
<ul>
<li>自适应学习率：Adam算法可以自适应地调整每个参数的学习率，根据梯度的一阶和二阶矩估计进行缩放。这有助于在训练过程中平衡收敛速度和参数稳定性。</li>
<li>适用于大规模数据和高维参数：Adam算法对于大规模数据和高维参数的训练具有较好的效果，因为它可以有效地利用梯度信息和自适应学习率。</li>
<li>低内存要求：相对于其他优化算法（如基于Hessian矩阵的方法），Adam算法的内存要求较低，因为它仅需要存储一阶和二阶矩估计。</li>
</ul>
<p>需要注意的是，Adam算法也有一些超参数需要调整，如学习率、动量系数和指数衰减率等，这些超参数的选择可能会对算法的性能产生影响。因此，在实践中，常常需要进行超参数调优来获得最佳的性能。</p>
</li>
</ol>
</blockquote>
<h2 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020953125.png" alt="image-20230702095337036"></p>
<p>为什么会用SGD：以前的研究经常讨论SGD的三个挑战：1）大曲率损失表面，2）局部最优解，以及3）鞍点（Ruder，2016; Sun等，2020）。现代优化器已经显示出处理1）问题的有效性，并且在某些情况下可以减轻2）和3）问题。然而，当我们将范围限定为微调LLM时，这三个挑战可能会有所不同。</p>
<p>更平滑的损失面（<strong>Smoother loss surface</strong>）</p>
<p>一个重要的假设是LLM的参数空间非常平滑，对参数进行小的扰动不会显着改变损失。有实证结果和理论分析支持这一假设（Hao等，2019）。如果我们相信更大的模型具有更平滑的损失曲面，那么我们可以得出结论：由于LLM的损失表面不应具有很大的曲率，因此1）的问题不是问题。请注意，仅当我们教LLM基于自然语言的任务（或者像以前一样用代码预训练）时，才有效。与预训练任务无关的合成损失函数确实会面临大曲率问题。</p>
<p>局部最优解已经足够（<strong>Local optimum is good enough</strong>）</p>
<p>微调的目标是将LLM调整到新任务和领域中，并且不会显着改变模型本身。因此，局部最优解通常是足够好的解决方案，并且有限的训练数据（与预训练语料库相比）使其难以将模型推向遥远的全局最优解。</p>
<p>算法： LOMO中的融合更新</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021044049.png" alt="image-20230702104423997"></p>
<blockquote>
<ol>
<li><p>输入：模型f(·)具有L层和p个参数，参数θ ∈ Rp，学习率α，最大步数T，训练数据集D，损失函数L。</p>
</li>
<li><p>对于每个步骤t &#x3D; 1, . . . , T 进行以下操作：</p>
<p>2.1 从训练数据集D中随机抽取一个批次B &#x3D; (x, y)，其中x是输入数据，y是对应的目标标签。</p>
<p>2.2 通过模型f(·)进行前向传播，计算预测值ŷ： ŷ ← f(x, θ) 这一步骤用于获取模型的输出。</p>
<p>2.3 计算损失函数ℓ： ℓ ← L(y, ŷ) 这一步骤用于计算模型预测值ŷ与真实标签y之间的差异。</p>
<p>2.4 对于每一层l &#x3D; L, . . . , 1 进行以下操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1 反向传播（Backward propagation）：</span><br><span class="line">      获取当前层的参数集合θl。</span><br><span class="line">2 计算当前层的梯度gl：</span><br><span class="line">      gl ← ∂ℓ/∂θl</span><br><span class="line">      这一步骤通过计算损失函数ℓ关于当前层参数θl的偏导数来获取梯度。</span><br><span class="line"></span><br><span class="line">3 参数更新：</span><br><span class="line">      θl ← θl - α * gl</span><br><span class="line">      使用梯度下降的方式更新当前层的参数θl。学习率α确定了参数更新的步长。</span><br><span class="line">4 清除梯度：</span><br><span class="line">      将当前层的梯度gl设置为None，以清除梯度信息。</span><br></pre></td></tr></table></figure>

<p>2.5 结束当前步骤的循环。</p>
</li>
<li><p>结束算法。</p>
</li>
</ol>
</blockquote>
<blockquote>
<p>这个算法是LOMO中的融合更新算法，它使用随机梯度下降（SGD）的方式对模型的参数进行更新。它在每个步骤中，通过随机抽取一个批次样本进行前向传播和损失计算，然后进行反向传播来计算梯度，并使用梯度下降的方式更新模型的参数。整个过程重复T个步骤，直到达到最大步数T。</p>
</blockquote>
<h2 id="实验："><a href="#实验：" class="headerlink" title="实验："></a>实验：</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021046738.png" alt="image-20230702104644688"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021048215.png" alt="image-20230702104809171"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8QLORA-%E6%9C%89%E6%95%88%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/" data-id="cllkmcmel0000ksu7edo7dq8o" data-title="7月第一周QLORA/有效剪枝/lomo全参数微调" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/QLORA-%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/" rel="tag">QLORA 剪枝 lomo全参数微调</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-6月第一周RWKV渐进promptLora-1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/6%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8RWKV%E6%B8%90%E8%BF%9BpromptLora-1/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T07:47:51.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/21/6%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8RWKV%E6%B8%90%E8%BF%9BpromptLora-1/">6月第一周RWKV渐进promptLora</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="6月第一周RWKV-渐进prompt-Lora"><a href="#6月第一周RWKV-渐进prompt-Lora" class="headerlink" title="6月第一周RWKV&#x2F;渐进prompt&#x2F;Lora"></a>6月第一周RWKV&#x2F;渐进prompt&#x2F;Lora</h1><h1 id="论文1：RWKV-Reinventing-RNNs-for-the-Transformer-Era"><a href="#论文1：RWKV-Reinventing-RNNs-for-the-Transformer-Era" class="headerlink" title="论文1：RWKV: Reinventing RNNs for the Transformer Era"></a>论文1：RWKV: Reinventing RNNs for the Transformer Era</h1><p>RWKV：为Transformer时代重塑RNN</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.13048.pdf">2305.13048.pdf (arxiv.org)</a></p>
</blockquote>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230609162340.png" alt="image-20230609162340406"></p>
<ul>
<li>a. 本文的研究背景:<ul>
<li>通过介绍RNN和Transformer的优劣势，针对长序列的任务提出了Receptance Weighted Key Value (RWKV)模型，将Transformer和RNN结合起来，实现并行可扩展性及高效率。</li>
</ul>
</li>
<li>b. 过去的方法及其问题和动机:<ul>
<li>介绍了替换或修改神经网络中的注意力机制以实现长序列的可扩展性的各种模型，其中包括MLP-Mixer、Attention Free Transformer (AFT)、Recurrent Memory Transformer和Linear Recurrent Units等。</li>
</ul>
</li>
<li>c. 本文提出的研究方法:<ul>
<li>提出了RWKV模型，利用时间混合组件，将长序列任务的Transformer和RNN的优点相结合，采用线性注意力机制，提高了可扩展性和并行可训练性，同时引入了多种策略以捕捉局部性和长程依赖关系。</li>
</ul>
</li>
<li>d. 方法在任务中的表现:<ul>
<li>在基准数据集上进行了全面系列的实验，在与相似大小的Transformer相当的性能的同时，在规模从1.69亿到140亿的Pile上训练了一个预先训练模型。</li>
</ul>
</li>
</ul>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景:"></a>背景:</h2><ul>
<li>a. 主题及其特点:<ul>
<li>本文的主题为神经网络的注意力机制在处理长序列时面临的挑战，以及RWKV模型的解决方法。</li>
</ul>
</li>
<li>b. 历史发展:<ul>
<li>在过去的相关研究中，RNNs在序列处理任务中具有高效的计算能力，但由于存在并行性和可扩展性方面的限制，无法达到Transformer的性能表现。对于长序列任务而言，Transformer具有卓越的性能，但由于其内存和计算复杂度随序列长度呈二次增长，其对于大规模模型的处理存在困难。</li>
</ul>
</li>
<li>c. 过去的方法:<ul>
<li>过去的研究着眼于替换或修改神经网络中的注意力机制。例如，MLP-Mixer等模型将注意力机制替换为多层感知机 (MLPs)；Attention Free Transformer (AFT)将点积自注意力机制替换为计算效率较高的替代机制；Recurrent Memory Transformer和Linear Recurrent Units等模型则将RNN风格的递归组件进行了修改以增加局部上下文的长度；S4等基于状态空间的模型 (SSM) 也被提出。</li>
</ul>
</li>
<li>d. 过去研究的不足:<ul>
<li>过去的研究中存在的问题包括：替换机制的有效性和性能、长序列的可扩展性、模型的拟合能力、训练效率等问题。</li>
</ul>
</li>
<li>e. 解决当前问题的必要性:<ul>
<li>随着神经网络在各个领域的应用，越来越多的任务需要处理长序列数据，因此提高神经网络的可扩展性和计算效率尤为重要。</li>
</ul>
</li>
</ul>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h2><ul>
<li>a. 研究的理论基础:<ul>
<li>RWKV模型利用时间混合组件融合了Transformer和RNN的优点，采用线性注意力机制提高了可扩展性和并行可训练性。</li>
</ul>
</li>
<li>b. 文章的技术路线:<ul>
<li>RWKV模型由多个堆叠的残差块组成，在其中包含时间混合和通道混合子块。回归过程的公式化表达如方程14所示，该模型可在时间并行模式下高效并行化。模型的性能在多种NLP任务中进行了基准测试，证明了其训练和内存效率方面的良好表现。</li>
</ul>
</li>
<li>c. 方法的创新性、性能和工作量:<ul>
<li>RWKV模型通过比较特殊的初始化技巧以及乘性交互方法，使得引入线性复杂度的注意力机制变得可行，同时提供了新技术来捕捉局部性和长程依赖关系， 改进了Transformer的局限性。该方法与以前的相似方法不同，RWKV模型采用可并行化和可扩展训练的形式。它在性能上与Transformer相当，并且展现了处理大规模模型的潜力。</li>
</ul>
</li>
<li>d. 研究结论:<ul>
<li>RWKV模型的出现解决了长序列任务中RNN和Transformer所面临的各自的限制。但本文也强调了模型的潜在局限性，包括模型的能力可能受限于之前token的数量，以及 prompt engineering的重要性。最后，该文总结了与RWKV的分块计算方案类似的最近工作，并讨论了无注意力模型作为改进Transformer效率的替代方法。RWKV开辟了一扇新门，可用于建模序列数据中的复杂关系。</li>
</ul>
</li>
</ul>
<h2 id="模型架构图"><a href="#模型架构图" class="headerlink" title="模型架构图"></a>模型架构图</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110548.png" alt="image-20230609204310375"></p>
<ol>
<li><p>Time Mixing</p>
<ol>
<li><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110617.png" alt="image-20230609204719541"></p>
</li>
<li><blockquote>
<p>这个函数描述了处理一个经过RNN网络后的状态的操作。它针对这个状态进行了三种操作，混合、扩展和归一化，并计算输出。这些操作通过一组输入变量(the current input state(x), last input state(last_x), numerator(last_num), denominator(last_den), exponential decay(decay), bonus值(bonus), mixing_coefficients(mix_k&#x2F;mix_v&#x2F;mix_r), query、key和value的weight(Wk, Wv, Wr)以及输出的weight(Wout))来实现。混合是一种对 <code>x</code> 和 <code>last_x</code> 执行加权操作的方法，其中参数是 <code>mix_k/mix_v/mix_r</code>。对于这个混合的结果计算权重 <code>kw</code>, <code>kw</code>取决于值的相似度和重要性。其中，重要性通过进一步加权得到，这里使用exp函数来实现。接下来，<code>numerator</code>和<code>denominator</code>分别用于计算 <code>kw</code>的平均值和标准差，然后再将这个值乘以查询结果得到输出结果。最后函数返回了两个值，输出结果和更新后的状态值。</p>
</blockquote>
</li>
</ol>
</li>
<li><p>Channel Mixing</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110845.png" alt="image-20230609205438997"></p>
<blockquote>
<p>这个函数执行一种基于 <code>x</code>和 <code>last_x</code>的加权和归一化操作，通过一组权重来实现，并返回一个新的值。权重是通过查询值和重要性(用sigmod函数计算)来计算的，其中的值经过了一些加权操作，包括在第二个权重层计算关键值。这个函数仍然返回两个值，输出结果以及状态值。</p>
</blockquote>
<h3 id="增加：传统transformer模型：生成长度为T的序列，需要O-T-2-复杂度"><a href="#增加：传统transformer模型：生成长度为T的序列，需要O-T-2-复杂度" class="headerlink" title="增加：传统transformer模型：生成长度为T的序列，需要O(T^2) 复杂度"></a>增加：传统transformer模型：生成长度为T的序列，需要O(T^2) 复杂度</h3><p>令 F[t] 为 t 时刻的系统状态（高维矢量）。</p>
<p>令 x[t] 为 t 时刻的外部输入信息状态。</p>
<p>预测 F[t+1] 时，需考虑 F[0], F[1], .. F[t]。因此，生成长度 T 的序列，需 O(T^2) 复杂度。</p>
<p>简化版本的公式：</p>
<p>$$<br>F[\mathrm{t}+1]&#x3D;\frac{\sum_{\mathrm{i}&#x3D;0}^{\mathrm{t}} \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}]) \cdot(\mathbf{V}F[\mathrm{i}])}{\sum_{\mathrm{i}&#x3D;0}^{\mathrm{t}} \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}])}<br>$$<br>这里 <strong>Q K V</strong> 是三个可训练的矩阵。</p>
<p>其意义为：</p>
<ul>
<li><p>每个状态 i 对于后续的潜在贡献是 <strong>V</strong>F[i]。</p>
</li>
<li><p>用 <strong>Q</strong>x[t] 矢量，与此前的所有 <strong>K</strong>F[i] 矢量分别做点乘，再 exp，得到 x[t] 与之前各个 F[i] 状态的匹配度。（为什么复杂度高）</p>
</li>
<li><p>如果匹配度<br>$$<br> \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}])<br>$$</p>
<p>越大，<strong>V</strong>F[i] 的权重越大。</p>
</li>
<li><p>分母为归一化因子。</p>
</li>
</ul>
<h2 id="RWKV-2-模型：生成长度-T-的序列，只需-O-T-复杂度（"><a href="#RWKV-2-模型：生成长度-T-的序列，只需-O-T-复杂度（" class="headerlink" title="RWKV-2 模型：生成长度 T 的序列，只需 O(T) 复杂度（&#x3D;&#x3D;"></a><strong>RWKV-2 模型：生成长度 T 的序列，只需 O(T) 复杂度</strong>（&#x3D;&#x3D;</h2><h2 id="启发：用于图片生成或分类transformer改造-）"><a href="#启发：用于图片生成或分类transformer改造-）" class="headerlink" title="启发：用于图片生成或分类transformer改造&#x3D;&#x3D;）"></a>启发：用于图片生成或分类transformer改造&#x3D;&#x3D;）</h2><p>$$<br>F[\mathrm{t}+1]&#x3D;\sigma(\mathbf{R}x[\mathrm{t}]) \cdot \frac{\sum_{\mathrm{i}&#x3D;0}^{\mathrm{t}} \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K}F[\mathrm{i}]) \cdot(\mathbf{V}F[\mathrm{i}])}{\sum_{\mathrm{i}&#x3D;0}^{\mathrm{t}} \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K }F[\mathrm{i}])}<br>$$</p>
<p>这里 <strong>R K V</strong> 是三个可训练的矩阵，<strong>W</strong> 是一个可训练的矢量（代表时间衰减率）。</p>
<p>其意义为：</p>
<ol>
<li><p>每个状态 i 对于后续的潜在贡献&#x3D;&#x3D;是 <strong>V</strong>F[i]。&#x3D;&#x3D;</p>
</li>
<li><p>匹配度由</p>
</li>
<li><p>$$<br>\exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}]) 改为 \sigma(\mathbf{R}x[\mathrm{t}]) \cdot \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K}F[\mathrm{i}])<br>$$</p>
</li>
<li><p>决定，其中</p>
</li>
<li><p>$$<br>中 \sigma 是非线性函数，经实验采用 sigmoid 函数的效果较好。注意 \sigma(\mathbf{R}x[\mathrm{t}]) 不参与归一化，所以将 R 称为 receptance。<br>$$</p>
</li>
<li><p>$$<br>这里 \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) 是显式的距离因子<br>$$</p>
</li>
<li><p>我们进一步变换，将其变为 RNN 递归形式。即，生成 F[t+1] 时，只需考虑 x[t]，以及固定大小的隐状态 A[t] 和 B[t]（A[t] 和 B[t] 是上一步的分子和分母。），无需与此前 F[i] 都进行计算。因此，生成长度 T 的序列，只需 O(T) 复杂度。</p>
</li>
<li><p>$$<br>F[1]&#x3D;\sigma(\mathbf{R }x[0]) \cdot \frac{ \exp (\mathbf{K }F[0]) \cdot(\mathbf{V }F[0])}{\exp (\mathbf{K }F[0])}</p>
<p>F[2]&#x3D;\sigma(\mathbf{R }x[1]) \cdot \frac{ \exp (\mathbf{K }F[1]) \cdot(\mathbf{V }F[1])+\exp (\mathbf{W} ) \cdot \exp (\mathbf{K }F[0]) \cdot(\mathbf{V }F[0])}{ \exp (\mathbf{K }F[1])+\exp (\mathbf{W} ) \cdot \exp (\mathbf{K }F[0])}<br>$$</p>
</li>
<li><p>可以推导得到</p>
</li>
<li><p>$$<br>F[t+1]&#x3D;\sigma(\mathbf{R }x[t]) \cdot \frac{\exp (\mathbf{K}F[\mathrm{t}]) \cdot(\mathbf{V}F[\mathrm{t}])+\exp (\mathbf{W}) \cdot A[\mathrm{t}]}{ \exp (\mathbf{K}F[\mathrm{t}])+\exp (\mathbf{W}) \cdot B[\mathrm{t}]}<br>$$</p>
</li>
</ol>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110854.png" alt="image-20230609210057755"></p>
<p>不同LLM的文本生成期间的累积时间。</p>
<h2 id="Zero-Shot-Performance-of-the-model-on-Common-Sense-Reasoning-Tasks"><a href="#Zero-Shot-Performance-of-the-model-on-Common-Sense-Reasoning-Tasks" class="headerlink" title="Zero-Shot Performance of the model on Common Sense Reasoning Tasks."></a>Zero-Shot Performance of the model on Common Sense Reasoning Tasks.</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110856.png" alt="image-20230609210350167"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110903.png" alt="image-20230609210608382"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110900.png" alt="image-20230609210627551"></p>
</li>
</ol>
<h1 id="chatpaper相关"><a href="#chatpaper相关" class="headerlink" title="chatpaper相关"></a>chatpaper相关</h1><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/613055271">我把ChatPaper开源了！用来速读PDF和刷ArXiv论文 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://juejin.cn/post/7220775341727399991">推荐几个可以免费使用的ChatGPT工具 - 掘金 (juejin.cn)</a></p>
</blockquote>
<p>之前准备写一篇专门介绍上述工具类的原理介绍（其实ChatGPT的 插件——chatgpt-retrieval-plugin），但是后来查看了几个项目的源码之后发现，这类工具的主要原理其实比较直观：</p>
<ul>
<li>解析相关输入为文本</li>
<li>将文本分句后获取句子的embedding（这一步目前处理的处理方式大都是根据长度截断）并存储至数据库</li>
<li>用户输入转换为embedding，并在数据库中召回相关性最高的句子集合langchain</li>
<li>将召回的句子与用户输入句子组装为ChaptGPT的输入，获取输出</li>
</ul>
<p>上述思路虽然直观，但要获取更好的结果，其实除了第三步，其余每一步都有优化的空间：</p>
<ul>
<li>文本解析可以针对不同类型的数据针对性解析</li>
<li>文本分句方式可以采取特殊标点进行分句，同时句子embedding也有很多可选生成方法</li>
<li>召回的句子与用户输入句子组装为ChaptGPT的输入，结合任务特定的prompt，获取更适合任务的输出</li>
</ul>
<p>参考langchain</p>
<h1 id="论文2：Progressive-Prompts-Continual-Learning-for-Language-Models-《渐进提示：持续学习的语言模型》"><a href="#论文2：Progressive-Prompts-Continual-Learning-for-Language-Models-《渐进提示：持续学习的语言模型》" class="headerlink" title="论文2：Progressive Prompts: Continual Learning for Language Models 《渐进提示：持续学习的语言模型》"></a>论文2：Progressive Prompts: Continual Learning for Language Models 《渐进提示：持续学习的语言模型》</h1><h2 id="启发：经典论文阅读，"><a href="#启发：经典论文阅读，" class="headerlink" title="启发：经典论文阅读，"></a>启发：经典论文阅读，</h2><p>ICLR2023</p>
<blockquote>
<p>[[2301.12314] 渐进式提示：语言模型的持续学习 (arxiv.org)](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.12314#:~:text=We">https://arxiv.org/abs/2301.12314#:~:text=We</a> introduce Progressive Prompts - a simple and,replay or a large number of task-specific parameters.)</p>
</blockquote>
<h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><ul>
<li>a. 这篇文章的研究背景:<ul>
<li>本文介绍了一种新的连续学习方法Progressive Prompts，用于语言模型的学习。</li>
</ul>
</li>
<li>b. 以往的方法、问题和动机:<ul>
<li>以往的方法存在遗忘问题，在新增任务学习后，之前学习的任务的性能会下降，这限制了语言模型的应用。</li>
</ul>
</li>
<li>c. 本文提出的研究方法:<ul>
<li>Progressive Prompts方法，通过重新描述提示嵌入和添加剩余连接，以防止遗忘，提高性能。</li>
</ul>
</li>
<li>d. 方法在任务上达到的性能:<ul>
<li>在多个数据集上进行测试，结果表明，Progressive Prompts方法在15个文本分类任务中表现优异，比之前的方法平均测试准确率提高20%以上，同时在长序列的任务中也表现出色。</li>
</ul>
</li>
</ul>
<h1 id="背景-1"><a href="#背景-1" class="headerlink" title="背景:"></a>背景:</h1><ul>
<li>a. 主题和特征:<ul>
<li>本文的主题为语言模型的连续学习方法研究，旨在应对以往方法在遗忘问题上的短板。</li>
</ul>
</li>
<li>b. 历史发展:<ul>
<li>以往的连续学习方法遇到了遗忘问题的挑战，需要一个新的方法来应对。</li>
</ul>
</li>
<li>c. 以往的方法:<ul>
<li>以往的方法主要采用重放机制，在每个任务后，使用之前学习任务的数据重新训练模型，但是这样有可能会忘记已掌握的知识。</li>
</ul>
</li>
<li>d. 以往研究中的不足:<ul>
<li>以往的连续学习方法限制了语言模型的应用范围，需要一种能够防止遗忘并提高性能的方法。</li>
</ul>
</li>
<li>e. 当前需要解决的问题:<ul>
<li>如何解决遗忘问题，并且提高模型性能，以便扩大应用范围。</li>
</ul>
</li>
</ul>
<h1 id="方法-1"><a href="#方法-1" class="headerlink" title="方法:"></a>方法:</h1><ul>
<li>a. 本研究的理论基础:<ul>
<li>Progressive Prompts方法，通过重新描述提示嵌入和添加剩余连接，以防止遗忘，提高性能。</li>
</ul>
</li>
<li>b. 文章的技术路线 (逐步):<ul>
<li>(1) 采用模型无关的方式重新描述提示嵌入，使用多层感知器 (MLP) 对提示嵌入进行重新参数化；</li>
<li>(2) 添加剩余连接，以改进优化和避免学习身份映射；</li>
<li>(3) 在多个数据集上进行测试，以比较其性能和可扩展性。</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110907.png" alt="image-20230610085314675"></p>
<blockquote>
<p>第一种方法叫做”progressive networks using prompt tuning”，简称”简单适应法”。这种方法是为每个新任务学习一个单独的提示(prompt)，并且对于每个新任务都会重复使用之前冻结的输入嵌入(frozen input embeddings)。这意味着每个任务都需要重复输入令牌(input tokens)。而第二种方法是”Progressive Prompts”，简称”逐步提示法”。在这种方法中，我们使用相同的输入，并逐步添加新的提示来处理每个新任务。添加新提示时不会修改先前任务的提示。简单来说，逐步提示法是一种更为高效的方法，不需要每次都重复输入令牌，而是逐渐添加新的提示来处理新任务。</p>
</blockquote>
<p>体现在公式上：<br>$$<br>\mathcal{L}(\theta_{P_k})&#x3D;-\sum_{x,y\in T_k}\log p(y|[P_k,…,P_1,x],\theta,\theta_{P_1},…,\theta_{P_k})<br>$$</p>
<blockquote>
<p>这个公式是用来计算在训练当前任务Tk时，对于当前任务的提示参数θPk进行训练的损失函数。其中，x和y分别是当前任务Tk的输入和输出，[P_k,…,P_1,x]表示在之前所有任务的提示P1到Pk以及当前输入x的基础上产生当前输出y的条件概率，这个概率是由语言模型(LM)的参数θ以及提示参数θPk共同决定的。损失函数的目的是最小化负对数似然(log-likelihood)的值，即最大化对数似然函数值。在这个公式中，θ表示LM的参数，θPk表示当前任务的提示参数，其中θ是不可训练的，而θPk只在学习当前任务的过程中可以训练，之后将被冻结。</p>
<p>这个方法的主要目的是在每个新任务中，逐步学习一个任务相关的提示，然后将该提示与之前的所有任务的提示拼接起来，并在输入嵌入前加入该提示。对于每个任务，都需要训练相应的提示参数θPk，以最大化预测正确的概率。可以通过逐步改变所有提示参数的值来使预测输出更准确。此外，在学习每个新任务时，之前学习的任务提示参数将会被冻结，只有当前任务的提示参数可以进行训练。</p>
</blockquote>
<h1 id="增加：公式解读："><a href="#增加：公式解读：" class="headerlink" title="增加：公式解读："></a>增加：公式解读：</h1><p>首先是prompt tuning常使用的目标函数如下：<br>$$<br>\max_{\theta, \theta_P} \log p_{\theta, \theta_P}(y | [P; x])<br>$$</p>
<p>$$</p>
<p>$$</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202306171000958.png" alt="image-20230617095731351"></p>
<p>在这项工作中，我们关注的是一种连续学习的设置，其中语言模型将连续地面对一系列的文本分类任务（T_1, \ldots, T_m）。在每个任务中，我们有一组独立同分布的训练样本 (x_i, y_i)<em>{i&#x3D;1}^N，其中 x_i 是输入文本，y_i 是与任务 T_k 相关联的一组预定义标签 Y_k 中的一个标签。我们假设模型的参数为 \Theta，并且在训练和推断过程中可以获得任务的标识信息。因此，跨所有任务的学习目标可以表示为：<br>$$<br>\max</em>{\Theta} \sum_{k&#x3D;1}^m \sum_{(x,y)\in T_k} \log p_{\Theta}(y|x)<br>$$<br>连续学习中最直接的方法是微调（finetuning），它通过按顺序更新所有模型参数来逐个优化每个任务 k（其中 k 的范围是从 1 到 m）的损失函数。任务 k 的损失函数，记为 L_k(\Theta)，可以表示为:<br>$$<br>L_k(\Theta) &#x3D; -\sum_{(x,y)\in T_k} \log p(y|x, \Theta)<br>$$<br>尽管连续微调可以向未来任务传递前向知识，但它经常导致灾难性遗忘。灾难性遗忘是指在学习新任务后，之前任务的性能下降，并最终导致更高的泛化损失。</p>
<p>Progressive Prompts是一种连续学习方法，它为每个新任务Tk逐步学习一个提示P<sub>k</sub>（见图1）。通过Progressive Prompts，我们为任务Tk学习一个独立的提示P<sub>k</sub>，并将其与之前学习的所有提示P<sub>i</sub>（其中i &lt; k）连接起来，然后将其添加到输入嵌入之前。在训练过程中，语言模型的参数θ始终保持冻结，而与提示P<sub>k</sub>相关的参数θP<sub>k</sub>仅在学习任务Tk时可训练，然后冻结。</p>
<p>任务Tk（其中k ∈ {1…m}）的训练目标是找到提示参数θP<sub>k</sub>，使得在我们的逐步提示和冻结的基础模型下，训练样本的负对数概率最小化：</p>
<p>$$<br>L(θPk) &#x3D; − Σ(log p(y|[Pk, …, P1, x], θ, θP1, …, θPk))<br>$$</p>
<p>其中x,y∈Tk表示任务Tk中的训练示例，p(y|[Pk, …, P1, x], θ, θP1, …, θPk)表示在给定提示序列[Pk, …, P1]、模型参数θ和提示参数θP1, …, θPk的情况下预测目标y的条件概率。</p>
<p>简而言之，Progressive Prompts通过学习逐步添加的提示序列，将每个任务的特定信息引入到语言模型中，同时保持之前任务的知识。这种方法通过冻结基础模型的参数，只训练特定任务的提示参数，从而避免了对整个模型进行大规模调整，提高了模型的可扩展性和效果。</p>
<p>不同点：</p>
<ol>
<li>渐进学习：Progressive Prompts通过逐步学习的方式处理每个新任务。对于每个新任务Tk，系统学习一个独立的提示P_k，该提示包含了任务特定的信息。这种渐进的学习方式允许模型在处理新任务时逐步积累知识。</li>
<li>提示序列：Progressive Prompts引入了一个提示序列[P_k, …, P_1]，其中P_k是针对任务Tk的提示，P_i是之前任务Ti的提示（i &lt; k）。在模型输入中，将提示序列与输入嵌入进行连接，以将任务特定的信息引入到模型中。</li>
<li>提示参数：与渐进学习相对应的是冻结和可训练的参数。在Progressive Prompts中，模型参数θ是冻结的，不进行训练，而与每个任务相关的提示参数θPk只在学习任务Tk期间进行训练，然后被冻结。这种方式保持了基础模型的稳定性，只对特定任务的提示参数进行更新，避免了对整个模型进行大规模调整。</li>
</ol>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论:"></a>结论:</h1><ul>
<li>a. 工作的意义:<ul>
<li>本文介绍了一种新的连续学习方法Progressive Prompts，解决了以往方法遗忘问题和性能下降问题，具有重要的实际应用价值。</li>
</ul>
</li>
<li>b. 创新、性能和工作量:<ul>
<li>Progressive Prompts方法只需要训练语言模型总参数的0.1%，是传统连续学习方法的轻量级替代方法，具有更好的性能和扩展性。</li>
</ul>
</li>
<li>c. 研究结论:<ul>
<li>在多个数据集上的测试结果表明，Progressive Prompts方法在15个文本分类任务中表现优异，能够防止遗忘并提高性能。在长序列的任务中，也比之前的方法表现出色。</li>
</ul>
</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230612154037.png" alt="image-20230610090348552"></p>
<blockquote>
<p>Finetune：在任务序列上训练所有模型参数（不添加任何正则化约束或重放来自先前任务的样本）。</p>
<p>· EWC：使用正则化损失来微调整个模型，以防止更新可能干扰先前学习的任务的参数。</p>
<p>· A-GEM：保存过去任务中的示例，并基于检索到的示例限制用于在新任务上更新模型的梯度。·</p>
<p>体验回放：使用内存缓冲区微调整个模型，并在学习新任务时重放旧任务的样本以避免遗忘。·</p>
<p>MBPA++（de Masson D ‘Autume等人，2019）：用情景记忆增强BERT，保存所有看到的例子。在训练期间执行回放，在测试期间执行本地调整。· </p>
<p>IDBR（Huang等人，2021）：BERT特定的方法，使用数据重放和正则化损失连续训练整个模型，将句子表示分解应用到特定于任务的空间和任务通用空间。当前SOTA对BERT的CL基准测试。·</p>
<p>progprompt：为每个任务训练单独的软提示，同时保持原始模型冻结。这种设置将消除灾难性遗忘，因为当学习新任务时，每个任务的参数不会改变，但不会导致向前传输。</p>
<p>·PromptTuning：在所有任务上按顺序训练共享软提示，同时保持原始模型参数冻结。</p>
<p>· LFPT5：连续地训练软提示，该软提示同时学习解决任务并生成训练样本，该训练样本随后用于体验重放。当前SOTA对T5的CL基准测试。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211548121.png" alt="image-20230610090708450"></p>
<blockquote>
<p>表2比较了常见的CL方法，包括T5和BERT模型的SOTA方法（Qin &amp; Joty，2021; Huang等人，2021年），连续学习15个任务。我们报告了T5-Large和BERT-base模型获得的三个任务订单（8，9和10）的平均结果。我们在附录B.1中提供了每个订单的完整非平均结果。为了研究有限数据设置的影响，我们在不同的数据集大小上进行训练，每个类有20，200和1000个样本。</p>
<p>MTL表示多任务学习。标有 * 的方法只训练一个软提示，同时保持模型冻结，其他方法训练整个模型。</p>
</blockquote>
<h1 id="论文3-LORA-LOW-RANK-ADAPTATION-OF-LARGE-LANGUAGE-MODELS"><a href="#论文3-LORA-LOW-RANK-ADAPTATION-OF-LARGE-LANGUAGE-MODELS" class="headerlink" title="论文3 LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS"></a>论文3 LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</h1><h2 id="LoRA-大模型的低秩自适应微调模型-qlora"><a href="#LoRA-大模型的低秩自适应微调模型-qlora" class="headerlink" title="LoRA:大模型的低秩自适应微调模型 qlora"></a>LoRA:大模型的低秩自适应微调模型 qlora</h2><h3 id="启发：用在cv-qlora-怎么找到分解的矩阵"><a href="#启发：用在cv-qlora-怎么找到分解的矩阵" class="headerlink" title="启发：用在cv qlora 怎么找到分解的矩阵"></a>启发：用在cv qlora 怎么找到分解的矩阵</h3><h1 id="基本信息："><a href="#基本信息：" class="headerlink" title="基本信息："></a>基本信息：</h1><ul>
<li>标题： LORA：大型语言模型的低秩适应 （LORA： 大型语言模型的低秩自适应）</li>
<li>作者：Edward Hu， Yelong Shen， Phillip Wallis， Zeyuan Allen-Zhu， Yuan， Yuanzhi Li， Shean Wang， Lu Wang， Weizhu Chen</li>
<li>隶属关系： 微软公司</li>
<li>关键词：自然语言处理;预培训;微调;低秩适配</li>
<li>网址： <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685%EF%BC%8C">https://arxiv.org/abs/2106.09685，</a> GitHub： <a target="_blank" rel="noopener" href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a></li>
</ul>
<h1 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h1><ul>
<li>a. 本文研究背景：<ul>
<li>简述了适用于下游任务大型预训练语言模型的低秩自适应方法LoRA的提出</li>
</ul>
</li>
<li>b. 过去的方法、问题和动机：<ul>
<li>描述了adapter层和prompt直接优化等现有方法的缺点，以及提出本文方法的动机</li>
</ul>
</li>
<li>c. 本文提出的研究方法：<ul>
<li>本文提出了在预训练模型的每个层中引入可训练秩分解矩阵的方法，从而大大降低下游任务的可训练参数数量</li>
</ul>
</li>
<li>d. 方法在下游任务中的表现：<ul>
<li>在RoBERTa、DeBERTa、GPT-2和GPT-3等模型下，本文提出的方法可以使可训练参数和GPU内存需求分别减少10，000倍和3倍，并保持达到或超过模型质量的水平。本文所提出的方法计算和内存效率均优于完全微调方法，无需增加推理延迟，与其他方法兼容</li>
</ul>
</li>
</ul>
<h1 id="背景："><a href="#背景：" class="headerlink" title="背景："></a>背景：</h1><ul>
<li>a. 主题和特点：<ul>
<li>本文主题是下游任务大型预训练语言模型的自适应问题，特点在于提出了使用可训练秩分解矩阵的方法</li>
</ul>
</li>
<li>b. 历史发展：<ul>
<li>描述了早期下游任务自适应方法的缺点和局限性</li>
</ul>
</li>
<li>c. 过去的方法：<ul>
<li>描述了adapter层和prompt直接优化等现有方法的问题，以及本文方法的创新之处</li>
</ul>
</li>
<li>d. 过去研究的缺陷：<ul>
<li>描述了现有方法的推理延迟、计算和内存效率不高等问题</li>
</ul>
</li>
<li>e. 当前需要解决的问题：<ul>
<li>描述了大规模生产中的适应问题，如何提高计算和内存效率</li>
</ul>
</li>
</ul>
<h1 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h1><ul>
<li>a. 研究的理论基础：<ul>
<li>描述了使用可训练秩分解矩阵的理论基础</li>
</ul>
</li>
<li>b. 文章的技术路线（步骤）：<ul>
<li>对于预训练模型的注意力权重，本文利用了低秩参数化和秩分解矩阵的方法</li>
<li>本文方法对每个预训练模型层进行秩分解，有效减少可训练参数数量</li>
<li>本文方法可以切换在下游任务中是否冻结MLP和LayerNorm层</li>
</ul>
</li>
<li>c. 实现效果：<ul>
<li>描述了在RoBERTa、DeBERTa、GPT-2和GPT-3等模型下，本文所提出的方法可以使可训练参数和GPU内存需求分别减少10，000倍和3倍，并保持达到或超过模型质量的水平</li>
<li>描述了本文所提出的方法计算和内存效率均优于完全微调方法，无需增加推理延迟，与其他方法兼容</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110917.png" alt="image-20230610092930638"></p>
<blockquote>
<p><strong>对于预训练的权重矩阵W0，可以让其更新受到用低秩分解表示后者的约束:</strong><br>$$<br>W_{0}+\Delta W &#x3D;W_0+BA,<br>$$<br><strong>在训练过程中，W0被冻结，不接受梯度更新，而A和B包含可训练参数。当h&#x3D;W0x时，修正后的正向传播变为:</strong><br>$$<br>h&#x3D;W_0x+\Delta Wx&#x3D;W_0x+BAx<br>$$<br><strong>对A使用随机高斯初始化，对B使用零初始化，因此ΔW&#x3D;BA在训练开始时为零（这点需要注意）。</strong></p>
<p><strong>这种方法的一个优点是，当部署到生产环境中时，只需要计算和存储W&#x3D;W0+BA，并像往常一样执行推理。与其他方法相比，没有额外的延迟，因为不需要附加更多的层。</strong></p>
<p><strong>在Transformer体系结构中，自关注模块中有四个权重矩阵(Wq、Wk、Wv、Wo)， MLP模块中有两个权重矩阵。LoRA只对下游任务调整关注权重，并冻结MLP模块。所以对于大型Transformer，使用LoRA可减少高达2&#x2F;3的VRAM使用量。比如在GPT-3 175B上，使用LoRA可以将训练期间的VRAM消耗从1.2TB减少到350GB。</strong></p>
</blockquote>
<p>补充公式解释：</p>
<blockquote>
<ol>
<li>神经网络中的权重矩阵：神经网络包含多个密集层，这些层执行矩阵乘法操作。这些操作涉及权重矩阵，该矩阵在每一层中都有不同的值。</li>
<li>全秩权重矩阵和内在维度：通常情况下，这些权重矩阵是全秩的，即它们的秩等于它们的最小维度。然而，Aghajanyan等人（2020）的研究表明，预训练的语言模型具有较低的”内在维度”，即尽管权重矩阵具有全秩，但它们在实际学习中可以被投影到较小的子空间中。</li>
<li>假设和约束：基于上述观察，论文假设权重矩阵在适应期间也具有较低的”内在秩”。为了约束权重的更新，他们使用了低秩分解。对于预训练的权重矩阵W0，它被分解为W0 + ΔW，其中B和A是较低秩的矩阵（B的维度为d×r，A的维度为r×k），并且秩r远小于min(d, k)。这样的低秩分解有助于减少模型参数和计算成本。</li>
<li>参数更新和冻结：在训练期间，原始的预训练权重矩阵W0被冻结，不接收梯度更新，而A和B成为可训练的参数。因此，更新的权重矩阵为W0 + ΔW，其中ΔW &#x3D; BA。这样的约束确保了更新的权重矩阵仍然具有较低的秩。</li>
<li>前向传播和缩放：在前向传播过程中，对于输入x，修改后的权重矩阵的前向传播计算为h &#x3D; W0x + ΔWx &#x3D; W0x + BAx。这样的修改确保了权重矩阵与原始输入相乘后，它们各自的输出向量在坐标方向上相加。</li>
<li>缩放因子αr：在训练开始时，ΔW &#x3D; BA为零，通过对A使用随机高斯初始化，对B使用零初始化。然后，用常数αr来缩放ΔWx，其中α是r中的常数。在使用Adam等优化算法进行训练时，适当缩放初始化可以使α的调整类似于学习率的调整。因此，只需将α设置为第一个尝试的r值，而不进行进一步的调整。这样的缩放有助于减少重新调整超参数的需求。</li>
</ol>
</blockquote>
<h2 id="低秩分解用于transformer架构的好处"><a href="#低秩分解用于transformer架构的好处" class="headerlink" title="低秩分解用于transformer架构的好处"></a>低秩分解用于transformer架构的好处</h2><blockquote>
<p>将低秩分解应用于Transformer模型中的注意力权重矩阵的好处和限制。</p>
<ol>
<li>权重矩阵子集：在Transformer架构中，自注意力模块包含四个权重矩阵（Wq、Wk、Wv、Wo），而MLP模块包含两个权重矩阵。这里的讨论限制在仅调整注意力权重，即Wq（或Wk、Wv），并将MLP模块冻结，不对其进行训练。</li>
<li>好处：最显著的好处是减少内存和存储使用量。对于大型的Transformer模型，在使用Adam优化器训练时，如果秩r远小于注意力权重矩阵的维度dmodel，那么VRAM（显存）的使用量可以减少2&#x2F;3，因为不需要存储优化器状态的冻结参数。例如，在GPT-3175B模型上，训练期间的VRAM消耗从1.2TB减少到350GB。此外，当r&#x3D;4且仅调整查询和值投影矩阵时，检查点文件的大小约减少了10000倍，从350GB减少到35MB。这使得可以使用较少的GPU进行训练，并避免I&#x2F;O瓶颈。另一个好处是，在部署时可以以更低的成本切换任务，只需交换低秩分解的权重，而不需要交换所有参数。这允许创建许多定制模型，可以在将预训练权重存储在VRAM中的机器上进行动态切换。此外，相对于完全微调（fine-tuning）的方法，观察到GPT-3175B在训练期间的速度提高了25%，因为不需要计算大多数参数的梯度。</li>
<li>限制：这种方法的限制是它仅适用于调整注意力权重，而不调整MLP模块的权重。这意味着MLP模块在下游任务中不会被训练，可能会限制其适应性和性能。此外，低秩分解的方法也会引入近似误差，因为原始的全秩权重矩阵被近似为低秩的矩阵。</li>
</ol>
<p>总之，将低秩分解应用于Transformer模型中的注意力权重矩阵可以减少内存和存储使用量，并提高训练速度。这种方法还允许在部署时以较低的成本切换任务，并创建定制模型。然而，它的限制是仅适用于注意力权重，而MLP模块不会进行训练，并且近似误差可能会对模型性能产生影响。</p>
</blockquote>
<h3 id="问题补充1：什么是低秩分解？"><a href="#问题补充1：什么是低秩分解？" class="headerlink" title="问题补充1：什么是低秩分解？"></a>问题补充1：什么是低秩分解？</h3><blockquote>
<p>低秩分解是一种矩阵分解的技术，它将一个高维矩阵分解为两个或多个低维矩阵的乘积。这种分解可以帮助我们降低数据的维度，提取数据中的重要信息，并减少存储和计算的成本。</p>
<p>下面是一步一步详细解释低秩分解的过程：</p>
<ol>
<li>原始矩阵：我们首先有一个原始矩阵，通常表示为一个m行n列的矩阵，记作M。</li>
<li>目标：我们的目标是将原始矩阵M分解为两个低秩矩阵的乘积，以便能够在降低维度的同时保留尽可能多的信息。</li>
<li>假设：假设我们将原始矩阵M分解为两个低秩矩阵U和V的乘积，其中U是一个m行k列的矩阵，V是一个k行n列的矩阵。k是一个较小的值，通常远小于m和n，表示我们希望降低的维度。</li>
<li>乘积：根据上述假设，我们得到分解后的矩阵表达式为M ≈ UV，其中≈表示近似等于。U和V的乘积近似地重构了原始矩阵M。</li>
<li>低秩性：由于U和V的维度较小，因此它们的秩也较小。这意味着它们包含的信息量较少，而丢弃的信息则通过U和V的乘积进行近似重构。</li>
<li>参数学习：我们的目标是找到合适的U和V，使得它们的乘积近似地重构原始矩阵M。这涉及到通过优化方法（如最小二乘法、梯度下降等）来学习U和V的参数。</li>
<li>选择低秩：选择合适的低秩值k是低秩分解的关键。较小的k可以帮助我们减少数据的维度，但可能会导致信息的损失。而较大的k可以更好地重构原始矩阵，但会增加存储和计算的成本。因此，需要在维度降低和信息保留之间进行权衡。</li>
</ol>
<p>总之，低秩分解是通过将原始矩阵分解为两个或多个低秩矩阵的乘积来降低数据维度并提取重要信息的技术。这种分解可以通过学习适当的参数来实现。</p>
</blockquote>
<h2 id="问题补充2：什么是矩阵的秩？"><a href="#问题补充2：什么是矩阵的秩？" class="headerlink" title="问题补充2：什么是矩阵的秩？"></a>问题补充2：什么是矩阵的秩？</h2><blockquote>
<p>矩阵的秩是指矩阵中线性无关的行或列的最大数量。简单来说，它表示矩阵中包含的独立信息的维度。</p>
<p>具体来说，对于一个m行n列的矩阵，它的秩可以通过以下方式确定：</p>
<ol>
<li>将矩阵转化为行简化阶梯形式（也称为行最简形式）或列简化阶梯形式。</li>
<li>计算行（或列）简化阶梯形式中非零行（或列）的数量。</li>
</ol>
<p>这个数量就是矩阵的秩。在行简化阶梯形式中，非零行的数量即为矩阵的行秩；在列简化阶梯形式中，非零列的数量即为矩阵的列秩。</p>
<p>矩阵的秩具有一些重要的性质：</p>
<ol>
<li>矩阵的行秩和列秩是相等的，它们都是矩阵的秩。</li>
<li>矩阵的秩不会超过它的行数或列数。即秩的最大值为min(m, n)。</li>
<li>当矩阵的秩等于min(m, n)时，该矩阵被称为满秩矩阵。</li>
</ol>
<p>矩阵的秩在线性代数和矩阵计算中具有广泛的应用。它可以用于判断矩阵的线性相关性、解决线性方程组、计算矩阵的逆、确定矩阵的特征值等。秩的概念也扩展到更一般的线性空间中，不仅仅适用于矩阵。</p>
</blockquote>
<h1 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h1><ul>
<li>a. 研究工作的意义：<ul>
<li>描述了本文所提出的方法可以在大型预训练语言模型自适应任务中显著减少计算和内存开销，同时达到或超过原有微调方法的模型质量水平</li>
</ul>
</li>
<li>b. 创新性、性能和工作量：<ul>
<li>描述了本文所提出的方法可以将下游任务的可训练参数数量降低数千倍，提高计算效率；而在模型质量上的表现达到或超过了原有微调方法</li>
</ul>
</li>
<li>c. 研究结论（列出重要点）：<ul>
<li>描述了低秩自适应方法可以使计算和内存效率显著提高，同时保证模型质量可达到或超过原有微调方法的水平</li>
<li>描述了超参优化需在不同任务中分别进行</li>
<li>描述了本文作者提供了代码和模型，方便其他学者和开发者使用</li>
</ul>
</li>
</ul>
<p>本文介绍了一种名为LoRA的低秩自适应方法，用于大型预训练语言模型的下游任务。相较于当前已有的方法，本文所提出的方法可以大幅减少可训练参数数量和GPU内存需求，同时保持达到或超过原有微调方法的模型质量水平，使计算和内存效率大幅提高，无需增加推理延迟，与其他方法兼容。本文的贡献在于在下游任务自适应问题中提出一种更加精细化的自适应方法，有望在大规模生产中得到更广泛的应用。回顾历史发展，早期下游任务自适应方法的推理效率较低，而当前的adapter层和prompt直接优化等现有方法仍存在效率和质量问题。未来工作中，需要在不同任务和不同模型上继续优化超参和算法，开发更有效的自适应策略，以面对不断增长的大规模自然语言处理任务。</p>
<h1 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110922.png" alt="image-20230610094344439"></p>
<ol>
<li>MNLI (Multi-Genre Natural Language Inference) - 包含来自不同来源和类型的句子对，用于推断它们之间的关系。</li>
<li>SST-2 (Stanford Sentiment Treebank) - 包含电影评论，用于情感分析任务。</li>
<li>MRPC (Microsoft Research Paraphrase Corpus) - 包含语义相似度匹配的句子对，用于文本匹配任务。</li>
<li>CoLA (Corpus of Linguistic Acceptability) - 包含含有不自然或不合乎语法的句子，用于句法分析的任务。</li>
<li>QNLI (Question NLI) - 包含问题和答案段落，用于推断问题和答案之间的关系，类似于MNLI数据集。</li>
<li>QQP (Quora Question Pairs) - 包含Quora上的问题对，目的是判断两个问题是否语义上相似。</li>
<li>RTE (Recognizing Textual Entailment) - 包含语言推理任务中的句子对，用于推断逻辑关系。</li>
<li>STS-B (Semantic Textual Similarity Benchmark) - 包含语义相似度匹配的句子对，用于评估不同模型在语义相似性任务上的表现。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110922.png" alt="image-20230610094538158"></p>
<blockquote>
<p>E2E NLG Challenge是一个用于测试和评估自然语言生成（NLG）系统的国际评估比赛。 E2E代表End-to-End，因为该竞赛关注的是完整的NLG系统，即从输入到输出的完整流程，而不仅仅是一个单一的任务。该比赛目标是推动NLG系统的发展和进步，包括对话系统、虚拟助手、智能个人助理等各种应用。竞赛的任务是将给定的语言输入转换为文本输出，例如在餐馆推荐中自动产生口头和书面指南。竞赛数据集基于各种领域来反映不同应用场景，包括餐馆、旅游、酒店等。评估基于多个自动生成描述的相关度和多样性的质量，以及诸如BLEU，NIST，METEOR，ROUGE - L和CIDEr等常见NLG评估指标。 E2E NLG Challenge自2017年以来已经成为自然语言生成研究中的重要活动，吸引了来自全球各地的研究人员和团队的广泛参与。</p>
<ul>
<li>Trainable Parameters：模型的可训练参数数量。</li>
<li>BLEU：一种常用的评估机器翻译质量的指标，使用n-gram重叠度来计算预测文本与参考文本之间的相似度。</li>
<li>NIST：另一种机器翻译质量的指标，比BLEU更加复杂，使用变换权重计算机生成文本与参考文本之间的相似度。</li>
<li>METEOR：综合评估指标，考虑机器翻译的准确率和流畅性，以及文本中出现的词汇、短语或实体名称等。</li>
<li>ROUGE-L：用于评估机器生成文本与参考文本之间的相似度的指标。它基于最长公共子序列（LCS）的概念，考虑到了长短不一的文本之间的差异。</li>
<li>CIDEr：评估多个自动生成描述的相对质量，并鼓励生成多样性和质量的指标。</li>
</ul>
</blockquote>
<h1 id="论文4-pile-of-law"><a href="#论文4-pile-of-law" class="headerlink" title="论文4 pile of law"></a>论文4 pile of law</h1><h1 id="基本信息：-1"><a href="#基本信息：-1" class="headerlink" title="基本信息："></a>基本信息：</h1><ul>
<li>题目：从法律和256GB开源法律数据集中学习负责任的数据过滤</li>
<li>作者：Peter Henderson，Mark S. Krass，Lucia Zheng，Neel Guha，Christopher D. Manning，Dan Jurafsky，Daniel E. Ho</li>
<li>隶属关系：斯坦福大学（Peter Henderson，Mark S. Krass，Lucia Zheng，Christopher D. Manning，Dan Jurafsky），斯坦福法学院（Daniel E. Ho）</li>
<li>关键词：大语言模型，负责任的数据过滤，预训练，法律，数据审查</li>
<li>网址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2207.00220%E3%80%81https://github.com/PileFoundation/pile">https://arxiv.org/abs/2207.00220、https://github.com/PileFoundation/pile</a></li>
</ul>
<p>注意：该论文尚未正式出版，但预印本可在arXiv上找到。GitHub 代码包含 256GB Pile of Law 数据集、用于训练模型的代码以及用于探索数据集的 Jupyter 笔记本。</p>
<h1 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h1><ul>
<li>a. 本文的研究背景：<ul>
<li>本文研究大型语言模型对于基于既定偏见或不当材料的预训练所带来的风险，同时探讨了现有的内容过滤方法存在的问题。本文提供了一个新的数据集，称为Pile of Law，包含256GB的开源英语法律和行政数据，可以供研究者预训练法律领域语言模型，提高人们获取司法方面的信息的准确性和便利性。同时，本文将法律领域对有毒内容和隐私信息处理的规范转化成可操作的经验教训，指导研究者们开发更加细致的过滤机制。另外，本文提供了一种新的基于模型的处理方法，通过学习Pile of Law中的隐式卫生规则，向研究模型过滤提供了一个新的方向。</li>
</ul>
</li>
<li>b. 过去的方法、问题和动机：<ul>
<li>过去的内容过滤方法存在不少问题，如偏见或者会给下游应用带来威胁。一些团队采取了内容过滤处理的流程，但是其他团队则没有采取。过滤有毒内容或隐私信息会涉及到一些复杂的权衡，在透明性和保护隐私之间寻求平衡的挑战是很大的。</li>
</ul>
</li>
<li>c. 本文提出的研究方法：<ul>
<li>本文提供了一个新的数据集Pile of Law，用于解决大型语言模型预训练过程中的过滤问题，并从法律和行政的规范出发，探讨内容过滤机制的实现方法。本文指出，从法律标准出发去制定过滤机制可能是提供更透明、更有责任感的过滤方法的一种途径。</li>
</ul>
</li>
<li>d. 本文方法所获得的结果和性能：<ul>
<li>Pile of Law数据集包含35个公共法律记录、行政规则和立法记录的数据源。通过研究数据过滤问题和相关挑战，本文提出了一种更全面更细致的过滤机制，使用上下文来确定是否包含可能涉及到隐私或有毒的信息。通过三个案例研究，本文展示了Pile of Law如何被用来识别特定场景下的隐私和有毒信号。研究结果表明，基于上下文的过滤机制或许能够帮助确保生成模型的准确性同时保护隐私权。</li>
</ul>
</li>
</ul>
<h1 id="背景：-1"><a href="#背景：-1" class="headerlink" title="背景："></a>背景：</h1><ul>
<li>a. 主题和特征：<ul>
<li>本文的主题是大型语言模型的训练和过滤问题。</li>
</ul>
</li>
<li>b. 历史发展：<ul>
<li>本文并没有涉及到历史发展部分的描述。</li>
</ul>
</li>
<li>c. 过去的方法：<ul>
<li>本文提到，过去的内容过滤方法存在一些问题，但并没有具体介绍。</li>
</ul>
</li>
<li>d. 过去研究的不足：<ul>
<li>本文并未进一步具体描述过去研究的不足在何处。</li>
</ul>
</li>
<li>e. 当前需要解决的问题：<ul>
<li>当前需要解决的问题围绕大型语言模型的内容过滤方法展开，同时需要考虑透明度和隐私保护之间的平衡问题。</li>
</ul>
</li>
</ul>
<h1 id="方法：-1"><a href="#方法：-1" class="headerlink" title="方法："></a>方法：</h1><ul>
<li>a. 研究的理论基础：<ul>
<li>本文研究基于法律和行政规范的内容过滤方法，具有较强的实践应用价值。</li>
</ul>
</li>
<li>b. 本文的技术路线：<ul>
<li>本文提出了一个新的数据集Pile of Law，同时对过去研究中存在的问题进行了探究，总结了法律标准对于内容过滤的经验教训。为此，本文首先描述了数据集的构建方法，同时对数据集的使用范围、特点和可能的应用做了具体介绍;其次，本文提出了一种基于上下文的过滤方法，借鉴了Pile of Law中的卫生规则;同时，本文阐述了如何使用Pile of Law数据集在有毒和隐私数据过滤中加入规则。总体而言，本文提供了一种完整的、跨学科的解决方案，可以帮助研究人员学习将法律准则应用于内容过滤，实现透明性和隐私保护之间的平衡。</li>
</ul>
</li>
</ul>
<h1 id="结论：-1"><a href="#结论：-1" class="headerlink" title="结论："></a>结论：</h1><ul>
<li>a. 研究的意义：<ul>
<li>本文的研究意义在于提供了一个新的数据集和基于法律标准的内容过滤机制，可以用于更准确、更可靠的大型语言模型的训练。</li>
</ul>
</li>
<li>b. 创新、性能和工作量：<ul>
<li>本文提出了大型语言模型预训练过程中从法律角度出发制定过滤机制的新思路，为解决文本数据中隐私和有毒的问题提供了新的方法和思路；同时，借助Pile of Law数据集，本文演示了如何开发基于上下文的过滤器，以平衡准确性和隐私保护；同时还特别强调了研究如何实现准确的、价值导向的有毒信息过滤，以防止内容过滤机制因意识形态偏见而不稳定的情况出现。</li>
</ul>
</li>
<li>c. 研究结论：<ul>
<li>本文的研究结论主要有三点：一是，Pile of Law数据集是一个适用于大型语言模型预训练的重要数据集，为语言模型的应用提供了更加准确、更有信任的文本数据;二是，从法律标准出发制定过滤机制的思路不仅可以解决大型语言模型训练过程中存在的问题，同时也有助于制定更可靠、更负责任的内容过滤规则;三是，通过借鉴Pile of Law中的卫生规则，采用基于上下文的过滤方式，可以在保护隐私权的前提下实现更精准的过滤机制。</li>
</ul>
</li>
</ul>
<h2 id="数据集细分"><a href="#数据集细分" class="headerlink" title="数据集细分"></a>数据集细分</h2><blockquote>
<ol>
<li>法律的案例意见和归档<ol>
<li>法院意见书、法院备审记录和法院文件。</li>
<li>美国最高法院案卷条目和法院文件。</li>
<li>美国退伍军人上诉委员会决定。</li>
<li>美国联邦贸易委员会咨询意见</li>
<li>美国国家劳工关系委员会的决定。</li>
<li>美国司法部移民审查移民和国籍决定执行办公室。</li>
<li>美国税务法院PLR语料库。</li>
<li>美国劳工部雇员补偿上诉委员会的决定。</li>
<li>欧洲人权法院的意见</li>
<li>加拿大法院意见。</li>
</ol>
</li>
<li>Legal Analyses法律分析<ol>
<li>美国法律的顾问办公室备忘录。</li>
<li>美国司法部监察长报告。</li>
</ol>
</li>
<li>Laws<ol>
<li>美国联邦法规美国州代码美国联邦证据规则联邦民事诉讼法。</li>
<li>美钞，美钞联邦登记册。 U.S. Bills, U.S. Federal Register.</li>
<li>美国创始人信。U.S. Founders Letters.</li>
<li>世界宪法。World Constitutions.</li>
<li>EUR-Lex.’</li>
</ol>
</li>
<li>合同&#x2F;业务文件<ol>
<li>信用卡协议，服务条款，埃德加Contracts，Atticus Contracts。</li>
</ol>
</li>
<li>Conversations 谈话？<ol>
<li>美国国会听证会。</li>
<li>欧洲议会议事平行语料库。</li>
<li>U.S. Supreme Court Oral Argument Transcripts.美国最高法院口头辩论记录。</li>
<li>联合国一般性辩论语料库。</li>
<li>Reddit r&#x2F;legaladvice &amp; r&#x2F;legaladviceofftopic. <ol>
<li>由于大多数法律的语言对于外行人来说通常难以理解，并且无法对简单的法律问题进行明确的回答，因此我们试图找到一个产生“简单英语”问答格式的数据集。我们选择了两个子标题r&#x2F;legaladvice和r&#x2F;legaladviceofftopic。由于存在对不正确的法律的建议进行编码的风险，我们对数据进行了严格的过滤。我们使用profanity-check过滤掉任何带有亵渎的帖子[130]。我们也只包括至少有一个答案的帖子，得分超过8净赞成票。然后，我们将数据重构为：标题：[文章标题]问题：[帖子内容]主题：[Post Flair]答案#[N]：[热门答案]…我们使用PushShift API来抓取每个subreddit的全部内容[9]。</li>
</ol>
</li>
</ol>
</li>
<li>Study Materials<ol>
<li>律师资格考试大纲。</li>
<li>开源案例簿。</li>
</ol>
</li>
</ol>
</blockquote>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202306171117865.png" alt="image-20230617111706742"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/6%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8RWKV%E6%B8%90%E8%BF%9BpromptLora-1/" data-id="cllkmcmew0004ksu7a1apb7gr" data-title="6月第一周RWKV渐进promptLora" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%8Cpromot%EF%BC%8Clora/" rel="tag">线性注意力，promot，lora</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-8月第一周自然语言蒸馏论文" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%92%B8%E9%A6%8F%E8%AE%BA%E6%96%87/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T07:31:26.235Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">title: 自然语言大模型蒸馏</span><br></pre></td></tr></table></figure>



<h1 id="8月第一周自然语言蒸馏论文"><a href="#8月第一周自然语言蒸馏论文" class="headerlink" title="8月第一周自然语言蒸馏论文"></a>8月第一周自然语言蒸馏论文</h1><h1 id="Title-Knowledge-Distillation-of-Large-Language-Models-大型语言模型的知识蒸馏"><a href="#Title-Knowledge-Distillation-of-Large-Language-Models-大型语言模型的知识蒸馏" class="headerlink" title="Title: Knowledge Distillation of Large Language Models (大型语言模型的知识蒸馏)"></a>Title: Knowledge Distillation of Large Language Models (大型语言模型的知识蒸馏)</h1><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Knowledge Distillation of Large Language Models (大型语言模型的知识蒸馏)</li>
<li>Authors: Yuxian Gu, Li Dong, Furu Wei, Minlie Huang</li>
<li>Affiliation: The CoAI Group, Tsinghua University (清华大学)</li>
<li>Keywords: Knowledge Distillation, Large Language Models, Generative Language Models, Reverse Kullback-Leibler Divergence, Optimization Approach</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.08543v1">Paper</a>, <a target="_blank" rel="noopener" href="https://aka.ms/MiniLLM">GitHub Code</a></li>
</ul>
<h1 id="论文简要-第一篇自然语言蒸馏-bert蒸馏？"><a href="#论文简要-第一篇自然语言蒸馏-bert蒸馏？" class="headerlink" title="论文简要 :第一篇自然语言蒸馏 bert蒸馏？"></a>论文简要 :第一篇自然语言蒸馏 bert蒸馏？</h1><ul>
<li>本文提出了一种名为MINILLM的方法，用于从生成型大型语言模型中蒸馏出较小的语言模型。通过使用逆向Kullback-Leibler散度作为目标函数，并引入有效的优化方法，MINILLM模型在指令跟随任务中表现出更高的生成质量、更低的暴露偏差、更好的校准性和更高的长文本生成性能。</li>
</ul>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 随着大型语言模型的快速发展，知识蒸馏（KD）成为减少计算资源需求的一种常用技术。然而，过去的知识蒸馏方法主要应用于白盒分类模型或训练小模型以模仿ChatGPT等黑盒模型API。如何有效地从白盒生成型语言模型中蒸馏知识仍然未被充分探索，而随着生成型语言模型的繁荣，这变得越来越重要。</li>
<li>过去方案: 过去的知识蒸馏方法主要应用于白盒分类模型或训练小模型以模仿黑盒模型API。白盒知识蒸馏方法主要研究了小型语言理解模型，而对于生成型语言模型的白盒知识蒸馏尚未被探索。</li>
<li>论文的Motivation: 随着生成型语言模型的兴起，白盒知识蒸馏对于研究社区和行业部门变得更有价值，因为学生模型可以从白盒教师模型中获得更好的信号，从而潜在地实现更高的性能。然而，白盒知识蒸馏方法主要研究了小型语言理解模型，而对于生成型语言模型的白盒知识蒸馏尚未被探索。因此，本文旨在研究生成型语言模型的白盒知识蒸馏，并提出了一种新的方法MINILLM，通过使用逆向Kullback-Leibler散度作为目标函数，实现了更好的生成质量和性能。</li>
</ul>
<h1 id="方法-创新点1：正则化整体样本"><a href="#方法-创新点1：正则化整体样本" class="headerlink" title="方法: 创新点1：正则化整体样本"></a>方法: 创新点1：正则化整体样本</h1><h2 id="可能的工作：在视觉蒸馏上使用正则化减少过拟合增加泛化能力，针对序列蒸馏改进"><a href="#可能的工作：在视觉蒸馏上使用正则化减少过拟合增加泛化能力，针对序列蒸馏改进" class="headerlink" title="可能的工作：在视觉蒸馏上使用正则化减少过拟合增加泛化能力，针对序列蒸馏改进"></a>可能的工作：在视觉蒸馏上使用正则化减少过拟合增加泛化能力，针对序列蒸馏改进</h2><p>可能的工作2：法学大模型对垂直领域大模型建设，提出了一个新的方法，包括微调，提示词，langchain，本文提到的蒸馏</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308061123535.png" alt="image-20230806112311078"></p>
<blockquote>
<ol>
<li>序列级知识蒸馏（KD）(左图): 这种方法强迫学生模型直接从教师模型生成的序列中学习。前向 KLD (Kullback-Leibler Divergence，库尔巴克-莱布勒散度)用作衡量教师输出和学生输出分布之间的不相似度。<strong>这种方法就像是死记硬背，学生模型本质上是试图复制教师的确切输出序列。</strong>提示 𝒙 输入到两个模型中，然后计算教师输出 𝒚 (从 𝑝 中采样)和学生预测输出之间的差异，以调整学生模型参数（𝜃）。然而，这种方法可能导致学生在语言生成任务中过度估计某些区域，特别是当学生模型缺乏捕获所有教师模型输出分布细微差异的能力时。</li>
<li>MINILLM (右图): 相比之下，这种方法允许学生模型通过教师模型的反馈来改善其生成能力。与单纯模仿教师输出不同，这种方法旨在帮助学生模型更好地泛化。这是通过最小化反向 KLD 来实现的，本质上是使学生模型的输出分布尽可能地接近教师的，但这样可以使学生模型也能从自己的错误中学习。<strong>提示 𝒙 输入到两个模型中，然后计算教师输出 (从 𝑞! 中采样)和学生预测输出之间的差异，为学生模型提供学习的机会。</strong></li>
</ol>
</blockquote>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308061238004.png" alt="image-20230806123825947"></p>
<blockquote>
<p>输入包括以下元素：</p>
<ul>
<li>条件生成数据集D，包含提示和真实响应</li>
<li>预训练语料库DPT，包含长文档纯文本</li>
<li>教师模型，其输出分布为p</li>
<li>初始学生模型，其输出分布为qθ0</li>
</ul>
<p>输出为一个学生模型，其输出分布为qθ。</p>
<p>这个算法的步骤如下：</p>
<ol>
<li>首先在数据集D上对学生模型进行微调，并选择具有最低验证损失的模型θ。</li>
<li>重复以下步骤直到收敛：<ul>
<li>从数据集D中抽取一批提示，并从其中收集响应以得到S &#x3D; {(xm, ym)}M m&#x3D;1。</li>
<li>从DPT中抽取一个小批量D’PT &#x3D; {dm}M m&#x3D;1。</li>
<li>计算主损失梯度(∇J)Main，这个值取决于学生模型qθ和教师模型pe之间的概率分布差异，详见等式5和等式6。</li>
<li>计算正则化损失梯度(∇J)Reg，这个值是在所有可能的输出yt中，学生模型qθ和教师模型p的分布差异的梯度，详见等式3。</li>
<li>计算预训练损失的梯度∇LPT，这个值是学生模型qθ在预训练语料库D’PT上的对数概率的梯度。</li>
<li>更新模型参数θ，根据计算出的梯度值，来调整模型的参数。</li>
</ul>
</li>
</ol>
<p>这个算法的核心思想是在模型参数更新过程中引入了一个正则化项，旨在减小学生模型和教师模型在相同输入下的输出分布差异，从而使学生模型更好地模仿教师模型。此外，为了保持模型的泛化能力，还计算了在预训练语料库上的损失，这有助于防止模型过度拟合教师模型的输出。</p>
</blockquote>
<p>步骤：</p>
<p>在这篇论文的实验设置中，研究者们首先在指令-响应数据集 D 上对一个大模型进行微调，将其作为教师模型 p。然后，他们比较了不同的知识蒸馏方法，以在教师模型的指导下对学生模型进行蒸馏，并通过评估蒸馏模型的指令执行性能来进行比较。</p>
<p>他们蒸馏了三种不同大小的模型：GPT-2，OPT和LLaMA，并分别使用GPT-2-1.5B，OPT-13B和LLaMA-13B作为各模型类型的教师模型。他们还使用GPT-J作为教师模型的结果。</p>
<p>他们从 databricks-dolly-15k 数据集中构建了训练数据，该数据集由 15K个人工编写的指令-响应对组成。他们随机分配了 14K个样本作为训练集 D，并留下了 500个样本用于验证和测试。对于 DPT，他们为 GPT-2 系列使用了 OpenWebText，而对于其他模型，他们使用了 RoBERTa 训练语料库。</p>
<p>实验中使用了两个指标来评估模型生成的响应：Rouge-L 分数和GPT-4反馈。</p>
<p>他们在主要实验中考虑了三个基线：</p>
<ol>
<li>SFT w&#x2F;o KD 直接在 D 上对学生模型进行微调，使用金色响应进行监督。</li>
<li>KD 使用教师分布作为每个标记步骤的监督，对 D 上的学生模型进行微调，也称为单词级别的 KD。</li>
<li>SeqKD 在教师生成的数据上对学生模型进行微调。</li>
</ol>
<p>总结实验步骤：</p>
<ol>
<li>在指令-响应数据集上微调大模型作为教师模型。</li>
<li>对多种大小的学生模型进行知识蒸馏，并评估其在教师模型指导下在数据集上的性能。</li>
<li>从 databricks-dolly-15k 数据集中构建训练数据。</li>
<li>使用 OpenWebText 和 RoBERTa 训练语料库进行预训练。</li>
<li>使用 Rouge-L 和 GPT-4 反馈对模型生成的响应进行评估。</li>
<li>比较与直接微调学生模型（SFT w&#x2F;o KD）、单词级别知识蒸馏（KD）以及在教师生成数据上微调学生模型（SeqKD）等基线方法的性能。</li>
<li>报告结果并进行分析。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308061243956.png" alt="image-20230806124343892"></p>
<ul>
<li>a. 理论背景:<ul>
<li>本文讨论了使用知识蒸馏（KD）来减少大型语言模型（LLM）的计算需求。先前的KD方法主要集中在白盒分类模型或训练小模型以模仿黑盒模型API，而作者提出了一种名为MINILLM的方法，从生成性更大的语言模型中提炼出较小的语言模型。作者认为标准的KD目标对于生成性LLM来说是次优的，并提出了最小化反向Kullback-Leibler散度（KLD）的方法。他们还引入了一种优化方法来学习这个目标。实验结果表明，MINILLM模型生成的响应更精确，整体质量更高，暴露偏差更低，校准性更好，并具有更高的长文本生成性能。该方法适用于不同模型家族，参数大小范围从120M到13B。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>MINILLM方法用于语言模型（LLM）中的知识蒸馏（KD）。与序列级KD不同，MINILLM专注于最小化反向Kullback-Leibler散度（KLD），并鼓励学生模型在其自身能力范围内生成教师模型偏好的样本。使用策略梯度定理进行优化，通过从学生模型中进行采样来计算目标函数的梯度。为了解决高方差和奖励欺骗等问题，提出了三种策略：单步正则化、教师混合采样和长度归一化。提供了MINILLM的训练算法，包括在数据集上微调学生模型、采样提示和响应以及计算梯度。还描述了实验设置和评估指标。</li>
</ul>
</li>
</ul>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><ul>
<li>a. 详细的实验设置:<ul>
<li>表1中的评估结果显示，标准的知识蒸馏（KD）方法，SFT w&#x2F;o KD、KD和SeqKD，成功地从教师模型中提取了知识，实现了更好的Rouge-L和GPT-4反馈分数。然而，MINILLM方法在几乎所有情况下都优于基线方法，表明其在提取具有高整体性能的小模型方面的有效性。MINILLM还生成了与真实值具有高重叠度的更精确的响应，并展示了良好的超出分布的泛化能力。MINILLM的改进在不同的模型大小和家族中保持一致，证明了其在大型语言模型（LLM）时代的可扩展性和泛化能力。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>实验结果显示，微调模型的ExAccErr在生成过程中不断增加，而MiniLLM具有较低的ExAccErr，并且错误在长文本生成中停止累积。在两个文本分类数据集SST2和BoolQ上的校准测试显示，使用KD和SeqKD训练的模型与教师模型相比校准性较差。另一方面，MINILLM缩小了学生和教师之间ECE分数的差距。模型的性能根据真实响应的长度而变化，在期望短响应的提示上，所有方法都获得了较低的分数。MINILLM通过保持生成响应中不同的4-gram比例来保持生成多样性，并且不会在测试集上导致语言建模损失的显著增加。消融研究表明，教师混合采样和长度归一化对于稳定训练至关重要，而单步正则化减少了训练过程的方差。在教师混合探索中，α值的选择会影响性能，一般而言，α &#x3D; 0.2是合适的。添加预训练损失有助于在规范NLP任务上保持能力，而不会对指令遵循任务的性能产生显著影响。</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%92%B8%E9%A6%8F%E8%AE%BA%E6%96%87/" data-id="cllkmcmf0000aksu7cww8epl4" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-7月第一周QLORA大模型有效剪枝LOMO全参数微调减少内存占用" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8QLORA%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%89%E6%95%88%E5%89%AA%E6%9E%9DLOMO%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83%E5%87%8F%E5%B0%91%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T07:31:26.231Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">title: 翟景治周报0626-0702</span><br><span class="line">date: 2023-07-05</span><br><span class="line">tags:</span><br><span class="line">    - Qlora</span><br><span class="line">    - 模型剪枝</span><br><span class="line">    - Lomo</span><br><span class="line">categories:</span><br><span class="line">    - 翟景治</span><br><span class="line">    - 微调</span><br><span class="line">    - 剪枝</span><br><span class="line">author:</span><br><span class="line">    - 翟景治</span><br></pre></td></tr></table></figure>

<h1 id="7月第一周QLORA-大模型有效剪枝-LOMO-全参数微调减少内存占用"><a href="#7月第一周QLORA-大模型有效剪枝-LOMO-全参数微调减少内存占用" class="headerlink" title="7月第一周QLORA&#x2F;大模型有效剪枝&#x2F;LOMO:全参数微调减少内存占用"></a>7月第一周QLORA&#x2F;大模型有效剪枝&#x2F;LOMO:全参数微调减少内存占用</h1><h1 id="论文1：QLORA：量化LLM的有效微调"><a href="#论文1：QLORA：量化LLM的有效微调" class="headerlink" title="论文1：QLORA：量化LLM的有效微调"></a>论文1：QLORA：量化LLM的有效微调</h1><h1 id="基本信息："><a href="#基本信息：" class="headerlink" title="基本信息："></a>基本信息：</h1><ul>
<li>题目：QLORA：量化LLM的有效微调 （QLORA：高效微调量化语言模型）</li>
<li>作者：蒂姆·德特默斯、阿蒂多罗·帕格诺尼、阿里·霍尔茨曼、卢克·泽特勒莫耶 （蒂姆·德特默斯、阿蒂多罗·帕格诺尼、阿里·霍尔茨曼、卢克·泽特尔莫耶）</li>
<li>隶属关系： 华盛顿大学</li>
<li>关键词：语言模型，微调，量化，低秩适配器（LLA），NormalFloat，双重量化，分页优化器，指令遵循，聊天机器人性能</li>
<li>网址：纸张：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14314v1%EF%BC%8CGitHub%EF%BC%9Ahttps://github.com/artidoro/qlora">https://arxiv.org/abs/2305.14314v1，GitHub：https://github.com/artidoro/qlora</a> 和 <a target="_blank" rel="noopener" href="https://github.com/TimDettmers/bitsandbytes">https://github.com/TimDettmers/bitsandbytes</a></li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><ul>
<li>a. 理论背景：<ul>
<li>本文讨论了QLORA和LoRA这两种参数高效的神经网络微调方法，以减少训练过程中的内存需求。LoRA通过分解投影来增强线性投影，而QLORA则使用4位NormalFloat量化和双量化来减少内存使用。QLORA还引入了分页优化器来防止梯度检查点过程中的内存溢出错误。</li>
</ul>
</li>
<li>b. 技术路线：<ul>
<li>QLORA使用LoRA方法在每个网络层引入适配器，以避免先前工作中出现的准确性折衷。QLORA使用4位NormalFloat（NF4）作为新的数据类型，该类型对于正态分布的权重是最优的。双量化用于量化量化常数，并使用分页优化器来管理内存峰值。</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927766.png" alt="image-20230625110446430"></p>
<blockquote>
<ol>
<li>LoRA（Low-Rank Adaptation）：这是一种微调方法，它通过低秩适应（low-rank adaptation）的技术来减少微调过程中的内存需求。具体而言，LoRA使用低秩矩阵分解的方法来近似和压缩原始的Transformer模型，以减少参数数量和内存占用。</li>
<li>QLORA（Quantized Low-Rank Adaptation）：这是QLORA对LoRA的改进。QLORA通过对Transformer模型进行量化，将模型参数的精度减少到4位，并使用分页式优化器（paged optimizers）来处理内存峰值，从而减少内存需求。</li>
</ol>
<p>关于QLORA相对于LoRA的改进，主要有两个方面：</p>
<ol>
<li>量化（Quantization）：QLORA使用较低的位精度（4位）来表示模型参数，从而降低了内存需求。通过减少每个参数的位数，可以大幅度减少存储参数所需的内存空间。</li>
<li>分页式优化器（Paged Optimizers）：QLORA使用分页式优化器来处理内存峰值问题。分页式优化器将模型参数分成多个页（pages），每次只加载一部分参数到内存中进行计算，以减少内存使用量。通过逐页加载和计算参数，可以处理大型模型在微调过程中可能产生的内存峰值问题。</li>
</ol>
</blockquote>
<h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><blockquote>
<p><strong>Block-wise k-bit Quantization</strong> 量化是将输入从保持更多信息的表示离散化到具有更少信息的表示的过程。这通常意味着将具有更多位的数据类型转换为更少位，例如从32位浮点数转换为8位整数。为了确保使用低位数据类型的整个范围，输入数据类型通常通过由输入元素的绝对最大值进行归一化而被重新缩放到目标数据类型范围中，输入元素通常被构造为张量。例如，将32位浮点（FP 32）张量量化为Int 8张量，范围为[-127，127]：<br>$$<br>\begin{aligned}<br>&amp; \<br>&amp;\mathbf{X}^{\mathrm{Int}}&amp; {}^{8}&#x3D;\mathrm{round}\left(\frac{127}{\mathrm{absmax}(\mathbf{X}^{\mathrm{FP32}})}\mathbf{X}^{\mathrm{FP32}}\right)&#x3D;\mathrm{round}(c^{\mathrm{FP32}}\cdot\mathbf{X}^{\mathrm{FP32}}),<br>\end{aligned}<br>$$</p>
<p>$$<br>\mathbf{dequant}(c^{\mathrm{FP32}},\mathbf{X}^{\mathrm{Int8}})&#x3D;{\frac{\mathbf{X}^{\mathrm{Int8}}}{c^{\mathrm{FP32}}}}&#x3D;\mathbf{X}^{\mathrm{FP32}}<br>$$</p>
<p><strong>Low-rank Adapters</strong>  低阶适配器（LoRA）微调是一种通过使用一小组可训练参数（通常称为适配器）来降低内存需求的方法，同时不更新保持固定的完整模型参数。随机梯度下降期间的梯度通过固定的预训练模型权重传递到适配器，该适配器被更新以优化损失函数。LoRA通过额外的因子分解投影来增强线性投影。</p>
<p>在参数高效微调（PEFT）方法中，LLM微调的大部分存储器占用来自激活梯度而不是来自学习的LoRA参数。对于在批量大小为1的FLAN v2上训练的7B LLaMA模型，LoRA权重相当于常用的原始模型权重的0.2%[28，37]，LoRA输入梯度的内存占用量为567 MB，而LoRA参数仅占用26 MB。使用梯度检查点[9]，输入梯度减少到每个序列平均18 MB，使其比所有LoRA权重组合更占用内存。相比之下，4位基本模型消耗5，048 MB内存。这突出了梯度检查点设置是重要的，但也突出了积极地减少LoRA参数的量仅产生较小的存储器益处。这意味着我们可以使用更多的适配器，而不会显著增加整体训练内存占用（详细的分类请参见附录G）。如后所述，这对于恢复完整的16位精度性能至关重要。</p>
</blockquote>
<h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>QLORA包含两个组件：4-bit NormalFloat量化和Double Quantization。其中：4-bit NormalFloat数据类型是基于Quantile Quantization技术开发的，通过估计输入张量的分位数来保证每个量化区间分配相等的值。Double Quantization是将额外的量化常数进行量化以减小内存开销的过程。</p>
<h3 id="4-bit-NormalFloat量化"><a href="#4-bit-NormalFloat量化" class="headerlink" title="4-bit NormalFloat量化"></a>4-bit NormalFloat量化</h3><p>NormalFloat（NF）数据类型建立在分位数量化[15]的基础上，这是一种信息理论上的最佳数据类型，可确保每个量化二进制具有从输入张量分配的相等数量的值。分位数量化通过经验累积分布函数估计输入张量的分位数来工作。分位数量化的主要限制是分位数估计的过程是昂贵的。因此，快速分位数近似算法，如SRAM分位数[15]，用于估计它们。由于这些分位数估计算法的近似性质，数据类型对于离群值具有较大的量化误差，离群值通常是最重要的值。当输入张量来自固定到量化常数的分布时，可以避免昂贵的分位数估计和近似误差。在这种情况下，输入张量具有相同的分位数，使得精确的分位数估计在计算上可行。</p>
<p>NF数据类型基于Quantile Quantization方法，它是一种信息论上最优的数据类型，可以确保将输入张量中的每个量化区间分配相等数量的值。Quantile quantization通过估计输入张量的分位数来实现，估计过程基于经验累积分布函数。Quantile quantization的主要限制在于分位数估计的过程比较昂贵。因此，需要使用快速的分位数近似算法（如SRAM quantiles）来进行估计。由于这些分位数估计算法是近似的，所以对于异常值（通常是最重要的值），数据类型存在较大的量化误差。当输入张量来自一个固定到量化常数的分布时，可以避免昂贵的分位数估计和近似误差。在这种情况下，输入张量具有相同的分位数，因此可以通过精确的分位数估计来降低计算成本。由于预训练的神经网络权重通常具有以零为中心的正态分布，标准差为σ（参见附录F），我们可以通过缩放σ来将所有权重转换为单个固定分布，使得该分布完全适配于我们的数据类型范围。对于我们的数据类型，我们将其范围设置为[-1, 1]。因此，数据类型的分位数和神经网络权重都需要被归一化到该范围内。对于零均值正态分布，标准差为σ且落在[-1, 1]范围内的情况，信息论上最优的数据类型计算步骤如下：（1）估计理论N(0, 1)分布的2k+1个分位数，以获得正态分布的k位量化数据类型；（2）将该数据类型的值归一化到[-1, 1]范围内；（3）通过绝对最大重缩放，将输入的权重张量归一化到[-1, 1]范围内。一旦权重范围和数据类型范围匹配，就可以像通常一样进行量化。步骤（3）等价于将权重张量的标准差重新缩放以匹配k位数据类型的标准差。<br>$$<br>q_i&#x3D;\frac{1}{2}\left(Q_X\left(\frac{i}{2^k+1}\right)+Q_X\left(\frac{i+1}{2^k+1}\right)\right),<br>$$</p>
<h3 id="Double-Quantization"><a href="#Double-Quantization" class="headerlink" title="Double Quantization"></a>Double Quantization</h3><p>这段文本介绍了一种称为Double Quantization (DQ) 的过程，用于对量化常数进行量化以实现更高的内存节省。精确的4位量化通常需要较小的块大小，但同时也会带来相当大的内存开销。例如，对于权重W，使用32位常数和块大小为64，平均每个参数需要增加32&#x2F;64 &#x3D; 0.5位的内存。Double Quantization可以帮助减少量化常数的内存占用。</p>
<p>具体而言，Double Quantization将第一次量化的量化常数cFP32作为第二次量化的输入。第二次量化得到了量化后的量化常数cFP8和第一层量化常数cFP32。我们在第二次量化中使用了8位浮点数（Floats），块大小为256，因为研究人员发现8位量化不会降低性能，这与Dettmers和Zettlemoyer的研究结果一致。</p>
<p>由于cFP32为正值，我们在量化之前从c2中减去均值，使值围绕零对称分布，以实现对称量化。平均而言，对于块大小为64，这种量化将每个参数的内存占用从32&#x2F;64 &#x3D; 0.5位减少到8&#x2F;64 + 32&#x2F;(64 · 256) &#x3D; 0.127位，每个参数减少了0.373位的内存占用。</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927895.png" alt="image-20230625141917832"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927319.png" alt="image-20230625142005506"></p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><ul>
<li>a. 详细的实验设置：<ul>
<li>使用QLORA，Guanaco模型系列在Vicuna基准测试中表现优于所有先前发布的模型，达到ChatGPT性能的99.3%，仅需要在单个GPU上进行24小时的微调。</li>
<li>QLORA用于微调超过1，000个模型，并对8个指令数据集、多个模型类型（LLaMA、T5）和模型规模进行了详细的指令跟随和聊天机器人性能分析。</li>
<li>结果表明，使用QLORA在小型高质量数据集上进行微调可以获得最先进的结果，即使使用比先前最先进的模型更小的模型。</li>
</ul>
</li>
<li>b. 详细的实验结果：<ul>
<li>QLORA在三种架构类型上的实验评估表明，其性能与完整模型微调相当，并优于16位适配器微调。</li>
<li>4位NormalFloat数据类型减少了量化误差，而双量化减少了内存开销，使QLORA成为一种有效且内存高效的微调方法。</li>
</ul>
</li>
</ul>
<h1 id="论文2：A-Simple-and-Effective-Pruning-Approach-for-Large-Language-Models"><a href="#论文2：A-Simple-and-Effective-Pruning-Approach-for-Large-Language-Models" class="headerlink" title="论文2：A Simple and Effective Pruning Approach for Large Language Models"></a>论文2：A Simple and Effective Pruning Approach for Large Language Models</h1><blockquote>
<ul>
<li>Title: A Simple and Effective Pruning Approach for Large Language Models (大型语言模型的简单有效修剪方法)</li>
<li>Authors: Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter</li>
<li>Affiliation: Carnegie Mellon University (卡内基梅隆大学)</li>
<li>Keywords: Large Language Models, Network pruning, Weight reconstruction, Sparsity, Magnitude pruning (大型语言模型，网络修剪，权重重构，稀疏性，幅度修剪)</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.11695v1">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/locuslab/wanda">GitHub</a></li>
<li>2023.06</li>
</ul>
</blockquote>
<h3 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h3><p>本研究提出了一种简单而有效的修剪方法，名为Wanda（基于权重和激活的修剪），用于大型语言模型（LLMs），通过在预训练的LLMs中修剪具有最小幅度的权重乘以相应的输入激活来诱导稀疏性。该方法无需重新训练或权重更新，并且修剪后的LLM可以直接使用。实验证明，Wanda在各种语言基准测试中明显优于幅度修剪，并与涉及大量权重更新的最新方法竞争激烈。</p>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 大型语言模型（LLMs）在复杂语言基准测试中表现出色，但由于其庞大的参数规模，需要大量的计算资源。为了降低LLMs的计算成本，研究人员一直在努力寻找有效的压缩方法。</li>
<li>过去方案: 以往的修剪方法要么需要重新训练，这对于规模庞大的LLMs来说很少可行，要么需要解决依赖于二阶信息的权重重构问题，这可能也会带来计算负担。这些方法在压缩LLMs方面的应用相对较少，这与在LLM之前的模型压缩领域的趋势相矛盾。</li>
<li>论文的Motivation: 鉴于以往方法的局限性，本研究旨在提出一种简单而有效的修剪方法，用于直接从预训练的LLMs中找到高效的稀疏网络，而无需重新训练或权重更新。通过观察LLMs中出现的大幅度特征，作者提出了一种基于权重和激活的修剪度量，并在每个输出上比较权重，以实现对LLMs的稀疏化。作者通过实验证明，该方法在性能上明显优于幅度修剪，并且在计算成本上要低于最新的LLMs修剪方法。</li>
<li>幅度修剪（Magnitude Pruning）：是一种剪枝技术，用于稀疏化神经网络中的权重参数。该技术通过保留模型中最重要的权重，将较小幅度的权重置为零或接近零的值，以达到减少模型复杂度和提高推理速度的目的。</li>
<li>Emergent Large Magnitude Features：（ELM特征）是指在训练神经网络时，由于网络的非线性特性和优化过程中的相互作用，某些具有较大幅度的特征或神经元在网络的上层逐渐变得重要。</li>
</ul>
<h1 id="方法-2"><a href="#方法-2" class="headerlink" title="方法:"></a>方法:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021429688.png" alt="image-20230702142920614"></p>
<blockquote>
<p>举个例子说明传统剪枝的额局限性：考虑一个具有两个输入和相应权重的神经元，其中y &#x3D; w1x1 + w2x2，且|w1| ≤ |w2|。传统的幅度剪枝方法总是会选择移除权重w1，但是在LLMs中，输入特征x1和x2的尺度可能存在显著差异。在某些情况下，可能有|x1| ≫ |x2|，导致|w1x1| ≫ |w2x2|。因此，在这种情况下，我们应该选择移除权重w2，因为相比于移除权重w1，它对神经元输出y的影响更小。</p>
<p>为了解决幅度剪枝方法的这个局限性，该段介绍了一种针对LLMs设计的剪枝度量。对于完全连接的线性层，考虑形状为(Cout, Cin)的权重W，输入激活为形状为(N × L, Cin)的X。对于每个权重，作者提出通过其幅度和相应输入特征范数的乘积来评估其重要性。具体地，当前权重Wij的得分由以下公式定义： Sij &#x3D; |Wij| · ∥Xj∥2 其中|·|表示绝对值运算符，∥Xj∥2计算了聚合在N × L个不同标记上的第j个特征的ℓ2范数，最终得分由这两个标量值的乘积计算而得。作者发现，ℓ2范数在衡量激活幅度时往往比其他范数函数（如ℓ1和ℓ∞范数）效果更好。</p>
<p>2范数（L2 norm），也称为欧几里德范数，用于计算向量的长度或矩阵的平方和的平方根。对于一个向量x&#x3D;(x1, x2, …, xn)，其ℓ2范数定义如下：</p>
<p>∥x∥2 &#x3D; sqrt(x1^2 + x2^2 + … + xn^2)</p>
</blockquote>
<ul>
<li>a. 理论背景:<ul>
<li>大型语言模型（LLMs）的重要性和由于其规模和计算要求而带来的挑战。</li>
<li>网络修剪方法的需求，以减小LLMs的规模。</li>
<li>现有方法要么需要重新训练，要么需要计算昂贵的权重重构。</li>
<li>引入一种名为Wanda的新型修剪方法，根据权重的大小乘以相应的输入激活来修剪权重。</li>
<li>Wanda不需要重新训练或权重更新，可以直接用于预训练的LLMs。</li>
<li>Wanda在性能和计算成本方面优于幅度修剪和其他最近的方法。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>Wanda是为修剪大型语言模型（LLMs）而设计的。</li>
<li>引入了一种修剪度量，根据输入特征激活的范数向量和权重的绝对值的点积来计算权重的重要性。</li>
<li>这种度量是稳健的，并且可以使用适量的校准样本进行估计。</li>
<li>Wanda建议使用更局部的修剪粒度级别，按输出逐个比较和删除权重。</li>
<li>修剪过程可以在LLM模型的单次前向传递中无缝实现，无需权重更新或进一步训练。</li>
<li>Wanda还可以扩展到结构化的N:M稀疏性，其中每M个连续权重中最多有N个非零。</li>
</ul>
</li>
</ul>
<h1 id="结果-1"><a href="#结果-1" class="headerlink" title="结果:"></a>结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021447017.png" alt="image-20230702144742941"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021459806.png" alt="image-20230702145942743"></p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>在LLaMA模型系列上评估了Wanda方法，该系列包括各种参数级别的Transformer语言模型。</li>
<li>将修剪方法应用于四个LLaMA模型：LLaMA-7B，LLaMA-13B，LLaMA-30B和LLaMA-65B。</li>
<li>通过在保留验证集上计算困惑度来评估修剪网络的性能。</li>
<li>使用128个从C4训练数据中采样的序列作为校准数据，用于估计输入统计信息。</li>
<li>Perplexity: 是一种用于评估语言模型性能的指标。它衡量模型对给定序列的预测能力和不确定性。较低的困惑度表示模型在给定序列上的预测更准确和更自信。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>将Wanda与两种先前的修剪方法（幅度修剪和SparseGPT）进行比较。</li>
<li>Wanda保持了幅度修剪的简单性，但在发现预训练LLMs中的稀疏网络方面非常有效。</li>
<li>Wanda比SparseGPT更快，因为它不涉及逆计算。</li>
<li>SparseGPT（稀疏GPT）是一种基于稀疏注意力机制的语言模型，它是对GPT（Generative Pre-trained Transformer）模型的改进和扩展。SparseGPT使用稀疏的注意力权重矩阵，其中只有少数非零元素，而其他元素为零。这种稀疏性允许SparseGPT在保持相对较低的计算成本的同时，仍能保持较好的模型性能。</li>
<li>提供了一个比较表，显示每种方法使用的时间复杂度和修剪度量。</li>
</ul>
</li>
</ul>
<h1 id="论文3：LOMO-Full-Parameter-Fine-tuning-for-Large-Language-Models-with-Limited-Resources"><a href="#论文3：LOMO-Full-Parameter-Fine-tuning-for-Large-Language-Models-with-Limited-Resources" class="headerlink" title="论文3：LOMO:Full Parameter Fine-tuning for Large Language Models with Limited Resources"></a>论文3：LOMO:Full Parameter Fine-tuning for Large Language Models with Limited Resources</h1><p><strong>论文：LOMO：利用有限的资源对大型语言模型进行全参数微调</strong></p>
<blockquote>
<p>ArXiv: <a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/2306.09782.pdf">https://arxiv.org/pdf/2306.09782.pdf</a><br>机构：复旦大学<br>时间：2023.6.16<br>Github：<a href="https://link.zhihu.com/?target=https://github.com/OpenLMLab/LOMO">https://github.com/OpenLMLab/LOMO</a></p>
</blockquote>
<h2 id="摘要：-1"><a href="#摘要：-1" class="headerlink" title="摘要："></a>摘要：</h2><p>大型语言模型（LLMs）已经彻底改变了自然语言处理（NLP），但是训练LLMs需要大量的GPU资源。降低LLMs训练的门槛将鼓励更多研究人员参与，从而使学术界和社会受益。虽然现有的方法着重于参数高效微调，即微调或添加少量参数，但很少有人解决了有限资源下调整LLMs的全部参数的挑战。在本文中，我们提出了一种新的优化器LOw-Memory Optimization（LOMO），它将梯度计算和参数更新融合在一步中以减少内存使用。通过将LOMO与现有的内存节省技术集成，我们将内存使用降低到10.8％，与标准方法（DeepSpeed解决方案）相比。因此，我们的方法使单台机器上的65B模型的全参数微调成为可能，该机器配有8×RTX 3090，每个显存为24GB。</p>
<h2 id="IDEA"><a href="#IDEA" class="headerlink" title="IDEA"></a>IDEA</h2><blockquote>
<p>分析了SGD可以finetune LLM的原因，不用Adam改用SGD，在SGD的基础上提出了一个LOw-Memory Optimization（LOMO）的优化器，来全参数finetune LLM，并在下游任务上获得了比lora等更好的效果。（可能因为资源问题没对比Adam的全参数finetune的结果，这个还不够有说服力）8张3090能微调65B的模型了</p>
</blockquote>
<h2 id="背景知识："><a href="#背景知识：" class="headerlink" title="背景知识："></a>背景知识：</h2><blockquote>
<h3 id="1-SGD代表随机梯度下降（Stochastic-Gradient-Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。"><a href="#1-SGD代表随机梯度下降（Stochastic-Gradient-Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。" class="headerlink" title="1.SGD代表随机梯度下降（Stochastic Gradient Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。"></a>1.SGD代表随机梯度下降（Stochastic Gradient Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。</h3><p>下面是SGD的详细解释：</p>
<ol>
<li><p>梯度下降：梯度下降是一种优化算法，用于找到函数的局部最小值或全局最小值。在机器学习中，我们使用梯度下降来最小化模型的损失函数。梯度表示函数在给定点的变化率方向，通过朝着梯度的反方向调整模型参数，我们可以逐步降低损失函数的值，从而找到最优参数。</p>
</li>
<li><p>随机梯度下降：随机梯度下降是梯度下降的一种变种，其目标是加速收敛过程。与传统的梯度下降一次性使用整个训练数据来计算损失函数的梯度相比，随机梯度下降每次迭代仅使用一个训练样本来估计梯度。这样做的好处是计算成本较低，特别是在大规模数据集上，同时它也使得算法更具随机性，有助于避免陷入局部最优解。</p>
</li>
<li><p>迭代过程：SGD的迭代过程如下：</p>
<ul>
<li>初始化模型参数，例如权重和偏置。</li>
<li>将训练数据打乱顺序，以随机顺序遍历样本。</li>
<li>对于每个训练样本，计算损失函数关于模型参数的梯度。</li>
<li>使用梯度来更新模型参数，通常是通过将梯度乘以一个学习率来确定更新步长。</li>
<li>重复以上步骤，直到达到指定的迭代次数或满足停止准则（例如损失函数的收敛）。</li>
</ul>
</li>
<li><p>学习率：学习率是SGD算法中的一个重要参数，用于控制参数更新的步长。较大的学习率可能导致参数更新过大，错过最优解；而较小的学习率可能导致收敛速度缓慢。因此，选择合适的学习率是使用SGD的关键之一。在实践中，可以使用学习率调度策略来逐步减小学习率，以获得更好的收敛性能。</p>
</li>
<li><p>批量大小：除了使用单个样本计算梯度外，SGD还支持使用多个样本计算梯</p>
</li>
</ol>
<p>度。将一批样本一起计算梯度称为小批量随机梯度下降（mini-batch SGD）。小批量SGD通常比单个样本的SGD更稳定，因为它可以减少参数更新的方差，并利用并行计算的优势。批量大小是一个需要调整的超参数，通常根据可用的计算资源和训练数据的规模进行选择。</p>
<p>总结来说，SGD是一种基于梯度下降的优化算法，通过每次迭代使用一个样本（或一小批样本）的梯度来更新模型参数。它是一种高效的训练算法，特别适用于大规模数据集和复杂模型。然而，SGD也有一些缺点，例如可能陷入局部最优解和对学习率的敏感性。因此，在使用SGD时需要进行适当的参数调整和正则化技术的应用。</p>
</blockquote>
<blockquote>
<p>2.Adam算法</p>
<p>Adam是一种优化算法，全称为自适应矩估计优化算法（Adaptive Moment Estimation）。它是一种基于梯度的优化算法，结合了梯度下降和动量优化的思想。Adam算法具有较快的收敛速度和良好的性能，在深度学习领域广泛使用。</p>
<p>下面是Adam算法的详细解释：</p>
<ol>
<li><p>梯度下降：梯度下降是一种基本的优化算法，用于最小化损失函数。它通过计算损失函数关于模型参数的梯度，并朝着梯度的反方向更新参数，以逐步降低损失函数的值。</p>
</li>
<li><p>动量优化：动量优化是一种改进的梯度下降算法，引入了动量（momentum）的概念。动量表示模型更新的惯性，使得参数更新在当前梯度方向的基础上，还考虑了历史梯度的影响。这有助于加速收敛，尤其在存在平坦区域或峡谷的情况下。</p>
</li>
<li><p>自适应学习率：Adam算法引入了自适应学习率的概念，通过自动调整学习率的大小来适应每个参数的变化情况。它基于梯度的一阶矩估计（mean）和二阶矩估计（variance）来自适应地调整学习率。</p>
</li>
<li><p>Adam算法的更新步骤：</p>
<ul>
<li>初始化模型参数和累计变量（一阶和二阶矩估计的初始值）。</li>
<li>在每次迭代中，计算当前的梯度。</li>
<li>更新一阶矩估计和二阶矩估计。</li>
<li>根据一阶和二阶矩估计计算参数更新的方向和大小。</li>
<li>更新模型参数。</li>
<li>重复以上步骤，直到达到指定的迭代次数或满足停止准则。</li>
</ul>
</li>
<li><p>Adam算法的优点：</p>
<ul>
<li>自适应学习率：Adam算法可以自适应地调整每个参数的学习率，根据梯度的一阶和二阶矩估计进行缩放。这有助于在训练过程中平衡收敛速度和参数稳定性。</li>
<li>适用于大规模数据和高维参数：Adam算法对于大规模数据和高维参数的训练具有较好的效果，因为它可以有效地利用梯度信息和自适应学习率。</li>
<li>低内存要求：相对于其他优化算法（如基于Hessian矩阵的方法），Adam算法的内存要求较低，因为它仅需要存储一阶和二阶矩估计。</li>
</ul>
<p>需要注意的是，Adam算法也有一些超参数需要调整，如学习率、动量系数和指数衰减率等，这些超参数的选择可能会对算法的性能产生影响。因此，在实践中，常常需要进行超参数调优来获得最佳的性能。</p>
</li>
</ol>
</blockquote>
<h2 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020953125.png" alt="image-20230702095337036"></p>
<p>为什么会用SGD：以前的研究经常讨论SGD的三个挑战：1）大曲率损失表面，2）局部最优解，以及3）鞍点（Ruder，2016; Sun等，2020）。现代优化器已经显示出处理1）问题的有效性，并且在某些情况下可以减轻2）和3）问题。然而，当我们将范围限定为微调LLM时，这三个挑战可能会有所不同。</p>
<p>更平滑的损失面（<strong>Smoother loss surface</strong>）</p>
<p>一个重要的假设是LLM的参数空间非常平滑，对参数进行小的扰动不会显着改变损失。有实证结果和理论分析支持这一假设（Hao等，2019）。如果我们相信更大的模型具有更平滑的损失曲面，那么我们可以得出结论：由于LLM的损失表面不应具有很大的曲率，因此1）的问题不是问题。请注意，仅当我们教LLM基于自然语言的任务（或者像以前一样用代码预训练）时，才有效。与预训练任务无关的合成损失函数确实会面临大曲率问题。</p>
<p>局部最优解已经足够（<strong>Local optimum is good enough</strong>）</p>
<p>微调的目标是将LLM调整到新任务和领域中，并且不会显着改变模型本身。因此，局部最优解通常是足够好的解决方案，并且有限的训练数据（与预训练语料库相比）使其难以将模型推向遥远的全局最优解。</p>
<p>算法： LOMO中的融合更新</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021044049.png" alt="image-20230702104423997"></p>
<blockquote>
<ol>
<li><p>输入：模型f(·)具有L层和p个参数，参数θ ∈ Rp，学习率α，最大步数T，训练数据集D，损失函数L。</p>
</li>
<li><p>对于每个步骤t &#x3D; 1, . . . , T 进行以下操作：</p>
<p>2.1 从训练数据集D中随机抽取一个批次B &#x3D; (x, y)，其中x是输入数据，y是对应的目标标签。</p>
<p>2.2 通过模型f(·)进行前向传播，计算预测值ŷ： ŷ ← f(x, θ) 这一步骤用于获取模型的输出。</p>
<p>2.3 计算损失函数ℓ： ℓ ← L(y, ŷ) 这一步骤用于计算模型预测值ŷ与真实标签y之间的差异。</p>
<p>2.4 对于每一层l &#x3D; L, . . . , 1 进行以下操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1 反向传播（Backward propagation）：</span><br><span class="line">      获取当前层的参数集合θl。</span><br><span class="line">2 计算当前层的梯度gl：</span><br><span class="line">      gl ← ∂ℓ/∂θl</span><br><span class="line">      这一步骤通过计算损失函数ℓ关于当前层参数θl的偏导数来获取梯度。</span><br><span class="line"></span><br><span class="line">3 参数更新：</span><br><span class="line">      θl ← θl - α * gl</span><br><span class="line">      使用梯度下降的方式更新当前层的参数θl。学习率α确定了参数更新的步长。</span><br><span class="line">4 清除梯度：</span><br><span class="line">      将当前层的梯度gl设置为None，以清除梯度信息。</span><br></pre></td></tr></table></figure>

<p>2.5 结束当前步骤的循环。</p>
</li>
<li><p>结束算法。</p>
</li>
</ol>
</blockquote>
<blockquote>
<p>这个算法是LOMO中的融合更新算法，它使用随机梯度下降（SGD）的方式对模型的参数进行更新。它在每个步骤中，通过随机抽取一个批次样本进行前向传播和损失计算，然后进行反向传播来计算梯度，并使用梯度下降的方式更新模型的参数。整个过程重复T个步骤，直到达到最大步数T。</p>
</blockquote>
<h2 id="实验："><a href="#实验：" class="headerlink" title="实验："></a>实验：</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021046738.png" alt="image-20230702104644688"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021048215.png" alt="image-20230702104809171"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8QLORA%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%89%E6%95%88%E5%89%AA%E6%9E%9DLOMO%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83%E5%87%8F%E5%B0%91%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8/" data-id="cllkmcmev0003ksu711yk9go2" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-7月第四周依赖图论文通用剪枝" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E5%9B%9B%E5%91%A8%E4%BE%9D%E8%B5%96%E5%9B%BE%E8%AE%BA%E6%96%87%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T07:31:26.228Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">title: 7月第四周依赖图论文通用剪枝</span><br><span class="line">date: 202308</span><br><span class="line">tags: 依赖图通用剪枝</span><br><span class="line">categories: 剪枝</span><br><span class="line">1234</span><br></pre></td></tr></table></figure>

<h1 id="7月第四周依赖图论文通用剪枝"><a href="#7月第四周依赖图论文通用剪枝" class="headerlink" title="7月第四周依赖图论文通用剪枝"></a>7月第四周依赖图论文通用剪枝</h1><h1 id="论文1-DepGraph-Towards-Any-Structural-Pruning"><a href="#论文1-DepGraph-Towards-Any-Structural-Pruning" class="headerlink" title="论文1 DepGraph: Towards Any Structural Pruning"></a>论文1 DepGraph: Towards Any Structural Pruning</h1><h1 id="DepGraph：面向任何结构修剪"><a href="#DepGraph：面向任何结构修剪" class="headerlink" title="DepGraph：面向任何结构修剪"></a>DepGraph：面向任何结构修剪</h1><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: DepGraph: Towards Any Structural Pruning (DepGraph: 通向任意结构剪枝)</li>
<li>Authors: Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, Xinchao Wang</li>
<li>Affiliation: National University of Singapore, Zhejiang University, Huawei Technologies Ltd.</li>
<li>Keywords: structural pruning, neural networks, dependency graph, automatic method, generalizability</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.12900v2">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/VainF/Torch-Pruning">GitHub</a></li>
</ul>
<h1 id="论文简要"><a href="#论文简要" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li><p>提出了一种非深度图算法DepGraph，实现了架构通用的结构化剪枝，适用于CNNs, Transformers, RNNs, GNNs等网络。DepGraph能够自动地分析复杂的结构耦合，从而正确地移除参数实现网络加速。基于DepGraph算法，我们开发了PyTorch结构化剪枝框架 <a href="https://link.zhihu.com/?target=https://github.com/VainF/Torch-Pruning">Torch-Pruning</a>。不同于依赖Masking实现的“模拟剪枝”，该框架能够实际地移除参数和通道，降低模型推理成本。在DepGraph的帮助下，研究者和工程师无需再与复杂的网络结构斗智斗勇，可以轻松完成复杂模型的一键剪枝。</p>
<p>论文标题：DepGraph: Towards Any Structural Pruning<br>论文链接：<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2301.12900">https://arxiv.org/abs/2301.12900</a><br>项目地址：<a href="https://link.zhihu.com/?target=https://github.com/VainF/Torch-Pruning">https://github.com/VainF/Torch-</a></p>
</li>
</ul>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 近年来，边缘计算应用的出现使得深度神经网络压缩变得必要。剪枝作为一种网络压缩方法已被证明是高效且实用的。然而，现有的剪枝方法往往依赖于手动设计的分组方案，限制了其在新的网络架构上的通用性。</li>
<li>过去方案: 传统的剪枝方法可以大致分为结构剪枝和非结构剪枝两种。结构剪枝通过物理上移除参数来改变神经网络的结构，而非结构剪枝则通过将特定权重置零来实现。然而，现有的结构剪枝方法往往需要针对不同网络进行个案分析，且无法直接推广到其他网络架构，限制了其在实际应用中的使用。</li>
<li>论文的Motivation: 鉴于现有结构剪枝方法的局限性，本文旨在提出一种通用的剪枝方案，能够适用于任意网络架构。为了解决结构剪枝中的依赖性问题，作者引入了Dependency Graph (DepGraph)方法，通过显式建模不同层之间的依赖关系，实现了全自动的剪枝过程。通过在多个网络架构上的实验证明了DepGraph的有效性和通用性。</li>
</ul>
<h1 id="论文摘要"><a href="#论文摘要" class="headerlink" title="论文摘要"></a>论文摘要</h1><blockquote>
<p>结构剪枝能够通过从神经网络中移除结构性分组的参数来加速模型。然而，不同模型中的参数分组模式各不相同，这使得依赖于手动设计的分组方案的特定架构的剪枝器无法泛化到新的架构。在这项工作中，我们研究了一个极具挑战性但鲜少被探索的任务，即<strong>任意结构剪枝</strong>，以解决诸如CNNs、RNNs、GNNs和Transformers等任意架构的通用结构剪枝。实现这个目标的最大难题在于<strong>结构耦合</strong>，它不仅强制不同的层同时被剪枝，还期望所有被移除的参数在重要性上保持一致，从而避免剪枝后出现结构问题和显著的性能下降。为解决这个问题，我们提出了一种通用且全自动的方法，依赖图（Dependency Graph，DepGraph），用以明确地建模层与层之间的依赖关系，以及为剪枝全面地组合耦合参数。在这项工作中，我们对多种架构和任务进行了广泛的评估，包括用于图像的ResNe(X)t、DenseNet、MobileNet和Vision transformer，用于图的GAT，用于3D点云的DGCNN，以及用于语言的LSTM，并表明，即使采用简单的基于规范的判别准则，所提出的方法也能够持续产生令人满意的性能。</p>
</blockquote>
<blockquote>
<ol>
<li>什么是结构耦合（structural coupling），并且它如何影响神经网络的剪枝？</li>
<li>结构耦合是指<strong>神经网络中不同层之间的关联性</strong>，这种关联性强制同时剪枝不同的层，同时也期望所有被剪枝的参数都一致地不重要，这样可以避免剪枝后出现结构问题和明显的性能下降。具体来说，如果一个参数被剪枝，那么它的耦合参数也应该被剪枝，以保持网络的结构完整性。</li>
</ol>
</blockquote>
<h2 id="图片1：不同神经网络模型结构耦合和剪枝策略的对比"><a href="#图片1：不同神经网络模型结构耦合和剪枝策略的对比" class="headerlink" title="图片1：不同神经网络模型结构耦合和剪枝策略的对比"></a>图片1：不同神经网络模型结构耦合和剪枝策略的对比</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221403214.png" alt="image-20230722140353693"></p>
<blockquote>
<p>a: 在”a”部分（CNNs）中，例如，为了剪枝Conv2，块内的所有其他层（如Conv1, BN1, BN2）也必须被剪枝。这强调了在剪枝过程中，如果要剪枝某个层，则必须同时剪枝块内的所有其他层。</p>
<p>b: b部分中，如果要对Transform的MLP模块进行剪枝，也就必须要对Norm和多头注意力进行剪枝，这种情况的出现是由于神经网络中的结构耦合。在神经网络中，不同层的参数是彼此依赖的，这种依赖关系迫使一旦一个层（如MLP）被剪枝，那么它的耦合层（如Norm和多头注意力机制）也必须被剪枝。各层之间的信息流动是相互依赖的。如果你剪掉了一个层，那么它的输出就不再能够作为下一层的输入。因此，下一层也就失去了它的输入源，相当于也被”剪掉”了。而这些层可能又是其他层的输入源，因此，剪枝一个层可能会导致多个层被剪掉。这就是为什么在剪枝MLP的同时，必须剪掉Norm和多头注意力机制。</p>
<p>c 一样的道理 RNNS</p>
<p>d GNNS  进行剪枝时，不仅要考虑节点或边的重要性，还需要考虑剪枝操作对整个网络结构的影响。例如，如果剪枝的节点或边是信息传播路径的一部分，那么剪枝可能会阻断信息的流动，从而影响网络的性能。</p>
</blockquote>
<h1 id="相关工作部分"><a href="#相关工作部分" class="headerlink" title="相关工作部分"></a>相关工作部分</h1><blockquote>
<ol>
<li><strong>结构剪枝和非结构剪枝</strong>：结构剪枝的目标是物理上移除一组参数，从而减少神经网络的大小。相比之下，非结构剪枝涉及将特定的权重置零，但不改变网络结构。非结构剪枝在实践中易于实施，并且本质上能够适应各种网络。然而，它通常需要专门的AI加速器或软件进行模型加速。相反，结构剪枝通过从网络中物理上移除参数来改善推理开销，从而在更广泛的应用领域中找到应用。</li>
<li><strong>剪枝分组参数</strong>：在复杂的网络结构中，参数组之间可能存在依赖性，需要同时进行剪枝。例如，当剪枝两个连续的卷积层时，从第一层移除一个过滤器需要剪枝下一层的关联核。虽然手动分析参数依赖性是可行的，但当应用于复杂的网络时，这个过程可能非常耗费人力。此外，这样的手动方案本质上不能转移到新的架构，这严重限制了剪枝的应用。最近，一些初步的工作已经被提出来解密层与层之间的复杂关系。不幸的是，现有的技术仍然依赖于经验规则或预定义的架构模式，使它们对所有结构剪枝应用的通用性不足。</li>
<li>作者指出了他们的工作目标：提出一种通用的方法来解决这个挑战，证明有效地处理参数依赖性可以使结构剪枝在各种网络中通用，从而在多个任务上得到满意的性能。</li>
</ol>
<p>在卷积神经网络（CNNs）中，一层的输出通常作为下一层的输入，这种依赖关系也适用于卷积层。如果你移除了Conv2的一个过滤器，那么Conv2的输入（也就是Conv1的输出）就会少了一个特征映射。为了保持网络的结构完整性，你需要同时剪掉Conv1中对应的输出通道。</p>
<p><strong>具体来说，Conv1的每个过滤器都会输出一个特征映射，这些特征映射被堆叠在一起，形成Conv2的输入。每个Conv2的过滤器都会在这些输入特征映射上进行卷积操作。如果你移除了Conv2的一个过滤器，那么那个过滤器对应的输入特征映射就不再需要了。这个输入特征映射是Conv1的一个输出特征映射，所以对应的Conv1的过滤器也就可以被剪掉了。</strong></p>
<p>这就是为什么在剪枝Conv2的时候，也需要剪掉Conv1。这是由于Conv1和Conv2之间的结构耦合，也就是他们的输出和输入之间的依赖关系。</p>
</blockquote>
<h1 id="方法-21、29"><a href="#方法-21、29" class="headerlink" title="方法:21、29"></a>方法:21、29</h1><ol>
<li><h2 id="Dependency-in-Neural-Networks神经网络中的依赖"><a href="#Dependency-in-Neural-Networks神经网络中的依赖" class="headerlink" title="Dependency in Neural Networks神经网络中的依赖"></a>Dependency in Neural Networks神经网络中的依赖</h2></li>
<li><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221459077.png" alt="image-20230722145939026"></p>
</li>
<li><p>在不同结构中具有相互依赖性的分组参数。必须同时修剪所有高亮显示的参数。</p>
</li>
<li><blockquote>
<p>在许多神经网络优化任务中，结构修剪是一种常用的技术，通过移除不重要的神经元来使网络更加高效。在这里，作者使用了一个由三个连续层组成的线性神经网络作为例子，每一层都由二维权重矩阵（wl, wl+1和wl+2）参数化。</p>
<p>当通过修剪神经元来瘦身网络时，参数之间的依赖性就显现出来了。在这里，wl和wl+1的依赖性是这样的：如果你想修剪连接wl和wl+1的第k个神经元，那么你就必须同时移除wl[k, :] 和 wl+1[:, k]。这就是所谓的“依赖性”，因为这两个参数（即wl和wl+1）必须一起修剪，不能单独修剪。</p>
<p>在现有的文献中，研究人员通常使用手动设计的、模型特定的方案来处理层依赖性并对深度神经网络进行结构修剪。然而，这个过程中存在着各种类型的依赖性，如作者所示。手动分析所有的依赖性并不可行，尤其是当简单的依赖性可能会嵌套或组合形成更复杂的模式时。</p>
</blockquote>
</li>
</ol>
<h2 id="Dependency-Graph依赖图"><a href="#Dependency-Graph依赖图" class="headerlink" title="Dependency Graph依赖图"></a>Dependency Graph依赖图</h2><p>1.分组：为了执行结构修剪（结构修剪是一种优化神经网络的方法，通过删除不重要的神经元或连接来减少网络的复杂性），我们首先需要将网络层按照它们的依赖关系进行分组。在这里，他们提出了一个分组矩阵G，它是一个L×L的矩阵，其中L是网络层的数量。在这个矩阵中，如果第i层和第j层之间存在依赖关系，那么Gij就是1。</p>
<p>然而，现代的深度网络可能由成千上万个具有复杂连接的层组成，这使得从神经网络中获取分组模式变得非常困难。也就是说，得到的分组矩阵G可能会非常大而复杂，其中Gij的值不仅取决于第i层和第j层，还会受到它们之间所有中间层的影响。在大多数情况下，这种非局部的、隐式的关系不能用简单的规则来处理。</p>
<p>因此，作者提出了一个新的解决方案：<strong>他们不直接估计分组矩阵G，而是提出了一个更易于估计的依赖模型方法，即依赖图。依赖图是从分组矩阵G中有效地提取出来的，它能更好地处理和展示层之间复杂的依赖关系。</strong></p>
<p> 1 分组矩阵的定义：<br>$$<br>G \in {0,1}^{L \times L}<br>$$</p>
<ol start="2">
<li><p>如果第i层和第j层之间存在依赖关系，那么：<br>$$<br>G_{ij} &#x3D; 1<br>$$</p>
</li>
<li><p>如果不存在依赖关系，则：</p>
</li>
</ol>
<p>$$<br>G_{ij} &#x3D; 0<br>$$</p>
<ol start="4">
<li>对角线元素的设置，即自依赖性：</li>
</ol>
<p>$$<br>Diag(G) &#x3D; \mathbf{1}_{1 \times L}<br>$$</p>
<p>其中$\mathbf{1}_{1 \times L}$是长度为L的全1向量。</p>
<ol start="5">
<li>表示与第i层有依赖关系的所有层的集合：</li>
</ol>
<p>$$<br>g(i) &#x3D; {j | G_{ij} &#x3D; 1}<br>$$</p>
<p>2 Dependency Graph ：介绍了一种新的图形数据结构——依赖图Dependency Graph，这是一种有效的分组矩阵G的压缩方式。依赖图D记录了具有直接连接的相邻层之间的本地依赖关系，与分组矩阵G的区别在于，它仅记录了直接连接的相邻层之间的依赖关系。依赖图D可以被视为分组矩阵G的传递性简化，即<strong>包含与G相同的顶点，但尽可能少的边</strong>。</p>
<p>在介绍Dependency Graph之前，首先用一个例子解释了依赖关系的冗余。考虑一个依赖组g &#x3D; {w1, w2, w3}，它有依赖关系w1 ⇔ w2，w2 ⇔ w3，和w1 ⇔ w3。经过仔细观察，可以发现这个依赖模型存在一些冗余。例如，依赖关系w1 ⇔ w3可以通过递归过程从w1 ⇔ w2和w2 ⇔ w3推导出来。开始时，我们以w1为起点，检查它与其他层（如w1 ⇔ w2）的依赖关系。然后，w2提供了一个新的起点，用于递归地扩展依赖关系，这反过来又“触发”了w2 ⇔ w3。这个递归过程最终以一个传递关系w1 ⇔ w2 ⇔ w3结束。在这种情况下，我们只需要两个依赖关系就可以描述组g中的关系。</p>
<p>同样的，3.2节中讨论的分组矩阵G对于依赖性建模也是冗余的，因此可以被压缩为一个形式更紧凑，边更少，但保留相同信息的形式。这就引出了依赖图。</p>
<p>依赖图D的正式构造规则是：对于所有的Gij &#x3D; 1，在D中存在从顶点i到j的路径。因此，Gij可以通过检查D中顶点i和j之间是否存在路径来推导出来。</p>
<blockquote>
<p>依赖图Dependency Graph的构造过程分步骤解释：</p>
<ol>
<li><strong>定义所有节点</strong>：首先，我们在图中定义每一层网络为一个节点。这些节点相当于我们要考虑的网络层。</li>
<li><strong>检查并添加直接的边</strong>：然后，我们仔细观察网络中每一层之间的直接连接。如果第i层和第j层之间有直接连接（即第i层的输出是第j层的输入），我们就在依赖图中添加一条从i到j的边。这样，我们就得到了所有直接相邻层之间的依赖关系。</li>
</ol>
<p>现在我们来考虑一个更为复杂的依赖关系：如果有第i层→第j层→第k层的依赖关系。在原始的分组矩阵G中，我们会同时标记Gij和Gjk为1，还需要标记Gik为1，表示第i层和第k层也存在依赖关系。</p>
<p>然而，在依赖图D中，我们不需要直接标记Gik。只要有从节点i到节点k的路径（在这种情况下，路径就是i→j→k），就认为Gik存在。这样，依赖图D只需要记录直接的依赖关系，而所有间接的依赖关系可以通过检查路径的存在来推导。这大大减少了需要记录的依赖关系的数量。</p>
<p>依赖图D的主要目标就是将分组矩阵G压缩为更简洁的形式，便于处理复杂的依赖关系。这种压缩是有效的，因为我们只关心是否存在从一个节点到另一个节点的路径，而不关心路径的具体形状。这种方法可以在保留所有重要信息的同时，大大简化了依赖关系的表示。</p>
</blockquote>
<p>3 Network Decomposition 网络分解</p>
<blockquote>
<p>这一段的主要内容是说，在实际操作中，仅仅在网络层级别上建立依赖图可能存在一些问题，因为某些基础层（如全连接层）可能有两种不同的修剪方案，比如 w[k, :] 和 w[:, k]，它们分别压缩了输入和输出的维度。此外，网络还包含一些非参数化的操作，比如跳过连接（skip connections），它们也会影响层之间的依赖关系。</p>
<p>为了解决这些问题，作者提出了一种新的表示法，将网络 F(x; w) 分解为更细的基本组件，表示为 F &#x3D; {f1, f2, …, fL}，其中每个组件 f 可以是参数化的层（如卷积）或非参数化的操作（如残差加法）。他们不再专注于层级别的关系，而是集中于层的输入和输出之间的依赖关系。特别地，他们将组件 fi 的输入和输出分别表示为 f- i 和 f+ i。对于任何网络，最终的分解可以被形式化为 F &#x3D; {f- 1 , f+ 1 , …, f- L , f+ L }。这种表示法使得依赖关系的建模更为容易，并允许同一层有不同的修剪方案。</p>
</blockquote>
<p>4 Dependency Modeling 依赖模型</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221521004.png" alt="image-20230722152158933"></p>
<blockquote>
<p>层分组是通过从f+ 4开始在DepGraph上递归传播来实现的。在该示例中，由于上面所示的发散修剪方案，在卷积输入f-4和输出f+ 4之间不存在层内依赖性。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221523007.png" alt="image-20230722152330970"></p>
<p>算法1 解释</p>
<blockquote>
<p>用于构建所“依赖图”，它是一个二维矩阵，用于表示网络中的层之间的依赖关系。</p>
<p>以下是这个算法的步骤：</p>
<ol>
<li><p>输入一个神经网络F(x; w)。</p>
</li>
<li><p>将网络F分解为两组组件：f− 和 f+。这些组件分别代表每一层的输入和输出。</p>
</li>
<li><p>初始化依赖图DepGraph D，这是一个 2L x 2L 的零矩阵，其中 L 是网络中层的数量。</p>
</li>
<li><p>双重循环遍历所有的 i 和 j，对于每一对 (i, j)，根据公式计算 D(f- i, f+ j) 和 D(f+ j, f- i) 的值。</p>
<ul>
<li><p>如果 f- i 与 f+ j 是相连的，或者它们位于同一层并且有相同的剪枝方案（sch(f- i) &#x3D; sch(f+ j)），则值为1；</p>
</li>
<li><p>否则，值为0。</p>
</li>
</ul>
</li>
<li><p>返回依赖图D。</p>
</li>
</ol>
<p>简单来说，这个算法通过检查网络中每一层的输入和输出，建立了一个表征层间和层内依赖关系的二维矩阵。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221524708.png" alt="image-20230722152425671"></p>
<blockquote>
<p>这个算法是用来在神经网络中创建一组分组，每一组中的层有依赖关系。</p>
<p>以下是这个算法的步骤：</p>
<ol>
<li><p>输入依赖图DepGraph D。</p>
</li>
<li><p>初始化一个空的集合G，这将存储分组。</p>
</li>
<li><p>遍历所有层i。</p>
</li>
<li><p>对于每个i，初始化一个包含i的新组g。</p>
</li>
<li><p>在一个循环中，持续更新g，直到没有新的元素可以添加进g为止：</p>
<ul>
<li><p>创建一个未见过的层的集合UNSEEN，这些层还没有被添加到g中。</p>
</li>
<li><p>创建一个新的集合g’，包含那些在UNSEEN中并且与g中的某个层k有依赖关系的层（即Dkj &#x3D; 1）。</p>
</li>
<li><p>更新g，将g’中的所有层添加到g中。</p>
</li>
</ul>
</li>
<li><p>将g添加到G中。</p>
</li>
<li><p>返回所有的分组G。</p>
</li>
</ol>
<p>简单来说，这个算法是通过遍历网络中的所有层，并查看哪些层与已经在组中的层有依赖关系，从而创建一系列分组。每个分组中的所有层都有相互依赖关系。</p>
</blockquote>
<h2 id="3-3-组级剪枝"><a href="#3-3-组级剪枝" class="headerlink" title="3.3 组级剪枝"></a>3.3 组级剪枝</h2><p>这部分主要讨论了如何在神经网络中实现组级别的剪枝。</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221527044.png" alt="image-20230722152727989"></p>
<blockquote>
<p>比较了三种不同的剪枝方法，即如何选择神经网络中应该删除的权重（即神经元之间的连接）。每种方法都有自己的优点和缺点，主要的区别在于他们如何定义哪些权重是“重要的”。</p>
<ol>
<li>方法(a) 是非结构化剪枝：在这种方法中，每个权重都被独立地看待。换句话说，我们只关注每个单独权重的重要性，而不考虑它们之间的关系。这种方法的优点是简单直观，缺点是它可能忽略了权重之间的依赖关系。</li>
<li>方法(b) 是结构化剪枝：这种方法在评估权重重要性时，会考虑到一层内的结构关系，即在同一层内的权重将会一起被考虑。但是，它依然忽视了在不同层之间可能存在的关联关系。</li>
<li>方法(c) 是本文提出的组级剪枝：这种方法考虑到了权重之间的依赖关系，无论这些权重是在同一层还是在不同层。它的目标是将所有关联的参数（即一组）一起设置为零，这样就可以很容易地通过一个简单的幅度方法（magnitude method）来识别它们。</li>
</ol>
<p>这三种方法的主要区别在于他们处理权重之间关系的方式。在组级剪枝中，作者认为在神经网络中，权重之间的关系是非常重要的，应该被一同考虑，从而得到更好的剪枝效果。</p>
</blockquote>
<p>在前面的部分中，作者们已经开发了一种用于分析神经网络内部依赖性的通用方法，这自然引导出了组级别剪枝问题。评估组合参数的重要性对于剪枝来说是一个重大的挑战，因为它涉及到几个关联的层。在这个部分中，作者们利用一个简单的基于范数的标准来为组级别剪枝建立了一个实用的方法。</p>
<p>给定一个参数组g &#x3D; {w1, w2, …, w|g|}，现有的标准，如L2范数的重要性I(w) &#x3D; w2，可以为g中的每个w产生独立的评分。估计组的重要性的一个自然的方式是计算一个聚合的评分I(g) &#x3D; ∑w∈g I(w)。但不幸的是，独立地在不同层上估计的重要性评分可能由于分布和大小的差异而不可加，因此没有意义。为了使这个简单的聚合适用于重要性估计，<strong>作者们提出了一个稀疏训练方法</strong>，用于在组级别上稀疏化参数，这样，那些被零化的组就可以从网络中安全地移除。</p>
<blockquote>
<p>作者在这一部分讨论了如何对神经网络进行“组级别”的剪枝，这是一种对网络进行优化的方法，用以删除网络中不重要或者冗余的部分，以减少网络的复杂性。</p>
<p>在这里，他们提到的”组”指的是一组相关的神经网络参数，比如，这些参数可能来自同一层，或者在不同层间有一些依赖关系。这些参数被看作是一个整体，而不是独立处理，因为他们在网络中的作用是相互关联的。</p>
<p>那么，问题来了，我们应该如何决定哪个“组”重要，哪个不重要呢？作者在这里提出了一种方法，即通过计算组内所有参数的L2范数（一种衡量参数重要性的方法）的总和来判断。如果一个组的总重要性得分低，那么这个组可能就不太重要，可以被剪枝掉。</p>
<p>但是，这个方法有个问题，就是不同层的参数可能具有不同的规模或分布，所以我们不能简单地将他们的重要性得分加在一起。为了解决这个问题，作者提出了一种“稀疏训练”的方法。这种方法的目标是尽量让不重要的组的参数变为0，这样这些参数就不会对网络的输出产生影响，可以安全地被剪枝掉。</p>
<p>作者使用了一个特定的公式，通过对参数进行“惩罚”，使得不重要的参数趋向于0，这就是所谓的”正则化项”。这个公式涉及一些复杂的数学概念，简单来说，就是根据每个参数的重要性对其进行加权，重要性越低的参数，其“惩罚”越大。</p>
<p>在进行了这种“稀疏训练”后，作者再用一个简单的公式来决定哪些参数是不重要的，即他们的分数低于某个阈值，然后将这些参数删除。</p>
<p>最后，作者通过实验证明，这种方法在剪枝效果上能达到与其他现代方法相当的水平，即使它的原理比较简单。</p>
</blockquote>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><ul>
<li><h2 id="a-详细的实验设置-作者在CIFAR数据集上进行了实验，评估了他们修剪算法的性能。-“Model-Data”-ResNet56和VGG19都是常见的深度学习模型，它们在许多任务上都表现出色。CIFAR-10和CIFAR-100是两个常用的图像分类数据集，分别包含10个和100个类别。-“Method”-这是正在测试的剪枝方法。表中列出了一系列的剪枝方法，包括本文作者的方法-“Ours”-。-“Base”-这是未进行剪枝的模型在测试数据集上的准确率。-“Pruned”-这是使用特定剪枝方法后的模型在测试数据集上的准确率。-“∆-Acc-”-这是剪枝前后模型准确率的变化，可以看作是剪枝对模型性能的影响。负值表示准确率下降，正值表示准确率提高。-“Speed-Up”-这是通过剪枝实现的加速比例，表示剪枝后的模型与原始模型相比，执行速度提高了多少倍。这是剪枝的主要目标之一，因为更快的模型可以更有效地处理数据和做出预测。"><a href="#a-详细的实验设置-作者在CIFAR数据集上进行了实验，评估了他们修剪算法的性能。-“Model-Data”-ResNet56和VGG19都是常见的深度学习模型，它们在许多任务上都表现出色。CIFAR-10和CIFAR-100是两个常用的图像分类数据集，分别包含10个和100个类别。-“Method”-这是正在测试的剪枝方法。表中列出了一系列的剪枝方法，包括本文作者的方法-“Ours”-。-“Base”-这是未进行剪枝的模型在测试数据集上的准确率。-“Pruned”-这是使用特定剪枝方法后的模型在测试数据集上的准确率。-“∆-Acc-”-这是剪枝前后模型准确率的变化，可以看作是剪枝对模型性能的影响。负值表示准确率下降，正值表示准确率提高。-“Speed-Up”-这是通过剪枝实现的加速比例，表示剪枝后的模型与原始模型相比，执行速度提高了多少倍。这是剪枝的主要目标之一，因为更快的模型可以更有效地处理数据和做出预测。" class="headerlink" title="a. 详细的实验设置:- 作者在CIFAR数据集上进行了实验，评估了他们修剪算法的性能。- - &gt; - “Model &#x2F; Data”: ResNet56和VGG19都是常见的深度学习模型，它们在许多任务上都表现出色。CIFAR-10和CIFAR-100是两个常用的图像分类数据集，分别包含10个和100个类别。  &gt; - “Method”: 这是正在测试的剪枝方法。表中列出了一系列的剪枝方法，包括本文作者的方法(“Ours”)。  &gt; - “Base”: 这是未进行剪枝的模型在测试数据集上的准确率。  &gt; - “Pruned”: 这是使用特定剪枝方法后的模型在测试数据集上的准确率。  &gt; - “∆ Acc.”: 这是剪枝前后模型准确率的变化，可以看作是剪枝对模型性能的影响。负值表示准确率下降，正值表示准确率提高。  &gt; - “Speed Up”: 这是通过剪枝实现的加速比例，表示剪枝后的模型与原始模型相比，执行速度提高了多少倍。这是剪枝的主要目标之一，因为更快的模型可以更有效地处理数据和做出预测。"></a>a. 详细的实验设置:<br>- 作者在CIFAR数据集上进行了实验，评估了他们修剪算法的性能。<br>- <img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221531158.png" alt="image-20230722153111108"><br>- &gt; - “Model &#x2F; Data”: ResNet56和VGG19都是常见的深度学习模型，它们在许多任务上都表现出色。CIFAR-10和CIFAR-100是两个常用的图像分类数据集，分别包含10个和100个类别。<br>  &gt; - “Method”: 这是正在测试的剪枝方法。表中列出了一系列的剪枝方法，包括本文作者的方法(“Ours”)。<br>  &gt; - “Base”: 这是未进行剪枝的模型在测试数据集上的准确率。<br>  &gt; - “Pruned”: 这是使用特定剪枝方法后的模型在测试数据集上的准确率。<br>  &gt; - “∆ Acc.”: 这是剪枝前后模型准确率的变化，可以看作是剪枝对模型性能的影响。负值表示准确率下降，正值表示准确率提高。<br>  &gt; - “Speed Up”: 这是通过剪枝实现的加速比例，表示剪枝后的模型与原始模型相比，执行速度提高了多少倍。这是剪枝的主要目标之一，因为更快的模型可以更有效地处理数据和做出预测。</h2><ul>
<li>报告了修剪模型的准确性和理论加速比。</li>
</ul>
</li>
<li>b. 详细的实验结果:</li>
<li><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221534860.png" alt="image-20230722153445775"></li>
<li><blockquote>
<p>这个表格展示了对各种网络结构（如ResNet-56、VGG-19、DenseNet-121、MobileNetv2和GoogleNet）进行不同策略的剪枝时的精确度结果。策略包括随机剪枝、不进行分组剪枝、只对卷积层进行剪枝、以及全分组剪枝。并且分别对比了统一的剪枝和学习到的剪枝的效果。</p>
<p>每行的数据对应一种剪枝策略对应的结果。</p>
<ul>
<li>“Architecture”: 这是正在测试的模型的名称。</li>
<li>“Strategy”: 这是正在测试的剪枝策略。表中列出了一系列的剪枝策略，包括随机剪枝（”Random”）、不进行分组剪枝（”No grouping”）、只对卷积层进行剪枝（”Conv-only”）以及全分组剪枝（”Full Grouping”）。</li>
<li>“Pruned Accuracy with Uniform &#x2F; Learned Sparsity”: 这是剪枝后的模型在测试数据集上的准确率，其中包括了使用统一的剪枝策略和学习到的剪枝策略的结果。剪枝策略分别对应了不同的速度提升比例，例如1.5倍、3.0倍、6.0倍、12倍。</li>
<li>“Avg.”: 这是各个速度提升比例下的平均精确度。</li>
</ul>
<p>表格的主要目的是展示在使用不同的剪枝策略以及不同的速度提升比例时，各个网络模型的精确度的变化情况，以此来评估和比较不同剪枝策略的效果。</p>
</blockquote>
<ul>
<li>结果显示，他们的方法在所有分组层（包括卷积、批归一化和全连接层）上都能促进稀疏性，从而提高修剪模型的准确性。</li>
<li>作者还可视化了他们的方法学习到的分组参数的范数，显示了组内的稀疏性。</li>
<li>进行了消融研究以验证分组的有效性，结果表明忽略分组信息会显著降低他们方法的性能。</li>
<li>作者还比较了均匀稀疏性和学习稀疏性，发现学习稀疏性通常优于均匀稀疏性，尽管有时会导致过度修剪和准确性下降。</li>
<li>他们还将其框架应用于包括DenseNet和GoogleNet在内的各种卷积神经网络，展示了其框架的通用性。</li>
<li>提供了DepGraph可视化，显示了参数的分组，为大型神经网络的修剪过程提供了便利。</li>
</ul>
</li>
</ul>
<h1 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h1><p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1TRvELQDNj9PwM-EERWbF3IQOyxZeDepp?usp=sharing#scrollTo=yaoMwy86Vhxz">Torch-Pruning-Demo.ipynb - Colaboratory (google.com)</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model_dict = &#123;</span><br><span class="line">    <span class="string">&#x27;resnet50&#x27;</span>: torchvision.models.resnet50,</span><br><span class="line">    <span class="string">&#x27;resnet18&#x27;</span>: torchvision.models.resnet18,</span><br><span class="line">    <span class="string">&#x27;convnext&#x27;</span>: torchvision.models.convnext_base,</span><br><span class="line">    <span class="string">&#x27;vgg_19_bn&#x27;</span>: torchvision.models.vgg19_bn,</span><br><span class="line">    <span class="string">&#x27;regnet_x_1_6gf&#x27;</span>: torchvision.models.regnet_x_1_6gf,</span><br><span class="line">    <span class="string">&#x27;efficientnet_b4&#x27;</span>: torchvision.models.efficientnet_b4,</span><br><span class="line">    <span class="string">&#x27;densenet121&#x27;</span>: torchvision.models.densenet121,</span><br><span class="line">    <span class="string">&#x27;vit_b_32&#x27;</span>: torchvision.models.vit_b_32,</span><br><span class="line">    <span class="string">&#x27;mobilenet_v3_large&#x27;</span>: torchvision.models.mobilenet_v3_large,</span><br><span class="line">    <span class="comment"># Register your models here. This demo only covers classification models.</span></span><br><span class="line">    <span class="comment"># Swin Transformers are not supported.</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">model = model_dict[<span class="string">&#x27;vit_b_32&#x27;</span>](pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">imp = tp.importance.MagnitudeImportance(p=<span class="number">2</span>) </span><br><span class="line"><span class="comment"># 创建一个重要性估计器，这基于权重的L2范数来评估神经网络各部分的重要性。</span></span><br><span class="line"></span><br><span class="line">ignored_layers = []</span><br><span class="line"><span class="comment"># 创建一个空的列表，用于保存我们不想剪裁的神经网络层。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> model.modules():</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, torch.nn.Linear) <span class="keyword">and</span> m.out_features == <span class="number">1000</span>: </span><br><span class="line">  <span class="comment"># 遍历模型中的所有模块。如果模块是线性层且输出特征数为1000（即分类器），则忽略这一层，不进行剪裁。</span></span><br><span class="line">    ignored_layers.append(m)</span><br><span class="line"></span><br><span class="line">round_to = <span class="literal">None</span></span><br><span class="line"><span class="comment"># 创建一个变量，用于设定剪裁的粒度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>( model, torchvision.models.vision_transformer.VisionTransformer):</span><br><span class="line">  round_to = model.encoder.layers[<span class="number">0</span>].num_heads </span><br><span class="line">  <span class="comment"># 模型是视觉变换器，则将round_to设为变换器的头部数量，这是模型特定的剪裁限制。</span></span><br><span class="line"></span><br><span class="line">pruner = tp.pruner.MagnitudePruner(</span><br><span class="line">    model = model,</span><br><span class="line">    <span class="comment"># 待剪裁的模型</span></span><br><span class="line"></span><br><span class="line">    example_inputs = torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>),</span><br><span class="line">    <span class="comment"># 输入的样例，用于模型的前向传播</span></span><br><span class="line"></span><br><span class="line">    importance = imp,     </span><br><span class="line">    <span class="comment"># 重要性估计器，用于评估哪些部分可以剪裁</span></span><br><span class="line"></span><br><span class="line">    global_pruning=<span class="literal">False</span>, </span><br><span class="line">    <span class="comment"># 设置全局剪裁为False，这意味着不是对整个网络进行剪裁，而是对各层独立进行剪裁。</span></span><br><span class="line"></span><br><span class="line">    ch_sparsity = <span class="number">0.5</span>,    </span><br><span class="line">    <span class="comment"># 设置剪裁后的稀疏度为0.5，即剪裁后每层保留的神经元或通道的比例。</span></span><br><span class="line"></span><br><span class="line">    iterative_steps = <span class="number">1</span>,  </span><br><span class="line">    <span class="comment"># 设置迭代步骤数为1，即达到目标稀疏度所需的剪裁步骤数。</span></span><br><span class="line"></span><br><span class="line">    ignored_layers = ignored_layers,        </span><br><span class="line">    <span class="comment"># 指定要忽略的层，这些层在剪裁过程中不会被剪裁。</span></span><br><span class="line"></span><br><span class="line">    round_to = round_to,  </span><br><span class="line">    <span class="comment"># 设置剪裁的粒度，即剪裁后的通道数需要是这个数的倍数。</span></span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model size before pruning</span></span><br><span class="line">base_macs, base_nparams = tp.utils.count_ops_and_params(model, torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"><span class="comment"># 计算剪裁前的模型大小。使用的度量包括MACs（乘加操作数）和参数数量。输入数据是随机生成的张量。</span></span><br><span class="line"></span><br><span class="line">pruner.step()</span><br><span class="line"><span class="comment"># 执行剪裁操作。在之前的代码中，已经对pruner对象进行了初始化，并设置了剪裁参数。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># modify some inferece-related attributes if necessary</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(model, torchvision.models.vision_transformer.VisionTransformer):</span><br><span class="line"><span class="comment"># 如果模型是视觉变换器（ViT），那么在剪裁后需要修改一些与推理相关的属性。</span></span><br><span class="line">    model.hidden_dim = model.conv_proj.out_channels</span><br><span class="line"><span class="comment"># 对于视觉变换器，剪裁后其隐藏维度（hidden_dim）需要被修改为卷积投影层的输出通道数。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameter &amp; MACs Counter</span></span><br><span class="line">pruned_macs, pruned_nparams = tp.utils.count_ops_and_params(model, torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"><span class="comment"># 计算剪裁后的模型大小。使用的度量依然是MACs和参数数量。输入数据是随机生成的张量。</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The pruned model:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"><span class="comment"># 打印剪裁后的模型。</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Summary:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Params: &#123;:.2f&#125; M =&gt; &#123;:.2f&#125; M&quot;</span>.<span class="built_in">format</span>(base_nparams/<span class="number">1e6</span>, pruned_nparams/<span class="number">1e6</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MACs: &#123;:.2f&#125; G =&gt; &#123;:.2f&#125; G&quot;</span>.<span class="built_in">format</span>(base_macs/<span class="number">1e9</span>, pruned_macs/<span class="number">1e9</span>))</span><br><span class="line"><span class="comment"># 打印剪裁前后的模型大小，包括参数数量和MACs。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Forward</span></span><br><span class="line">output = model(torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"><span class="comment"># 测试剪裁后模型的正向传播。输入数据是随机生成的张量。</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output.shape: &quot;</span>, output.shape)</span><br><span class="line"><span class="comment"># 打印正向传播的输出形状。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Backward</span></span><br><span class="line">loss = torch.nn.functional.cross_entropy(output, torch.randint(<span class="number">1</span>, <span class="number">1000</span>, (<span class="number">1</span>,)))</span><br><span class="line"><span class="comment"># 计算反向传播的损失。使用的损失函数是交叉熵，目标值是随机生成的整数。</span></span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment"># 执行反向传播，计算梯度。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="论文2：Scaling-TransNormer-to-175-Billion-Parameters"><a href="#论文2：Scaling-TransNormer-to-175-Billion-Parameters" class="headerlink" title="论文2：Scaling TransNormer to 175 Billion Parameters"></a>论文2：Scaling TransNormer to 175 Billion Parameters</h1><p>Zhen Qin♯, 1,2Dong Li♯, 1,2Weigao Sun♯, 1,2Weixuan Sun♯, 1,2Xuyang Shen♯,<br>2Xiaodong Han, 2Yunshen Wei, 2Baohong Lv, 1Fei Yuan, 2Xiao Luo,<br>1Yu Qiao, 1,2Yiran Zhong∗<br>1Shanghai AI Laboratory, 2OpenNLPLab</p>
<h1 id="Basic-Information-1"><a href="#Basic-Information-1" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Scaling TransNormer to 175 Billion Parameters (将TransNormer扩展到1750亿参数)</li>
<li>Authors: Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, Yu Qiao, Yiran Zhong</li>
<li>Affiliation: Shanghai AI Laboratory, OpenNLPLab (上海人工智能实验室, OpenNLPLab)</li>
<li>Keywords: Large Language Model, TransNormerLLM, linear attention, efficiency, scalability (大型语言模型, TransNormerLLM, 线性注意力, 效率, 可扩展性)</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.14995v1">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/OpenNLPLab/TransnormerLLM">GitHub</a></li>
</ul>
<h1 id="论文简要-1"><a href="#论文简要-1" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>本文介绍了TransNormerLLM，这是第一个基于线性注意力的大型语言模型，它在准确性和效率方面优于传统的基于softmax注意力的模型。通过引入位置嵌入、线性注意力加速、门控机制、张量归一化和推理加速等先进修改，TransNormerLLM从之前的线性注意力架构TransNormer发展而来。通过一系列的实验验证，该模型在训练和推理阶段都表现出卓越的效率和性能。</li>
</ul>
<h1 id="背景和创新点"><a href="#背景和创新点" class="headerlink" title="背景和创新点:"></a>背景和创新点:</h1><blockquote>
<ol>
<li><p>尽管现有的 Transformer 模型在许多任务上表现优秀，但是当面对大规模参数时，它们往往会遇到一些问题。首先，计算效率较低，这主要是因为 Transformer 模型需要计算和存储所有 token 之间的注意力权重，这在处理长序列时会导致计算和存储开销极大。其次，Transformer 模型还存在注意力稀释问题，即模型可能无法有效地关注到距离较远的 token。</p>
<p>为了解决这些问题，论文作者提出了 TransNormerLLM。TransNormerLLM 是对 TransNormer 模型的一个改进，它通过一系列创新的设计来解决上述问题。以下是 TransNormerLLM 的一些主要创新点：</p>
<ol>
<li><strong>位置编码</strong>：在 TransNormerLLM 中，作者引入了一种名为 LRPE-d 的新方法，这是一种线性化相对位置编码（LRPE）与指数衰减相结合的方法。这种方法可以让模型对距离较远的 token 给予较小的注意力权重，从而有效地解决了注意力稀释问题。同时，由于 LRPE-d 可以被分解为关于每个输入元素的函数，因此它与线性注意力方法完全兼容。值得一提的是，LRPE-d 的参数可以通过训练进行学习，这为模型提供了更大的灵活性。</li>
<li><strong>门控机制</strong>：TransNormerLLM 引入了一种新的门控机制，被称为 SGLU（Simple Gated Linear Unit）。SGLU 是一种改进的激活函数，它引入了一个门控参数来控制信息的流动。不同于传统的 GLU，SGLU 去除了激活函数，因为门本身就可以引入非线性，从而使得模型更加高效。</li>
<li><strong>张量归一化</strong>：在 TransNormerLLM 中，作者引入了一种新的张量归一化方法，称为 SRMSNorm（Simple RMS Norm）。SRMSNorm 是一种更简单的归一化方法，它不依赖于特征维度，因此在处理大规模模型时更为高效。</li>
</ol>
</li>
</ol>
</blockquote>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291115870.png" alt="image-20230729111523716"></p>
<ul>
<li><blockquote>
<p>当然，下面是这些公式的整理以及对应的详细解释：</p>
<ol>
<li><p><strong>位置编码 (LRPE-d)</strong>:<br>   $$<br>a_{st} &#x3D; q_s^T k_t \lambda^{s-t} \exp(i\theta(s-t))<br>$$</p>
<p>  这是 TransNormerLLM 使用的位置编码公式，其中 (q_s) 和 (k_t) 分别是位置 (s) 和 (t) 的查询和键，(\lambda) 是衰减率，(\theta) 是可学习的参数。这个公式表示，模型在计算位置 (s) 和 (t) 之间的注意力时，会考虑它们之间的相对位置，并通过 (\lambda^{s-t}) 和 (\exp(i\theta(s-t))) 来调整这个注意力。</p>
<ol start="2">
<li><strong>门控线性注意力 (GLA)</strong>:</li>
</ol>
</li>
</ol>
<p>$$<br>  O &#x3D; \text{Norm}(QK^TV) \odot U<br>$$</p>
<pre><code>  这是 TransNormerLLM 的门控机制，其中 \(Q\), \(K\), \(V\) 和 \(U\) 是模型的查询、键、值和门控参数，\(\odot\) 是元素级的乘法，\(\text&#123;Norm&#125;(\cdot)\) 是归一化函数。这个公式表示，模型在计算输出 \(O\) 时，会先计算 \(QK^TV\) 的归一化，然后与门控参数 \(U\) 进行元素级的乘法。
</code></pre>
<ol start="3">
<li><p><strong>简单门控线性单元 (SGLU)</strong>:</p>
<p>$$<br> O &#x3D; [V \odot U]W_o<br>$$</p>
<p>   这是 TransNormerLLM 的通道混合机制，其中 (V) 和 (U) 是模型的值和门控参数，(W_o) 是输出的权重矩阵。这个公式表示，模型在计算输出 (O) 时，会先计算 (V) 和 (U) 的元素级乘法，然后乘以权重矩阵 (W_o)。</p>
</li>
<li><p><strong>张量归一化 (SRMSNorm)</strong>:</p>
<p>$$<br> \text{SRMSNorm}(x) &#x3D; \frac{x}{|x|_2&#x2F;\sqrt{d}}<br>$$</p>
<p>   这是 TransNormerLLM 的张量归一化函数，其中 (x) 是输入的张量，(|x|_2) 是 (x) 的 L2 范数，(d) 是特征的维度。这个公式表示，模型在归一化 (x) 时，会先计算 (x) 的 L2 范数，然后除以 (\sqrt{d})，最后用 (x) 除以这个结果。</p>
</li>
</ol>
<p>以上四个公式是 TransNormerLLM 模型的关键部分，它们分别对应了模型的位置编码、门控机制、通道混合和张量归一化等重要功能。理解这些公式对于理解 TransNormerLLM 模型的工作原理是非常重要的。</p>
</blockquote>
</li>
<li><p>a. 理论背景:</p>
<ul>
<li>本文介绍了TransNormerLLM，这是一个基于线性注意力的大型语言模型（LLM），在准确性和效率方面超越了传统的softmax注意力模型。该模型建立在之前的线性注意力架构TransNormer的基础上，并引入了位置嵌入、线性注意力加速、门控机制、张量归一化和推理加速等先进的修改。本文强调了传统Transformer的局限性以及LLM中更高效的序列建模方法的需求。</li>
</ul>
</li>
<li><p>b. 技术路线:</p>
<ul>
<li>本文介绍了TransNormerLLM中的架构改进。它引入了将TransNormer的DiagAttention替换为线性注意力，以增强全局交互。本文还引入了具有指数衰减的LRPE来解决注意力稀释问题。在训练过程中，提出了Lightning Attention技术，显著加速线性注意力，将内存使用减少了四倍。本文简化了GLU和归一化，总体加速了20%。通过稳定的推理算法，确保了数值稳定性和恒定的推理速度。本文强调了TransNormerLLM的可扩展性以及在大规模集群上部署的能力。还提到计划开源预训练模型，促进LLM的社区驱动进展。</li>
</ul>
</li>
</ul>
<h1 id="结果-1"><a href="#结果-1" class="headerlink" title="结果:"></a>结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291141964.png" alt="image-20230729114150913"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291138381.png" alt="image-20230729113850322"></p>
<p>Transformer vs TransNormerLLM.TransNormerLLM在相同配置下，在385M和1B大小上的性能分别优于Transformer 5%和9%。</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291138203.png" alt="image-20230729113857153"></p>
<p>我们比较原始的TransNormer和改进的TransNormerLLM，结果示于表4中。TransNormerLLM分别表现出2%和1%的增强，同时显著更快。</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291142007.png" alt="image-20230729114258960"></p>
<p>位置编码在位置编码实验中，我们进行了一系列测试，比较了LRPE-d、APE（绝对位置编码）、LRPE和Exp-Decay（指数衰减）。从表5中可以明显看出，我们提出的增强已经显示出对原始模型的改进。此外，最终的方案表现出2%的LRPE方法的改进。</p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>本研究使用PyTorch和Triton在Metaseq框架中实现了TransNormerLLM模型。</li>
<li>使用Adam优化器进行训练，并使用FSDP将模型扩展到NVIDIA A100 80G集群。还采用了模型并行技术进行性能优化。模型在包含300B个标记的样本语料库上进行训练。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>架构消融实验表明，TransNormerLLM在大小为385M时比Transformer性能提高了5%，在大小为1B时提高了9%。</li>
<li>对比了不同的位置编码方法，LRPE+LRPE-d组合的效果最优，比LRPE提高了2%。</li>
<li>对于衰减温度的消融研究表明，添加衰减温度可以降低TransNormerLLM的困惑度。</li>
<li>TransNormerLLM中引入门控机制可以降低损失值。</li>
<li>对GLA激活函数进行了不同的测试，结果表明激活函数的选择对最终结果影响很小。</li>
<li>在Gated Linear Units（GLU）结构中去除激活函数对结果影响微乎其微。</li>
<li>对于归一化方法进行了多种测试，结果表明这些方法在应用于TransNormerLLM时几乎没有区别。然而，使用Triton实现的修改版SRMSNorm在计算速度上比PyTorch实现方法提供了显著的提升。</li>
<li>Lightning Attention的计算速度至少比NormAttention的PyTorch实现快2倍。</li>
<li>Lightning Attention的内存占用随序列长度线性增长，当序列长度为8192时，比基线模型的内存效率提高了4倍。</li>
<li>模型并行显著降低了内存消耗，当模型并行大小设置为8时，TransNormerLLM-7B模型在单个GPU上只需要24.1GB的内存，相比模型并行大小为1时，内存减少了62.3%。</li>
<li>TransNormerLLM在训练速度和内存消耗方面始终优于Transformer，即使启用了模型并行。</li>
<li>TransNormerLLM模型在计算速度上始终优于Transformer模型，即使模型规模更大。</li>
<li>TransNormerLLM能够以更长的上下文长度进行训练，实现更高的计算速度。</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/7%E6%9C%88%E7%AC%AC%E5%9B%9B%E5%91%A8%E4%BE%9D%E8%B5%96%E5%9B%BE%E8%AE%BA%E6%96%87%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" data-id="cllkmcmey0008ksu74hs77e3r" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/QLORA-%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/" rel="tag">QLORA 剪枝 lomo全参数微调</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%89%AA%E6%9E%9D-%E8%92%B8%E9%A6%8F/" rel="tag">剪枝+蒸馏</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E9%9B%86%E8%BF%9E%E6%8E%A5%E3%80%81%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%89%AA%E6%9E%9D/" rel="tag">密集连接、滤波器剪枝</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%8Cpromot%EF%BC%8Clora/" rel="tag">线性注意力，promot，lora</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B7%A8%E8%AF%AD%E8%A8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" rel="tag">跨语言多模态知识蒸馏</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" rel="tag">通用剪枝</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/QLORA-%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/" style="font-size: 10px;">QLORA 剪枝 lomo全参数微调</a> <a href="/tags/%E5%89%AA%E6%9E%9D-%E8%92%B8%E9%A6%8F/" style="font-size: 10px;">剪枝+蒸馏</a> <a href="/tags/%E5%AF%86%E9%9B%86%E8%BF%9E%E6%8E%A5%E3%80%81%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%89%AA%E6%9E%9D/" style="font-size: 10px;">密集连接、滤波器剪枝</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%8Cpromot%EF%BC%8Clora/" style="font-size: 10px;">线性注意力，promot，lora</a> <a href="/tags/%E8%B7%A8%E8%AF%AD%E8%A8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" style="font-size: 10px;">跨语言多模态知识蒸馏</a> <a href="/tags/%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" style="font-size: 10px;">通用剪枝</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8cvil%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">7月第二周cvil多模态知识蒸馏</a>
          </li>
        
          <li>
            <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%89%E5%91%A8/">8月第三周HomoDistil</a>
          </li>
        
          <li>
            <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3/">8月第二周剪枝相关</a>
          </li>
        
          <li>
            <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%92%B8%E9%A6%8F/">8月第一周自然语言大模型蒸馏</a>
          </li>
        
          <li>
            <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%91%A8%E6%8A%A5%E4%BE%9D%E8%B5%96%E5%9B%BE%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/">7月第四周周报依赖图通用剪枝</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>