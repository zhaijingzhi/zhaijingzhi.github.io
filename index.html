<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-10月第一周chatdb" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/08/10%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8chatdb/" class="article-date">
  <time class="dt-published" datetime="2023-10-08T07:52:42.000Z" itemprop="datePublished">2023-10-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/08/10%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8chatdb/">10月第一周chatdb</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="论文1-CHATDB-AUGMENTING-LLMS-WITH-DATABASES-AS-THEIR-SYMBOLIC-MEMORY-CHATDB-用数据库增强LLM的符号性内存"><a href="#论文1-CHATDB-AUGMENTING-LLMS-WITH-DATABASES-AS-THEIR-SYMBOLIC-MEMORY-CHATDB-用数据库增强LLM的符号性内存" class="headerlink" title="论文1 CHATDB: AUGMENTING LLMS WITH DATABASES AS THEIR SYMBOLIC MEMORY (CHATDB: 用数据库增强LLM的符号性内存)"></a>论文1 CHATDB: AUGMENTING LLMS WITH DATABASES AS THEIR SYMBOLIC MEMORY (CHATDB: 用数据库增强LLM的符号性内存)</h3><h3 id="1-Basic-Information"><a href="#1-Basic-Information" class="headerlink" title="1 Basic Information:"></a>1 Basic Information:</h3><ul>
<li>Title: CHATDB: AUGMENTING LLMS WITH DATABASES AS THEIR SYMBOLIC MEMORY (CHATDB: 用数据库增强LLM的符号性内存)</li>
<li>Authors: Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao</li>
<li>Affiliation: Tsinghua University (清华大学)</li>
<li>Keywords: Large language models, symbolic memory, multi-hop reasoning, databases, SQL statements</li>
<li>URLs: arXiv:2306.03901v2 [cs.AI] 7 Jun 2023, GitHub: <a target="_blank" rel="noopener" href="https://chatdatabase.github.io/">https://chatdatabase.github.io/</a></li>
<li><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081552265.png" alt="image-20231006103506241"></li>
<li><strong>用户 (users)</strong>: 代表与系统互动的最终用户。</li>
<li><strong>LLM 控制器 (LLM Controller)</strong>: 控制器负责控制内存的读写操作。它与”Memory”进行交互，并执行符号化（Symbolic）和非符号化（Non-Symbolic）的操作。</li>
<li><strong>ChatGPT, GPT4</strong>: 这些是系统中使用的模型或组件。它们与数据库进行交互，执行插入、更新和删除操作。</li>
<li><strong>LlaMA</strong>: 与控制器交互，并为输出提供非符号化数据。</li>
<li><strong>ChatGLM</strong>: 负责读取历史内容及其向量嵌入。</li>
<li><strong>Memory tokens&#x2F;memory matrices</strong>: 这是系统内部使用的数据结构或存储形式。</li>
<li><strong>数据库操作</strong>: 包括选择、查询和检索等操作。</li>
<li><strong>整体工作流程</strong>: 图片中描述了ChatDB的整体工作流程。LLM控制器控制对内存的读写操作。此内存存储历史信息，并提供相关的历史信息来帮助响应用户输入。在ChatDB中，重点是使用数据库作为它们的符号化内存来增强LLMs。</li>
</ul>
<h3 id="以前的工作："><a href="#以前的工作：" class="headerlink" title="以前的工作："></a>以前的工作：</h3><h4 id="1-Memory-Augmented-Large-Language-Models-LLMs"><a href="#1-Memory-Augmented-Large-Language-Models-LLMs" class="headerlink" title="1. Memory-Augmented Large Language Models (LLMs):"></a>1. Memory-Augmented Large Language Models (LLMs):</h4><ul>
<li><strong>主要点</strong>: LLMs 如 GPT-4 和 PaLM 2 已显示出强大的推理和决策能力。但由于其有限的上下文窗口大小（例如 GPT-4 只能处理 32K 令牌），它们的能力受到限制。</li>
<li><strong>启发</strong>: Memory-augmented LLMs 引入了一个内存模块，该模块可以防止模型忘记关键信息，使其能够处理超出上下文窗口大小的长文本输入。</li>
</ul>
<h4 id="2-Retrieval-augmented-in-context-learning（prompt）"><a href="#2-Retrieval-augmented-in-context-learning（prompt）" class="headerlink" title="2. Retrieval-augmented in-context learning（prompt）:"></a>2. Retrieval-augmented in-context learning（prompt）:</h4><ul>
<li><strong>主要点</strong>: 使用检索模型 (RM) 检索与 LLM 相关的信息作为提示。</li>
<li><strong>启发</strong>: 例如，Auto-GPT 3 和 Generative Agents 使用一个内存模块直接存储文本提示，允许代理跟踪其历史。这些提示然后输入到 LLM 进行处理。</li>
</ul>
<h4 id="3-Neural-Machine-Techniques"><a href="#3-Neural-Machine-Techniques" class="headerlink" title="3. Neural Machine Techniques:"></a>3. Neural Machine Techniques:</h4><ul>
<li><strong>主要点</strong>: Neural Turing Machines (NMT) 和其他技术如 GGS-NN 和 RMT 被设计为与外部可训练的内存资源互动。</li>
<li><strong>启发</strong>: 这些技术表明，结合传统的神经网络模型与外部内存可以增强模型的能力。</li>
</ul>
<h4 id="4-Reasoning-with-LLMs"><a href="#4-Reasoning-with-LLMs" class="headerlink" title="4. Reasoning with LLMs:"></a>4. Reasoning with LLMs:</h4><ul>
<li><strong>主要点</strong>: LLMs 在复杂的推理任务中存在挑战。最近的方法通过 In-Context Learning 来增强其推理能力。</li>
<li><strong>启发</strong>: Chain-of-Thought (CoT) 展示了解决样本问题的中间推理过程，大大增强了其推理能力。</li>
</ul>
<h4 id="5-LLMs-with-Databases"><a href="#5-LLMs-with-Databases" class="headerlink" title="5. LLMs with Databases:"></a>5. LLMs with Databases:</h4><ul>
<li><strong>主要点</strong>: LLMs 已展示了生成代码的能力，包括 Python 代码、Excel 命令和数据库的 SQL。</li>
<li><strong>启发</strong>: 虽然以前的工作在某种程度上涉及到数据库，但 ChatDB 将数据库视为 LLM 的外部符号化内存模块，然后利用数据库进行读写操作，从而增强了通过 chain-of-memory 的推理过程。</li>
</ul>
<h4 id="6-Tool-using-LLMs"><a href="#6-Tool-using-LLMs" class="headerlink" title="6. Tool-using LLMs:"></a>6. Tool-using LLMs:</h4><ul>
<li><strong>主要点</strong>: 从使用工具的角度看，ChatDB 也可以看作是使用 DB 作为工具的 LLM。</li>
<li><strong>启发</strong>: ChatDB 的优势在于它允许语言模型保持更准确的记录，并使用历史数据，从而解决更复杂的问题，尤其是那些需要准确的历史数据进行推理的问题。</li>
</ul>
<p>总结：过去的工作主要集中在增强 LLM 的存储和推理能力，特别是在处理超出其上下文窗口大小的长文本输入时。通过结合外部内存模块、数据库和其他工具，这些 LLM 可以更好地理解和响应用户的请求。”ChatDB” 的创新之处在于它将数据库视为 LLM 的外部符号化内存模块，并利用这个数据库进行读写操作，从而增强其推理能力。</p>
<h3 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法:"></a>2 方法:</h3><p>任务定义：给定自然语言中的用户输入和数据库中现有表的详细信息（如果没有现有表，则目标是操纵符号内存，即外部数据库，以满足用户的请求。例如，如果用户（例如，存储管理器）命令是记录、修改、查询和删除特定数据，则相应的 SQL 操作应该是分别插入、更新、选择和删除适当表中的相关数据。这些操作通常涉及数据库中的多个表。</p>
<img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081553958.png" alt="image-20231006160453417"  />

<p>ChatDB 框架概览图。红色箭头线表示内存链的过程流程，表示多个内存操作之间的连接。数据库表之间的红色箭头线表示主键和外键之间的引用关系，从主键开始到外键。为简洁起见，仅显示每个表的前四列。这个例子展示了返回 2023-01-02 上购买商品的过程，该商品的电话号码为 823451。</p>
<p>ChatDB 的框架包括三个主要阶段：输入处理、链式记忆 (chain-of-memory) 和响应摘要。</p>
<h4 id="1-输入处理-Input-Processing"><a href="#1-输入处理-Input-Processing" class="headerlink" title="1 输入处理 (Input Processing):"></a>1 <strong>输入处理 (Input Processing)</strong>:</h4><ul>
<li>职责：对用户输入进行初步处理。</li>
<li>工作原理：如果响应用户输入需要使用记忆，则 ChatDB 使用 LLMs 生成一系列中间步骤来操作符号化记忆。否则，直接使用 LLMs 生成回复。</li>
</ul>
<h4 id="2-链式记忆-Chain-of-Memory"><a href="#2-链式记忆-Chain-of-Memory" class="headerlink" title="2. 链式记忆 (Chain-of-Memory):"></a>2. <strong>链式记忆 (Chain-of-Memory)</strong>:</h4><ul>
<li>职责：执行与符号化记忆的交互的一系列中间记忆操作步骤。</li>
<li>工作原理：<ul>
<li>ChatDB 按照先前生成的一系列 SQL 语句的顺序操作符号化记忆，包括插入、更新、选择、删除等操作。</li>
<li>外部数据库执行相应的 SQL 语句，更新数据库，并返回结果。</li>
<li>需要注意的是，ChatDB 根据先前 SQL 语句的结果决定是否更新记忆操作步骤。</li>
<li>ChatDB 使用相同的程序执行下一步，直到完成所有对记忆的操作。</li>
</ul>
</li>
</ul>
<p>实现流程</p>
<blockquote>
<ol>
<li><strong>输入解析</strong>:</li>
</ol>
<ul>
<li>分析用户的输入，确定需要进行的操作和查询。</li>
<li>对输入进行预处理，例如实体识别、关系识别等，以确定要操作或查询的特定数据。</li>
</ul>
<ol start="2">
<li><strong>生成中间步骤</strong>:</li>
</ol>
<ul>
<li>使用模型（例如 LLMs）将用户的请求分解成一系列的操作和查询步骤。这可以是 SQL 语句、API 调用或其他与数据库或记忆系统的交互。</li>
<li>这些步骤应该是有序的，确保每个步骤都是在前一个步骤的基础上执行的。</li>
</ul>
<ol start="3">
<li><strong>与符号化记忆交互</strong>:</li>
</ol>
<ul>
<li>执行生成的操作和查询步骤，与外部数据库或其他符号化记忆系统进行交互。</li>
<li>根据每个步骤的结果，保存或更新中间状态，以供后续步骤使用。</li>
</ul>
<ol start="4">
<li><strong>结果处理</strong>:</li>
</ol>
<ul>
<li>一旦所有中间步骤都完成，对结果进行处理和汇总。</li>
<li>如果需要，可以使用 LLMs 生成更详细或描述性的输出。</li>
</ul>
<ol start="5">
<li><strong>反馈和学习</strong>:</li>
</ol>
<ul>
<li>根据用户的反馈和系统的性能，持续更新和训练模型，以改善其生成中间步骤和与符号化记忆交互的能力。</li>
<li>使用 “in-context learning” 和其他技术，提供模型训练时的示例，帮助模型更好地理解和执行复杂的操作。</li>
</ul>
<ol start="6">
<li><strong>优化和扩展</strong>:</li>
</ol>
<ul>
<li>随着时间的推移，不断优化和更新 CoM 的实现，以处理更复杂的请求和场景。</li>
<li>考虑与其他技术和方法结合，如 “Chain-of-Thought”，以进一步增强模型的推理和操作能力。</li>
</ul>
</blockquote>
<h4 id="3-响应摘要-Response-Summary"><a href="#3-响应摘要-Response-Summary" class="headerlink" title="3. 响应摘要 (Response Summary):"></a>3. <strong>响应摘要 (Response Summary)</strong>:</h4><ul>
<li>职责：基于一系列 chain-of-memory 步骤的结果为用户总结最终响应。</li>
<li>工作原理：经过与数据库的一系列交互后，ChatDB 将结果总结为一个清晰、有意义的回复，返回给用户。</li>
</ul>
<p>chatdb算法原理：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">**Algorithm 1: ChatDB 的算法**</span><br><span class="line"></span><br><span class="line">**输入**: </span><br><span class="line">- userInput: 用户的输入</span><br><span class="line">- dataBase: 数据库</span><br><span class="line"></span><br><span class="line">**输出**: </span><br><span class="line">- reply: 回复</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 输入处理 (Input Processing)</span><br><span class="line"></span><br><span class="line">```markdown</span><br><span class="line">1. if need manipulate memory to respond to userInput then  # 判断是否需要操作记忆来响应用户输入。</span><br><span class="line">2: memOps = LLMgetSteps(userInput)  # 使用 LLMs 生成中间操作步骤。</span><br><span class="line">3: else</span><br><span class="line">4: reply = LLM(userInput)  # 直接使用 LLMs 生成回复。</span><br><span class="line">5: return reply</span><br><span class="line">6: end if</span><br><span class="line">7: sqlResults = []  # 初始化一个数组来存储 SQL 操作的结果。</span><br><span class="line">8: newMemOps = []  # 初始化一个数组来存储更新后的记忆操作。</span><br><span class="line">9: for each memOp in memOps do  # 遍历之前生成的每个记忆操作。</span><br><span class="line">10: if need update memOp based on sqlResults then  # 判断是否需要基于先前的 SQL 结果更新当前的记忆操作。</span><br><span class="line">11: newMemOp = LLMupdateOperation(memOp, sqlResults)  # 使用 LLMs 更新当前的记忆操作。</span><br><span class="line">12: else</span><br><span class="line">13: newMemOp = memOp  # 保持当前的记忆操作不变。</span><br><span class="line">14: end if</span><br><span class="line">15: sqlResult = executeOperation(newMemOp, dataBase)  # 在数据库上执行更新后的操作。</span><br><span class="line">16: sqlResults.append(sqlResult)  # 将执行结果添加到 `sqlResults` 数组中。</span><br><span class="line">17: newMemOps.append(newMemOp)  # 将更新后的记忆操作添加到 `newMemOps` 数组中。</span><br><span class="line">18: end for</span><br><span class="line">19: reply = LLMsummary(userInput, newMemOps, sqlResults)  # 使用 LLMs 生成最终的回复基于用户输入、更新后的记忆操作和 SQL 操作的结果。</span><br><span class="line">20: return reply  # 返回生成的回复。</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081553269.png" alt="image-20231006171204438"></p>
<ol>
<li><strong>记忆格式 (Memory Format)</strong>:<ul>
<li>ChatDB 使用数据库作为其记忆存储格式。</li>
<li>基于提示的记忆存储相关的交互内容和&#x2F;或其对应的向量嵌入。</li>
<li>基于矩阵的记忆使用额外的可训练的记忆令牌或记忆矩阵作为记忆。</li>
</ul>
</li>
<li><strong>支持的操作 (Supported Operations)</strong>:<ul>
<li>ChatDB 支持插入、删除、更新和选择数据库中的数据等操作。</li>
<li>基于提示的记忆主要支持插入和选择操作，但对更新和删除的支持不完整。</li>
<li>基于矩阵的记忆支持读取和写入操作，但神经网络执行的确切操作并不明确。</li>
</ul>
</li>
<li><strong>记忆存储 (Memory Storage)</strong>:<ul>
<li>ChatDB 使用数据库以结构化格式存储记忆。</li>
<li>基于提示的记忆和基于矩阵的记忆都被视为半结构化。</li>
</ul>
</li>
<li><strong>记忆执行 (Memory Execution)</strong>:<ul>
<li>ChatDB 使用 SQL（一种符号语言）在其数据库记忆上执行操作，从而具有符号性。</li>
<li>基于提示的记忆和基于矩阵的记忆执行都被认为是非符号性的。</li>
</ul>
</li>
<li><strong>可解释性 (Interpretability)</strong>:<ul>
<li>ChatDB 的记忆具有高度的可解释性。</li>
<li>基于提示的记忆的可解释性通常受到限制。</li>
<li>对于基于矩阵的记忆方法，可解释性较低。</li>
</ul>
</li>
<li><strong>状态跟踪 (State Tracking)</strong>:<ul>
<li>ChatDB 的记忆准确地跟踪了 LLMs 的当前状态。</li>
<li>基于矩阵的记忆方法可以跟踪 LLMs 的当前状态。</li>
<li>基于提示的记忆方法仅存储历史上下文，而没有明确理解当前状态。</li>
</ul>
</li>
</ol>
<ul>
<li>a. 理论背景:<ul>
<li>本文提出了ChatDB，这是一个将大型语言模型（LLMs）与数据库结合的框架，将数据库作为其外部符号内存。该框架使用结构化查询语言（SQL）语句实现历史数据的结构化存储，并允许进行符号和复杂数据操作。通过将用户输入转换为多步中间内存操作，提出的链式内存（CoM）方法实现了有效的内存操作，提高了ChatDB的性能。将LLMs与数据库作为外部符号内存的组合特别适用于高准确性和长期数据记录和处理。实验数据表明，ChatDB在合成数据集上明显优于ChatGPT。该提出的框架在使用数据库作为符号内存方面独特，可以实现更准确的记录，并利用历史数据进行复杂和准确的推理。ChatDB解决了LLMs在存储、检索和操作历史信息方面面临的主要挑战。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>ChatDB是一个基于用户自然语言输入和现有表格细节来操作符号内存（如数据库）的框架。该框架由三个主要阶段组成：输入处理、链式内存和响应摘要。链式内存是ChatDB的主要组成部分，负责执行一系列中间内存操作步骤与符号内存进行交互。这种方法使得大型语言模型能够以更高的准确性执行复杂的数据库操作，并更好地处理意外情况，使其成为现实世界应用的一种有前景的方法。ChatDB使用数据库作为其内存格式，并支持在数据库内存中插入、删除、更新和选择数据等操作。与基于提示和基于矩阵的内存方法相比，ChatDB具有更高的可解释性、状态跟踪和符号存储，并支持更广泛的内存操作。</li>
</ul>
</li>
</ul>
<h3 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3 实验结果:"></a>3 实验结果:</h3><p>我们通过实验来评估使用数据库增强 LLM 作为其符号记忆的有效性。实验结果表明，ChatDB 显着优于基线模型 ChatGPT，突出了符号记忆集成的优势。 </p>
<p>3.1 实验设置：</p>
<p>使用数据库作为符号记忆特别适用于需要精确记录和处理历史信息的场景，例如各种数据管理场景。为了适应ChatDB的用例，并与其他模型进行定量比较，我们构建了一个模拟水果车间管理的合成数据集。此外，为了评估模型的性能，我们收集了一组 50 个问题和带注释的候选答案。这些问题的难度各不相同，从需要多跳推理的简单推理到只需要从历史数据中检索信息的难问题不等。有 15 个简单的问题和 35 个难题。每个问题都由模型独立回答。</p>
<p>3.2 模型配置</p>
<p>ChatDB：ChatDB 中使用的 LLM 是 ChatGPT (GPT-3.5 Turbo)，超参数大小设置为 0。我们使用 MySQL 数据库作为外部符号内存。</p>
<p>baseline：我们使用ChatGPT (GPT-3.5 Turbo)作为最大令牌长度为4096的基线模型。与ChatDB类似，我们将温度设置为0。</p>
<p>在神经网络中，特别是生成模型如语言模型，”temperature”是一个常用的参数，它会影响模型输出的多样性。Temperature值的调整可以使模型的输出从非常确定性（低多样性）到非常随机（高多样性）。</p>
<ul>
<li><strong>Temperature &#x3D; 1</strong>：不改变模型的输出概率分布。模型将按其学到的内容生成输出。</li>
<li><strong>Temperature &gt; 1</strong>：会增加所有事件的输出概率，导致输出更加随机。</li>
<li><strong>Temperature &lt; 1</strong>：会使高概率的事件更有可能发生，使输出更加确定性。</li>
<li><strong>Temperature &#x3D; 0</strong>：会使模型总是选择概率最高的输出，即最确定性的输出。但在实际实施中，通常不会完全设置为0，因为这可能会导致计算问题。</li>
</ul>
<p>3.3 数据集配置</p>
<p>我们综合了水果车间管理记录的数据集，称为“水果车间数据集”。该数据集模拟了商店中的四种常见操作：购买、销售、不断变化的价格和商品回报。我们确保所有历史记录都是有效的，并且不会出现负面清单等问题。我们生成 70 条按时间顺序排列的记录，总共大约 3.3k 个标记，位于 ChatGPT (4096 个标记) 的最大标记长度限制内。</p>
<p>下面是如何对外部数据库（符号化内存）进行操作以响应与水果商店相关的四种常见操作：购买货物、销售货物、商品退货、更改价格。</p>
<ol>
<li>购买货物数据的交互（这个是购买商品的一些chatdb sql交互）<ol>
<li>商店从供应商购买货物。</li>
<li>系统首先检查是否已经有该供应商的记录，如果没有，则插入供应商信息。</li>
<li>然后，系统检查是否有该水果的记录（在这里是樱桃），如果没有，则插入水果信息。</li>
<li>接着，系统会插入购买记录，并记录购买的每一项内容。</li>
<li>最后，系统更新库存数量，并设置或更新销售价格。</li>
</ol>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081553529.png" alt="image-20231005092615061"></p>
<ol start="2">
<li>销售货物的交互：<ol>
<li>客户从商店购买货物。</li>
<li>系统首先检查是否已经有该客户的记录，如果没有，则插入客户信息。</li>
<li>接着，系统插入销售记录，并记录每一项销售内容。</li>
<li>最后，系统更新水果的库存数量。</li>
</ol>
</li>
</ol>
<p>​	<img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081553076.png" alt="image-20231005093345524"></p>
<ol start="3">
<li><p><strong>退货</strong>操作交互：</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081553100.png" alt="image-20231005093909814"></p>
<ol>
<li>客户退回之前购买的货物。</li>
<li>系统首先找到对应的销售记录。</li>
<li>根据找到的销售记录，系统获取该次销售的所有商品及其数量。</li>
<li>然后，系统增加库存数量，表示退货商品已回到库存。</li>
<li>最后，系统删除与这次销售相关的所有记录。</li>
</ol>
</li>
<li><p><strong>更改价格</strong></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081553504.png" alt="image-20231005093926576"></p>
<ol>
<li>商店更改货物的销售价格。</li>
<li>系统更新对应货物的销售价格。</li>
</ol>
</li>
</ol>
<p>实验对比：相当于自己构造数据集了</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081553578.png" alt="image-20231005100328764"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081553199.png" alt="image-20231005100503684"></p>
<p>想法：</p>
<p>chatfig：与图片进行对话</p>
<h3 id="图片解析为LLM能理解的形式："><a href="#图片解析为LLM能理解的形式：" class="headerlink" title="图片解析为LLM能理解的形式："></a>图片解析为LLM能理解的形式：</h3><ol>
<li><p><strong>图像到文本的转换</strong>: </p>
<ul>
<li>最直观的方法是使用图像识别技术（如OCR）来从图像中提取文本。但这仅适用于包含明确文本的图片。</li>
<li>对于非文本图片（例如风景、物体、人物等），可以使用图像描述或图像字幕生成技术。这种技术使用深度学习模型将图像内容转化为描述性的句子。例如，对于一张狗在草地上玩耍的图片，模型可能会输出“一只棕色的狗在绿色的草地上玩耍”。</li>
<li>科研图片，论文图片理解和识别</li>
</ul>
</li>
<li><p><strong>语义理解</strong>:</p>
<ul>
<li>仅仅将图像转化为文本可能不足以捕捉其所有语义内容。例如，图像中的色彩、情感或复杂的交互可能难以用简单的句子描述。这可能需要更高级的模型来理解和解释。</li>
</ul>
</li>
<li><p><strong>向量嵌入</strong>:</p>
<ul>
<li>一种可能的策略是将图像直接转化为向量形式（称为嵌入），这些向量可以表示图像的语义内容。这些向量可以被视为LLM的输入，与文本数据一起处理。现有的技术，如预训练的图像分类模型，可以用于此目的。</li>
</ul>
</li>
</ol>
<h3 id="难点：VQA"><a href="#难点：VQA" class="headerlink" title="难点：VQA"></a>难点：VQA</h3><ol>
<li><p><strong>高质量的图像描述</strong>: </p>
<ul>
<li>自动图像描述技术虽然已经取得了很大进步，但仍然面临挑战，尤其是对于复杂或不常见的图像。</li>
</ul>
</li>
<li><p><strong>实时性</strong>:</p>
<ul>
<li>图像处理和描述生成可能会比简单的文本处理更耗时，这可能影响到系统的响应时间。</li>
</ul>
</li>
<li><p><strong>信息丢失</strong>:</p>
<ul>
<li>任何从图像到文本的转换都可能导致信息的丢失。例如，图像中的某些细节、色彩或情感可能难以完全转化为文本。</li>
</ul>
</li>
<li><p><strong>模型的训练和维护</strong>:</p>
<ul>
<li>需要大量的标注数据来训练高质量的图像描述模型。此外，这些模型需要定期更新以处理新的图像内容。</li>
</ul>
</li>
</ol>
<p>将用户上传的图片解析为LLM能理解的形式是一个具有挑战性的任务，但随着技术的进步，这些挑战逐渐得到了解决。为了实现这一目标，可能需要结合多种技术，并进行大量的模型训练和优化。</p>
<h3 id="论文2：Human-Parity-on-CommonsenseQA-Augmenting-Self-Attention-with-External-Attention-CommonsenseQA上的人类水平：通过外部注意力增强自注意力"><a href="#论文2：Human-Parity-on-CommonsenseQA-Augmenting-Self-Attention-with-External-Attention-CommonsenseQA上的人类水平：通过外部注意力增强自注意力" class="headerlink" title="论文2：Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention (CommonsenseQA上的人类水平：通过外部注意力增强自注意力)"></a>论文2：Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention (CommonsenseQA上的人类水平：通过外部注意力增强自注意力)</h3><h3 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h3><ul>
<li>Title: Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention (CommonsenseQA上的人类水平：通过外部注意力增强自注意力)</li>
<li>Authors: Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng Gao, Pengcheng He, Michael Zeng, Xuedong Huang</li>
<li>Affiliation: Microsoft Corporation (微软公司)</li>
<li>Keywords: self-attention, transformer architecture, external attention, commonsense reasoning, human parity (自注意力，Transformer架构，外部注意力，常识推理，人类水平)</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.03254v3">Paper</a>, <a href="github:">GitHub Code</a></li>
</ul>
<h3 id="论文简要"><a href="#论文简要" class="headerlink" title="论文简要 :"></a>论文简要 :</h3><ul>
<li>本文提出了一种在Transformer架构中增加外部注意力机制的方法，通过将外部知识和上下文引入预测过程，希望减少对越来越大的模型的需求，增加人工智能系统的民主化。通过在常识推理任务上的实验证明，这种外部注意力机制可以显著提高现有人工智能系统的性能，使从业者能够轻松地将基础人工智能模型定制为多样化的下游应用，达到了人类水平的性能。</li>
<li>当今大多数人工智能系统都专注于在大量不同数据上使用自注意力机制和转换器架构，以实现令人印象深刻的性能提升。在本文中，我们建议使用外部注意机制来增强 Transformer 架构，以带来外部知识和上下文以承担。通过将外部信息整合到预测过程中，我们希望减少对越来越大模型的需求并增加人工智能系统的民主化。我们发现所提出的外部注意机制可以显着提高现有 AI 系统的性能，允许从业者轻松地将基础 AI 模型定制到许多不同的下游应用程序。特别是，我们专注于常识推理的任务，表明所提出的外部注意力机制可以增强现有的 Transformer 模型并显着提高模型的推理能力。该系统，常识推理 (KEAR) 的基于知识的外部注意力，在开放 CommonsenseQA 研究基准上达到了人类平等，与人类准确率为 89.4%，而人类准确率为 88.9%。</li>
</ul>
<h3 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h3><ul>
<li>论文背景: 近年来，自注意力机制和Transformer架构在大量多样化数据上取得了令人瞩目的性能提升，成为当今人工智能系统的关注重点。然而，巨大模型的使用已经引发了一系列问题，包括模型理解和生成能力仍然落后于人类，以及模型规模对于利用、部署、解释和环境影响等方面带来的挑战。因此，本文提出了一种增加外部注意力机制的方法，以引入相关的上下文和知识，从而减少对大型模型的依赖，增加人工智能技术的可访问性和民主化。</li>
<li>过去方案: 过去的研究表明，随着模型规模的增大和更多数据的使用，模型的学习能力会得到提升。然而，这种“扩大规模”的方法在可持续性和实用性方面存在问题，并且与人类水平的性能仍有差距。</li>
<li>论文的Motivation: 鉴于当前Transformer模型的机制，本文提出了一种外部注意力机制的思路。通过提供相关的上下文和知识，使模型能够从外部获取信息，而不仅仅依赖于内部的自注意力机制。这种方法类似于人类进行智能活动的方式，我们经常借助搜索引擎、字典或他人的信息来获取所需的知识。此外，外部注意力的另一个好处是，由于相关的知识存储在模型之外，从业者可以轻松更新知识源，改变模型的行为。通过明确表示知识，模型的决策过程变得更加透明和可解释。</li>
<li><strong>Transformer模型的影响</strong>：自从2017年Vaswani等人提出，Transformer模型在许多AI领域都取得了巨大的成功，这主要归功于它的自注意力机制。</li>
<li><strong>模型规模</strong>：随着时间的推移，Transformer模型越来越大，从BERT的1.1亿参数到GPT-3的1750亿参数。但尽管如此，这些大型模型的理解和生成能力仍然落后于人类。</li>
<li><strong>模型规模的问题</strong>：这些大型模型不仅对环境产生了影响，而且在实际应用中也面临许多挑战。</li>
<li><strong>新的方法</strong>：文章提出了一种新的方法，即给模型提供外部上下文和知识。这种方法不仅可以减小模型的规模，而且还可以使AI技术更加普及。</li>
<li><strong>外部注意力的优势</strong>：由于知识是以符号方式存储的，因此即使是中等大小的Transformer模型也可以在NLP任务上表现出色。另外，这种方法的另一个好处是，由于相关知识是存储在模型之外的，所以从业者可以轻松地更新知识来源，从而改变模型的行为。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081553783.png" alt="image-20231007105432886"></p>
<ol>
<li>展示了一个新的方法，即将外部知识与输入数据相结合，以改进常识推理。</li>
<li><strong>数据来源</strong>：模型从多个外部数据集中提取相关知识，这些数据集包括知识图、词典和训练数据。例如，有一部分显示了从ConceptNet、OpenBookQA、CommonsenseQA、RiddleSense和Rainbow等数据集中检索的数据。</li>
<li><strong>如何工作</strong>：对于给定的输入问题（例如：“人们在弹吉他时都做什么？”），模型会查找相关的外部知识（例如，从知识图中获取“弹吉他的子事件是唱歌”），然后将这些信息与原始输入进行结合。</li>
<li><strong>注意力机制</strong>：模型将使用自注意力机制来处理这些组合的输入，并通过外部注意力机制来关注外部知识。</li>
<li><strong>结果预测</strong>：模型最后将基于这些处理后的输入来预测答案。</li>
<li><strong>特点</strong>：这种方法的一个关键特点是，它不需要对原始Transformer模型进行任何结构上的修改，只需通过文本级连接将输入和知识结合在一起。</li>
</ol>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h3><p><strong>任务定义</strong><br>研究的核心是一个多选问题的常识推理任务。在这个任务中，模型的目标是从给定的选择列表c1, c2, …, cn中为一个常识问题q选择正确的答案。模型输出一个概率分布P，反映每个选项被选择为正确答案的概率。常识推理</p>
<p><strong>2.1 外部关注</strong></p>
<ul>
<li><p><strong>自注意力 (Self Attention)</strong><br>自注意力是Transformer结构的核心组件。其计算方式可以用以下公式表示：<br>[$ Q &#x3D; H_l W_q$ ]<br>[ $K &#x3D; H_l W_k $]<br>[ $V &#x3D; H_l W_v$ ]<br>[ $A &#x3D; \frac{Q K^T}{\sqrt{d}} $]<br>[ $H_{l+1} &#x3D; \text{softmax}(A)V$ ]<br>其中，($ H_l$ ) 是传入第l层的隐藏向量，($ W_q$, $W_k,$ $W_v $) 是投影矩阵，N代表输入长度，d代表隐藏向量的维度。</p>
</li>
<li><p><strong>外部注意力 (External Attention)</strong><br>在常识问题答案中，通常所需的答案信息并不直接包含在输入中。为此，这项研究提出了引入外部知识的方法。此处，该外部知识被表示为K，并与原始输入文本连接。然后，通过在合并的输入上应用自注意力，模型可以在知识文本和问题&#x2F;答案之间进行推理。</p>
<p><strong>外部注意力 (External Attention)</strong></p>
<p>这部分内容是关于如何在模型中整合外部知识，以提高模型对常识问题答案的推理能力。对于常识问题，所需的答案信息很可能不直接出现在输入中，因此，引入外部知识变得尤为重要。</p>
<p><strong>1. 外部知识 K：</strong></p>
<ul>
<li>描述为文本格式的额外知识，记为 K &#x3D; [xK1 , xK2 , …, xK Nk ]。</li>
<li>这可以是来自不同知识库、文章或任何其他信息源的文本。</li>
</ul>
<p><strong>2. 如何整合外部知识：</strong></p>
<ul>
<li>虽然存在许多方法可以将外部知识整合到模型中（例如，使用图神经网络），但在此论文中，作者选择了一种简单的方法：直接将知识与输入文本连接起来。</li>
<li>新的输入 H0 是原始输入 X 和外部知识 K 的连接：H0 &#x3D; [X; K] &#x3D; [x1, …, xN , xK1 , …, xK Nk ]。</li>
</ul>
<p><strong>3. 连接的优势：</strong></p>
<ul>
<li>该方法的一个主要优势是不需要修改现有的模型架构。因为你只是扩展了输入，而不是改变了模型的核心。</li>
<li>这样，模型可以直接处理增加了知识的输入，而不需要额外的处理步骤或修改。</li>
<li>一旦外部知识与原始输入连接，可以在 H0 上应用自注意力。</li>
<li>这允许模型自由地在知识文本和问题&#x2F;选择之间进行推理。</li>
<li>结果是，模型获得了增强的推理能力，因为它现在可以利用外部知识来解答问题。</li>
</ul>
</li>
</ul>
<p><strong>2.2 外部知识源</strong></p>
<ul>
<li><p>**知识图 (Knowledge Graph)**：知识图提供了一些有关常识推理的事实，这些事实可能不在通常的语料库中。</p>
<ul>
<li><p><strong>目的</strong>：知识图谱包含经过策划的事实，这些事实可能在常规语料库中没有出现，但对常识推理很有帮助。</p>
</li>
<li><p><strong>数据来源</strong>：该文本使用ConceptNet图谱（由Speer等人于2017年开发），并遵循KCR项目来检索相关的关系三元组。</p>
</li>
<li><p><strong>检索方式</strong>：假设问题中的实体是<code>eq</code>，选项中的实体是<code>ec</code>。如果ConceptNet中<code>eq</code>到<code>ec</code>有直接的边<code>r</code>，则选择这个三元组<code>(eq, r, ec)</code>。如果没有这样的边，就检索所有从<code>ec</code>出发的三元组，然后为每个三元组打分。分数是由三元组的置信度<code>wj</code>（由ConceptNet提供）和关系类型权重<code>trj</code>的乘积计算的。最后，选择得分最高的三元组。</p>
</li>
<li><p><strong>结果表示</strong>：如果选中的三元组是<code>(e1, r, e2)</code>，那么KG的知识表示为<code>KKG(q, c) = [e1 r e2]</code>。</p>
</li>
<li><p><strong>其他注意事项</strong>：在CommonsenseQA数据集中，<code>eq</code>和<code>ec</code>都已提供。如果没有提供，他们会使用实体链接技术找到与输入文本相关的知识图谱节点。</p>
</li>
</ul>
<p><strong>2. 词典 (Dictionary)：</strong></p>
<ul>
<li><p><strong>目的</strong>：虽然预训练的语言模型接触过大量的文本数据，但由于单词的长尾分布，一个单词的表示质量高度依赖于它在预训练语料库中的频率。与此相反，词典可以为单词提供准确的语义解释，而不考虑它们在数据集中的频率。</p>
</li>
<li><p><strong>数据来源</strong>：作者遵循DEKCOR项目，使用Wiktionary的定义作为问题和答案概念的外部知识。</p>
</li>
<li><p><strong>操作方式</strong>：对于每一个概念，他们从Wiktionary中获取最常见（最频繁）的定义。定义文本对于<code>eq</code>是<code>dq</code>，对于<code>ec</code>是<code>dc</code>。</p>
</li>
<li><p><strong>结果表示</strong>：词典的知识表示为<code>Kdict(q, c) = [eq : dq ; ec : dc]</code>。</p>
</li>
</ul>
<p>作者使用了两个外部知识源：一个是知识图谱，特别是ConceptNet，来提供与问题和答案相关的事实；另一个是词典，特别是Wiktionary，来提供与问题和答案相关的概念的定义。</p>
</li>
<li><p>**训练数据 (Training Data)**：这是一个创新之处，研究者提议从训练数据中检索相关的问题和答案，并将其作为额外的知识源。</p>
</li>
</ul>
<p><strong>2.3 提高常识推理能力的一般方法</strong></p>
<ul>
<li><p><strong>文本编码器 (Text Encoders)</strong></p>
<p>为了提高NLU（自然语言理解）的性能，之前的研究尝试使用了多种文本编码器，如BERT、RoBERTa、ALBERT、T5、ELECTRA和DeBERTa。这些模型在GLUE基准测试上都取得了最先进的性能。基于这一事实，作者选择评估这些模型作为常识推理任务的编码器。</p>
</li>
<li><p><strong>虚拟对抗性训练 (Virtual Adversarial Training, VAT)</strong></p>
<p>VAT是一种强化模型鲁棒性的方法，已被证明能够提高一般NLU和问答任务的性能。</p>
<ul>
<li><p><strong>基本目标</strong>: 多项选择的常识推理任务的主要目标是最小化交叉熵损失。公式(2)表示这一目标，其中<code>f</code>代表模型的预测（关于选择的分布P），θ表示模型参数，y是真实答案的one-hot向量，CE是交叉熵，D是经验数据分布。</p>
</li>
<li><p><strong>VAT操作</strong>: VAT的工作原理是找到一个更新δ，这个更新会导致预测分布的最大变化，但受到L2范数约束。此后，增加一个一致性正则化损失项，目的是最小化函数输出与输入变化δ之间的差异。这就是公式(3)所表示的内容。</p>
<p>公式(3)的第二部分，即$max‖δ‖2≤ε CE(f (x; θ), f (x + δ; θ)$)，确保模型对小的输入扰动（如δ）是鲁棒的。α和ε是超参数，可以调整以控制一致性正则化的强度和允许的输入扰动大小。</p>
</li>
</ul>
</li>
<li><p>a. 理论背景:</p>
<ul>
<li>本文介绍了在AI系统中使用自注意力机制和Transformer架构，并强调了它们在性能上的显著提升。然而，作者提出了在Transformer架构中增加外部注意力机制以融入外部知识和上下文。这种方法旨在减少对更大模型的依赖，并提高AI系统的可访问性。作者证明了所提出的外部注意力机制显著提高了现有AI系统的性能，特别是在常识推理任务中。所提出的系统名为Knowledgeable External Attention for Commonsense Reasoning (KEAR)，在CommonsenseQA基准测试中实现了人类水平的准确性，达到了89.4%。</li>
</ul>
</li>
<li><p>b. 技术路线:</p>
<ul>
<li>本文描述了所提出方法中使用的外部注意力框架。自注意力机制是Transformer架构的关键组成部分，用于分析输入数据的内部结构。除了自注意力，作者还提出将外部知识和上下文整合到模型中。这是通过将外部知识与输入文本进行连接来实现的。这种方法的优点是不需要对现有模型架构进行任何修改。模型可以自由地使用自注意力在知识文本和问题&#x2F;选项之间进行推理。</li>
</ul>
</li>
</ul>
<p>实验：</p>
<p>数据集：</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081553412.png" alt="image-20231006211215926"></p>
<p>用于训练数据检索的数据集。 NLI 代表自然语言推理，MC 是多项选择，MRC 是机器阅读理解，CLF 是分类，NSP 是下一句预测</p>
<p>展示三种外部知识来源如何增强常识推理性能，通过结合这三种技术，帮助实现在CommonsenseQA基准测试上达到人类的表现水平。</p>
<p><strong>实验概述</strong>:</p>
<ol>
<li><strong>设置</strong>:<ul>
<li><strong>数据</strong>: 重点研究CommonsenseQA (CSQA)基准测试，它是一个需要常识知识的多选题问答数据集，包含12k问题，都是基于ConceptNet创建的。</li>
<li><strong>模型设置</strong>: 输入文本传入预训练的文本编码器（如DeBERTa），取[CLS]标记的表示，并对问题和答案文本设置段落ID为0，对附加的知识文本设置段落ID为1。</li>
<li><strong>实现细节</strong>: 使用AdamW优化器对模型进行微调，批大小设置为48或更小，以适应单个GPU。模型训练10个周期，取dev集上的最佳结果。</li>
</ul>
</li>
<li><strong>单独组件的效果</strong>:<ul>
<li><strong>常规方法</strong>: NLI任务的总体表现与CommonsenseQA上的常识推理能力呈正相关。</li>
<li><strong>外部注意力的效果</strong>: 所有提出的知识来源在所有基本编码器模型上都提高了常识推理准确性。</li>
</ul>
</li>
<li><strong>结合技术</strong>:<ul>
<li><strong>KEAR结果</strong>: KEAR模型结合了之前实验中的最佳技术，包括最佳编码器和对所有知识来源的外部关注，以进一步提高性能。最佳单一模型（DeBERTaV3-large + KEAR）在dev集上达到了91.2%的准确性。</li>
<li><strong>集成模型</strong>: 经过多次实验，最终的集成模型在dev集上达到了93.4%的准确率。</li>
</ul>
</li>
<li><strong>案例研究</strong>: 通过CommonsenseQA数据集中的两个例子，说明模型如何在所有检索到的知识来源之间进行推理，从而得出正确的答案。</li>
</ol>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/2023.10/assets/image-20231007101741253.png" alt="image-20231007101741253"></p>
<p>MNLI 是 MultiNLI 的简称，表示 “Multi-Domain Natural Language Inference” 数据集。这是一个大型、多领域的自然语言推理（NLI）任务的数据集。自然语言推理是一种常见的NLP任务，要求模型判断一个假设（hypothesis）是否根据一个前提（premise）是真的、假的，还是中立的。例如，给定前提 “The man is playing the piano” 和假设 “The man is playing a musical instrument”，模型应该判断假设是真的。</p>
<p>MultiNLI 数据集包含了大量这样的前提-假设对，并涵盖了多个领域和文体，这使得模型需要具备广泛和多样的语言理解能力。数据集旨在帮助研究人员训练和评估他们的NLP模型在多种情境下的泛化能力。</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/2023.10/assets/image-20231007101952871.png" alt="image-20231007101952871"></p>
<p>描述了为不同的知识源应用外部注意力时的效果。这里可以看到，对于不同的基础编码器模型（如E-l+VAT, D-xxl, DV3-l），添加外部的知识源（如KG, Dictionary, Training data）都带来了性能提升。</p>
<ol>
<li><strong>KG (知识图谱)</strong>:<ul>
<li>知识图谱是一种结构化的知识库，通常表示为实体和关系的图。例如，一个知识图谱可能会有一个实体为“苹果”和关系为“是”连接到另一个实体“水果”。这提供了一种将丰富的背景知识融入到模型中的方式。</li>
<li><strong>特点</strong>：结构化，能够表示复杂的实体和关系，可以帮助模型理解背景知识和上下文关系。</li>
</ul>
</li>
<li><strong>Dictionary (词典)</strong>:<ul>
<li>词典通常为单词或短语提供定义和解释。它可以帮助模型理解特定词语的含义和使用上下文。</li>
<li><strong>特点</strong>：为单词和短语提供准确的定义和解释，可以帮助模型更好地理解文本。</li>
</ul>
</li>
<li><strong>Training data (训练数据)</strong>:<ul>
<li>原始的训练数据，或者是为了提高模型性能而特别选定的其他数据。这些数据可以提供更多的上下文，帮助模型更好地理解和回答问题。</li>
<li><strong>特点</strong>：与任务直接相关，为模型提供了更多的上下文和示例，可以帮助模型更好地进行常识推理。</li>
</ul>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081554277.png" alt="image-20231007102906419"></p>
<h3 id="idea-chatkg-长文本理解"><a href="#idea-chatkg-长文本理解" class="headerlink" title="idea: chatkg  长文本理解"></a>idea: chatkg  长文本理解</h3><h2 id="论文3：Fusing-Context-Into-Knowledge-Graph-for-Commonsense-Question-Answering-将上下文融入知识图谱以进行常识问答"><a href="#论文3：Fusing-Context-Into-Knowledge-Graph-for-Commonsense-Question-Answering-将上下文融入知识图谱以进行常识问答" class="headerlink" title="论文3：Fusing Context Into Knowledge Graph for Commonsense Question Answering (将上下文融入知识图谱以进行常识问答)"></a>论文3：Fusing Context Into Knowledge Graph for Commonsense Question Answering (将上下文融入知识图谱以进行常识问答)</h2><h3 id="Basic-Information-1"><a href="#Basic-Information-1" class="headerlink" title="Basic Information:"></a>Basic Information:</h3><ul>
<li>Title: Fusing Context Into Knowledge Graph for Commonsense Question Answering (将上下文融入知识图谱以进行常识问答)</li>
<li>Authors: Yichong Xu, Chenguang Zhu, Ruochen Xu, Yang Liu, Michael Zeng, Xuedong Huang</li>
<li>Affiliation: Microsoft Cognitive Services Research Group (微软认知服务研究小组)</li>
<li>Keywords: commonsense question answering, knowledge graph, language modeling, contextual information, external entity descriptions</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.04808v3">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/microsoft/DEKCOR-CommonsenseQA">GitHub Code</a></li>
</ul>
<h4 id="论文简要-1"><a href="#论文简要-1" class="headerlink" title="论文简要 :"></a>论文简要 :</h4><ul>
<li>本研究提出了一种将上下文融入知识图谱的方法，用于常识问答。通过从外部实体描述中提取上下文信息，将其作为额外输入提供给预训练语言模型，从而提高常识问答的性能。该方法在CommonsenseQA数据集中取得了最先进的结果，并在OpenBookQA中超过了其他非生成模型的最佳结果。</li>
</ul>
<p>背景信息:</p>
<ul>
<li>论文背景: 常识问答是一项需要模型理解常识和事实知识以回答关于世界事件的问题的任务。过去的方法通常将语言建模与知识图谱相结合，但是知识图谱虽然包含丰富的结构信息，却缺乏上下文来提供对概念的更精确理解，尤其在标注数据不足的情况下，将知识图谱融入语言建模存在一定的差距。</li>
<li>过去方案: 过去的方法将语言模型与各种形式的知识图谱相结合，包括知识库、关系路径、图关系网络和异构图等，以提高常识推理和问答的性能。然而，这些模型的性能与人类之间仍存在一定的差距，其中一个原因是知识图谱虽然可以编码概念之间的拓扑信息，但缺乏丰富的上下文信息。</li>
<li>论文的Motivation: 鉴于知识图谱缺乏上下文信息，本研究提出了一种利用外部实体描述提供上下文信息的方法。通过从Wiktionary中检索相关概念的描述，并将其作为额外输入提供给预训练语言模型，从而提供对知识的更全面理解。通过实验证明，该方法在CommonsenseQA数据集中取得了最先进的结果，并在OpenBookQA中超过了其他非生成模型的最佳结果。</li>
<li><strong>对常识的理解是人类智慧的关键</strong>：人类有能力基于观察和知识对日常事务进行推理。这种能力为人与人之间的沟通和与世界的互动提供了基础。因此，常识推理成为了自然语言理解领域的重要任务。</li>
<li><strong>预训练模型的局限性</strong>：尽管大规模的预训练模型在语言理解方面非常有效，但它们缺乏明确处理知识和常识的模块。而结构化数据，如知识图谱，比非结构化文本更有效地表示常识。</li>
<li><strong>知识图谱与语言模型的结合</strong>：多种方法已经尝试将语言模型与各种形式的知识图谱结合起来进行常识推理。这些方法结合了语言建模和结构化知识信息的优点，提高了常识推理和问题回答的性能。</li>
<li><strong>知识图谱的局限性</strong>：尽管知识图谱可以在概念之间编码拓扑信息，但它缺乏丰富的上下文信息。例如，知识图谱可能会表示实体“Mona Lisa”与其他实体的关系，但只靠这些关系信息，很难推断它是一幅画。</li>
<li><strong>引入外部描述性知识</strong>：为了提供知识图谱中每个概念的全景视图，包括其邻近的概念、与它们的关系以及对其的明确描述，作者提议引入外部的描述性知识。例如，从Wiktionary中获取“Mona Lisa”的定义。</li>
</ul>
<h4 id="方法-1"><a href="#方法-1" class="headerlink" title="方法:"></a>方法:</h4><ol>
<li><p><strong>问题定义</strong>：</p>
<ul>
<li>本文关注的是一个QA任务，即给定一个常识问题 q，从若干选项 c1, …, cn 中选择正确答案。</li>
<li>在大多数情况下，问题本身不直接包含答案的提及。因此，可以使用外部知识源来提供额外的信息。</li>
</ul>
</li>
<li><p><strong>知识图谱</strong>：</p>
<ul>
<li>作者选择使用ConceptNet作为他们的知识图谱G，这是一个包含超过800万实体作为节点和超过2100万关系作为边的图。</li>
<li>在这里，作者将两个相邻节点及其之间的边称为“三元组”。其中u是主体，p是关系，v是对象。</li>
</ul>
</li>
<li><p><strong>如何选择关系三元组</strong>：</p>
<ul>
<li>假设问题提及了一个实体 eq，而选项中包含了一个实体 ec。</li>
<li>作者使用KCR方法来选择关系三元组。<ul>
<li>如果在知识图谱G中从eq到ec有直接的边r，那么就选择这个三元组 (eq, r, ec)。</li>
<li>如果没有这样的直接边，那么我们会检索包含实体ec的所有N个三元组。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>三元组的评分机制</strong>：</p>
<ul>
<li>为了决定哪个三元组最相关，每个三元组j都会被分配一个分数 sj。</li>
<li>三元组的分数是由ConceptNet提供的三元组权重 wj 和关系类型权重 trj 的乘积决定的。</li>
<li>在这里，rj 是三元组j的关系类型，而 Nrj 是检索到的三元组中具有关系类型 rj 的三元组数量。</li>
<li>这个过程倾向于选择更稀有的关系类型。</li>
<li>最终，权重最高的三元组被选为最相关的三元组。</li>
</ul>
</li>
</ol>
<p>简而言之，这个方法旨在为给定的问题和选择找到最相关的知识图谱中的三元组。如果问题和选项中的实体在知识图谱中有直接的关系，则直接选择该三元组；否则，会从包含选项实体的所有三元组中选择权重最高的那个。</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081554336.png" alt="image-20231007144448698"></p>
<p>图片解释</p>
<ol>
<li><strong>问题与选项</strong>：<ul>
<li>Q: Where would you find magazines alongside many other printed works?</li>
<li>C: Bookstore 这是一个典型的问答任务，其中问题询问你在哪里可以找到杂志和其他印刷品，答案或选择是“书店”。</li>
</ul>
</li>
<li><strong>实体描述</strong>：<ul>
<li>为了提供更多的上下文，模型使用了问题和选项中涉及的实体的描述。</li>
<li>Ques_ent: magazines</li>
<li>Choice_ent: bookstore</li>
<li>bookstore 的描述是：A store where books are bought and sold.</li>
<li>magazines 的描述是：A non-academic periodical publication.</li>
</ul>
</li>
<li><strong>知识图谱的三元组</strong>：<ul>
<li>为了增强模型的预测能力，从知识图谱（如ConceptNet）中提取了一个与问题和答案相关的三元组。</li>
<li>Triple: Rel: magazines, AtLocation, Bookstore 这意味着“杂志”在“书店”中的关系是“AtLocation”，这提供了额外的上下文支持书店是正确的答案。</li>
</ul>
</li>
<li><strong>模型输入</strong>：<ul>
<li>所有这些信息都被组织起来，形成一个复杂的输入，供ALBERT处理。</li>
<li>输入包括：问题、选择、实体名、描述文本和三元组。</li>
</ul>
</li>
<li><strong>模型输出</strong>：<ul>
<li>ALBERT处理输入后，其输出经过一个基于注意力的加权和以及一个softmax层进行处理，以产生一个预测。</li>
<li>基于注意力的加权和可以帮助模型确定输入中哪些部分对于给定问题最为重要。</li>
<li>softmax层将模型的输出转化为一个概率分布，表示每个选项被选中为正确答案的概率。</li>
</ul>
</li>
</ol>
<p>上下文信息如何获取</p>
<ol>
<li><strong>使用在线字典获取上下文</strong>:<ul>
<li>为了提供这些实体的上下文描述，作者利用了大规模的在线字典，特别是Wiktionary的数据转储。这个转储包含了999,614个概念的定义。</li>
<li>对于每个概念，选择其在Wiktionary中的第一个定义条目作为描述。</li>
</ul>
</li>
<li><strong>寻找匹配的描述</strong>:<ul>
<li>为了为每个问题&#x2F;选择中的概念找到最合适的描述，作者使用了以下三种方法按顺序查找： i) 原始形式。 ii) 使用Spacy (Honnibal and Montani, 2017) 的词元形式。 iii) 基词（最后一个词）。</li>
<li>举例来说，”taking notes”这个概念在Wiktionary的原始形式中并不存在，但它的词元形式”take notes”是存在的。从中，我们得到了描述文本：”To make a record of what one hears or observes for future reference”。</li>
</ul>
</li>
<li><strong>整合所有描述信息</strong>:<ul>
<li>找到所有实体的描述后，将问题和选项概念的描述分别表示为dq和dc。</li>
<li>然后，将问题、选择、描述和三元组（来自第3.1节）以特定的格式输入到ALBERT模型中，格式如下：[CLS] q c [SEP] eq: dq [SEP] ec: dc [SEP] triple。</li>
</ul>
</li>
</ol>
<ul>
<li>a. 理论背景:<ul>
<li>本文介绍了常识问题回答任务，并强调了基于观察和知识推理的重要性。它提到了语言模型在处理知识和常识方面的局限性，以及知识图谱在表示结构化数据方面的有效性。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>本文提出了DEKCOR模型用于常识问题回答。该模型利用ALBERT语言模型，并融合来自知识图谱的上下文信息。模型使用来自Wiktionary的实体描述作为ALBERT的额外输入。ALBERT模型的输入格式包括问题、选项、描述和三元组。使用基于注意力的加权和和softmax层生成问题和选项之间的相关性分数。模型的架构和输入构造如图1所示。</li>
</ul>
</li>
</ul>
<p>ALBERT模型上使用基于注意力的加权求和和softmax层来为问题-选项对生成相关性得分。</p>
<ol>
<li><p><strong>ALBERT的输出表示</strong>:</p>
<ul>
<li>假设ALBERT的输出表示为 ( ($x_0, …, x_m$) ) ，其中每个 ( $x_i $) 是一个 ( d ) 维的向量。</li>
</ul>
</li>
<li><p><strong>基于注意力的加权求和</strong>:</p>
<ul>
<li>使用参数向量 ( u ) 计算每个表示 ( x_i ) 的权重（或注意力得分） ( q_i ) 。</li>
<li>具体计算为： ( $q_i &#x3D; u^T x_i$ ) （方程(2)）。</li>
<li>然后使用softmax函数计算每个 ( q_i ) 的归一化权重（或注意力权重） ( $\alpha_i$ )。</li>
<li>具体计算为： ($ \alpha_i &#x3D; softmax(q_i)$ ) （方程(3)）。</li>
<li>使用这些归一化的权重计算表示 ( x_i ) 的加权求和 ( v )。</li>
<li>具体计算为： ($ v &#x3D; \sum_{i&#x3D;0}^{m} \alpha_i x_i $) （方程(4)）。</li>
</ul>
</li>
<li><p><strong>计算问题和选项之间的相关性得分</strong>:</p>
<ul>
<li>使用参数向量 ( b ) 和前面计算的加权向量 ( v ) ，计算问题和选项之间的相关性得分 ( s )。</li>
<li>具体计算为： ($ s &#x3D; softmax(v^T b) $) 。这里的softmax是对所有选项计算，用于交叉熵损失函数。</li>
</ul>
</li>
</ol>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h3><ul>
<li><p><strong>数据集</strong>:</p>
<ul>
<li><strong>CommonsenseQA</strong>: 基于ConceptNet的实体和关系来创建问题。</li>
<li><strong>OpenBookQA</strong>: 探索来自一本包含1,326个事实的书的基础科学知识。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081554233.png" alt="image-20231007143735578"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081554561.png" alt="image-20231007143827049"></p>
</li>
<li><p>a. 详细的实验设置:</p>
<ul>
<li>本文使用了ConceptNet作为知识图谱，并描述了从ConceptNet中检索相关三元组的过程。根据直接边缘或检索分数选择关系三元组。</li>
</ul>
</li>
<li><p>b. 详细的实验结果:</p>
<ul>
<li>本文使用DEKCOR模型进行常识问题回答实验。模型的输入包括问题、选项、描述和三元组。通过基于注意力的加权和和softmax层生成问题和选项之间的相关性分数。模型的架构和输入构造如图1所示。</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/10/08/10%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8chatdb/" data-id="clnh66623000070u78ona2kzc" data-title="10月第一周chatdb" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-9月第二周周报" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/08/9%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%91%A8%E6%8A%A5/" class="article-date">
  <time class="dt-published" datetime="2023-10-08T07:32:40.000Z" itemprop="datePublished">2023-10-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/08/9%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%91%A8%E6%8A%A5/">9月第二周周报</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="大模型结合数据库系列"><a href="#大模型结合数据库系列" class="headerlink" title="大模型结合数据库系列"></a>大模型结合数据库系列</h2><h2 id="开源项目：DB-GPT-一个隐私的数据库垂直领域本地化大模型框架"><a href="#开源项目：DB-GPT-一个隐私的数据库垂直领域本地化大模型框架" class="headerlink" title="开源项目：DB-GPT: 一个隐私的数据库垂直领域本地化大模型框架"></a>开源项目：DB-GPT: 一个隐私的数据库垂直领域本地化大模型框架</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081546732.png" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081546313.png" alt="img"></p>
<p><strong>DB-GPT：使用私有LLM技术革命化数据库交互</strong></p>
<p><strong>1. 概述</strong>： DB-GPT旨在使用私有的大型语言模型（LLM）技术来革命化我们与数据库的交互方式。该项目旨在确保没有数据泄露的风险，保证数据100%的隐私和安全性。</p>
<p><strong>2. 主要特点</strong>：</p>
<ul>
<li><strong>SQL能力</strong>：该项目提供SQL语言能力、SQL生成和SQL诊断。</li>
<li><strong>私有领域问答</strong>：它提供私有领域的问答和数据处理功能。</li>
<li><strong>知识管理</strong>：支持各种文档格式，如txt、pdf、md、html、doc、ppt和url。</li>
<li><strong>聊天界面</strong>：包括ChatDB、ChatExcel和ChatDashboard。</li>
<li><strong>支持多种LLM</strong>：系统支持各种大型语言模型，包括InternLM、Baichuan2、Vicuna-v1.5、llama-2等。</li>
<li>??<strong>统一的向量存储&#x2F;索引</strong>：允许存储和索引知识库。</li>
<li><strong>插件和扩展</strong>：设计原生支持Auto-GPT插件和其他扩展。</li>
<li><strong>架构</strong>：DB-GPT的核心能力包括知识库能力、大型模型管理、统一的数据向量存储和索引、连接模块、代理和插件、提示生成和优化以及多平台产品界面。</li>
</ul>
<p><strong>3. 原理和架构</strong>： DB-GPT的核心原理是利用大型语言模型（LLM）与数据库进行交互。这些LLM能够理解和生成SQL查询，从而使非技术用户能够通过自然语言与数据库进行交互。此外，该框架还提供了知识管理和私有领域问答功能，使得用户可以在保持数据私密性的同时，与其数据和环境进行交互。</p>
<p>为了实现这些功能，DB-GPT采用了多种技术，包括知识库存储、大型模型管理、统一的数据向量存储和索引、以及连接模块。这些技术共同工作，使得DB-GPT能够提供高效、安全和用户友好的数据库交互体验。</p>
<h1 id="论文1：KagNet-Knowledge-Aware-Graph-Networks-for-Commonsense-Reasoning-KagNet-知识图谱网络用于常识推理-以前的论文粗略看下"><a href="#论文1：KagNet-Knowledge-Aware-Graph-Networks-for-Commonsense-Reasoning-KagNet-知识图谱网络用于常识推理-以前的论文粗略看下" class="headerlink" title="论文1：KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning (KagNet: 知识图谱网络用于常识推理)以前的论文粗略看下"></a>论文1：KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning (KagNet: 知识图谱网络用于常识推理)以前的论文粗略看下</h1><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning (KagNet: 知识感知图网络用于常识推理)</li>
<li>Authors: Bill Yuchen Lin, Xinyue Chen, Jamin Chen, Xiang Ren</li>
<li>Affiliation: University of Southern California (美国南加州大学)</li>
<li>Keywords: commonsense reasoning, knowledge-aware graph networks, explainable inferences, graph convolutional networks, LSTMs</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.02151">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/INK-USC/KagNet">GitHub Code</a></li>
</ul>
<h1 id="论文简要"><a href="#论文简要" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>本文提出了一种用于常识推理的文本推理框架，通过有效利用外部结构化的常识知识图来进行可解释的推理。该框架首先将问题-答案对从语义空间映射到基于知识的符号空间作为模式图，然后使用名为KagNet的知识感知图网络模块对模式图进行表示，并最终使用图表示对答案进行评分。通过在CommonsenseQA数据集上进行实验，使用ConceptNet作为唯一的外部资源，我们的模型在常识推理方面取得了最先进的性能。</li>
</ul>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 常识推理旨在赋予机器人类的能力，使其能够对日常生活中的普通情况进行推断。然而，现有的方法在推理能力、透明度和可解释性方面仍存在一定的局限性。</li>
<li>过去方案: 过去的方法主要依赖于对大型预训练语言模型进行微调，如GPT和BERT，但是这些方法在性能和透明度方面与人类的表现仍存在较大差距。</li>
<li>论文的Motivation: 为了提高常识推理的性能和可解释性，本文提出了一种知识感知的推理框架，该框架利用外部的常识知识图来增强推理能力，并通过图表示提供可解释的结果。通过实验验证，该框架在CommonsenseQA数据集上取得了最先进的性能，为未来的常识推理研究提供了有价值的工具和方法。</li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081546639.png" alt="image-20230923112135319" style="zoom:150%;" />

<ol>
<li><strong>问答系统的输入</strong>：用户提供一个问题 (Question) 和一组候选答案 (Answer)。</li>
<li><strong>概念识别</strong>：系统对输入的问题和答案进行处理，通过Concept Recognition来 识别和提取关键概念。</li>
<li><strong>语言编码器</strong>：使用例如BERT这样的LanguageEncoder对问题和答案进行编码，将其转化为向量形式，即Statement Vector。</li>
<li><strong>知识图谱构建</strong>：通过Graph Constructionvia Path Finding在大型知识图谱中找到与识别概念相关的路径，从而为问题和答案构建一个Schema Graph。</li>
<li><strong>深度学习模型</strong>：使用GCN-LSTM-HPA这样的复合神经网络结构对Schema Graph进行处理。其中，GCN用于处理图结构数据，LSTM用于处理序列数据，而HPA可能是某种注意力机制或其他方法。</li>
<li><strong>评分机制</strong>：处理后的数据通过MLP (多层感知器) 获得一个Plausibility score，该分数代表了各个答案的可能性或合理性。</li>
<li><strong>输出</strong>：系统选择具有最高Plausibility score的答案作为最终答案，并返回给用户。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081546629.png" alt="image-20230923140234305"></p>
<ul>
<li>a. 理论背景:<ul>
<li>本文提出了一种名为KagNet的文本推理框架，用于回答常识问题。它利用外部知识图谱进行可解释的推理。该框架将问题-答案对表示为模式图，并使用名为KAGNET的知识感知图网络模块对这些图进行建模。该模型在CommonsenseQA数据集上取得了最先进的性能。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>模式图接地：通过识别概念、构建模式图和路径修剪，将问题-答案对接地到知识图谱上。</li>
<li>知识感知图网络：使用图卷积网络对模式图进行编码，以实现知识感知的推理。</li>
</ul>
</li>
</ul>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081546111.png" alt="image-20230923135408753"></p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>作者使用CommonsenseQA数据集进行实验，该数据集包含需要常识推理的自然语言问题。</li>
<li>作者使用BERT-LARGE作为句子编码器，并在KagNet模型中使用两个GCN层和一个双向LSTM层。</li>
<li>知识图嵌入使用TransE进行预训练，并使用GloVe嵌入进行初始化。</li>
<li>作者使用路径得分阈值0.15对知识图中的路径进行修剪。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>作者将KagNet模型与基线方法在官方划分和自行划分的数据集上进行比较。</li>
<li>在官方划分上，基于BERT和GPT的预训练方法表现优于其他基线方法。</li>
<li>KagNet在测试数据上取得了最先进的性能，准确率提高了2.2%。</li>
<li>在自行划分上，KagNet在所有设置中都优于微调方法。</li>
<li>KagNet在准确率方面也大幅优于其他知识感知的基线方法。</li>
</ul>
</li>
</ul>
<h2 id="论文十问"><a href="#论文十问" class="headerlink" title="论文十问"></a>论文十问</h2><ol>
<li><p><strong>论文试图解决什么问题？</strong></p>
<p>论文主要试图解决零样本学习(ZSL)中的问题。在ZSL中，系统需要对在训练阶段未见过的类别进行分类。传统的深度学习方法在这种情境下表现不佳，因为它们依赖于大量的标记数据。这篇文章尝试通过利用知识图谱来为这些未见过的类别提供先验知识，从而改善ZSL的性能。</p>
</li>
<li><p><strong>这是否是一个新的问题？</strong></p>
<p>不，零样本学习(ZSL)并不是一个新问题。但是，如何有效地利用知识图谱来改进ZSL是一个相对较新并且具有挑战性的研究方向。</p>
</li>
<li><p><strong>这篇文章要验证一个什么科学假设？</strong></p>
<p>文章的核心假设是：知识图谱可以为零样本学习提供有价值的先验知识，并且通过将知识图谱和深度神经网络结合，可以提高ZSL的性能。</p>
</li>
<li><p><strong>有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</strong></p>
<p>论文引用了多篇关于零样本学习、知识图谱和图卷积网络的研究。这些研究可以大致归类为：</p>
<ul>
<li>零样本学习的传统方法。</li>
<li>利用知识图谱进行深度学习的方法。</li>
<li>图卷积网络相关研究。</li>
</ul>
<p>论文中没有特别指出哪些研究员是该领域的关键人物，但通过查看参考文献，我们可以得知哪些作者的工作被频繁引用，从而推断出领域内的主要研究员。</p>
</li>
<li><p><strong>论文中提到的解决方案之关键是什么？</strong></p>
<p>KGNet是一个知识引导的网络结构，它的核心思想是将知识图谱的结构性信息和深度神经网络的表示能力结合起来。具体来说：</p>
<ul>
<li><strong>知识图谱的嵌入</strong>：KGNet使用图卷积网络(GCN)对知识图谱进行嵌入，从而捕获到类别之间的关系。这种关系有助于为未见过的类别提供上下文信息。</li>
<li><strong>特征融合</strong>：KGNet不仅仅依赖知识图谱的信息，它还将来自图谱的特征与从传统CNN提取的图像特征进行融合。这确保了模型在处理视觉任务时仍然具有强大的表示能力。</li>
</ul>
</li>
<li><p><strong>论文中的实验是如何设计的？</strong></p>
<ul>
<li><strong>基准测试</strong>：KGNet在三个公开数据集上进行了评估，这些数据集被广泛用于零样本学习的研究。通过与其他先进方法的比较，作者意图展示KGNet的优越性。</li>
<li><strong>消融实验</strong>：为了理解KGNet的每个组件对性能的贡献，作者设计了消融实验。这些实验帮助确定知识图谱的嵌入、图卷积网络以及特征融合的重要性。</li>
</ul>
</li>
<li><p><strong>用于定量评估的数据集是什么？代码有没有开源？</strong></p>
</li>
<li><p><strong>论文中的实验及结果有没有很好地支持需要验证的科学假设？</strong></p>
<p>是的，实验结果显示，KGNet在所有测试数据集上的性能都优于其他先进的ZSL方法。这支持了知识图谱可以为ZSL提供有价值的先验知识的假设。</p>
</li>
<li><p><strong>这篇论文到底有什么贡献？</strong></p>
<ol>
<li><strong>知识引导网络 (KGNet)</strong>: 传统的深度学习模型往往需要大量的标记数据，但在零样本学习(ZSL)中，这种数据是不可用的。KGNet的创新之处在于，它利用知识图谱为未见过的类别提供先验知识，从而克服了数据不足的问题。</li>
<li><strong>结合知识图谱和深度神经网络</strong>：此前的许多研究只关注如何将知识图谱融入深度学习，但KGNet不仅仅是简单的融合。它在设计上确保了两种信息（即来自知识图谱的结构性信息和来自图像的视觉信息）都被充分利用。</li>
</ol>
<p><strong>如何结合知识图谱和深度神经网络：</strong></p>
<ol>
<li><strong>知识图谱的嵌入</strong>：KGNet首先使用图卷积网络(GCN)对知识图谱进行嵌入。这一步的目的是捕获实体（如类别）之间的关系，并将这些关系转化为连续的向量表示。这些嵌入捕获了类别之间的语义关系，这对于ZSL任务至关重要。</li>
<li><strong>特征融合</strong>：在获取了知识图谱的嵌入之后，KGNet将这些嵌入与通过CNN从图像中提取的视觉特征进行融合。这种融合确保了模型在做决策时，既考虑了视觉信息，又考虑了语义关系。</li>
<li><strong>联合训练</strong>：KGNet在训练过程中同时优化知识图谱的嵌入和图像的分类任务。这意味着，知识图谱的信息不仅被用作先验知识，而且在训练过程中与视觉信息进行了互动。</li>
</ol>
</li>
<li><p><strong>下一步呢？有什么工作可以结合知识图谱和大模型？</strong></p>
<ol>
<li><p><strong>知识增强的Transformer</strong>：</p>
<ol>
<li><strong>背景</strong>：Transformers已经在多种任务中证明了其出色的性能，但它们通常需要大量的数据。通过将知识图谱的语义信息引入Transformer，我们可能能够在数据稀缺的任务中取得更好的效果。</li>
<li><strong>方法</strong>：在Transformer的自注意力机制中，可以引入知识图谱的结构性信息，使模型能够考虑实体之间的关系。</li>
</ol>
</li>
<li><p><strong>预训练语言模型与知识图谱的结合</strong>：</p>
<ol>
<li><strong>背景</strong>：像BERT和GPT这样的预训练语言模型捕获了大量的语言知识，但它们可能缺乏明确的结构性知识。</li>
<li><strong>方法</strong>：在预训练过程中，将知识图谱的事实作为额外的训练信号，从而使模型更好地理解和使用这些事实。</li>
</ol>
</li>
<li><p><strong>零样本学习的大模型</strong>：</p>
<ol>
<li><strong>背景</strong>：尽管大模型具有强大的表示能力，但在零样本或少样本学习中仍然存在挑战。</li>
<li><strong>方法</strong>：结合KGNet的思想，使用知识图谱为大模型提供结构性的先验知识，从而改进其在这些任务中的性能。</li>
</ol>
</li>
<li><p><strong>知识图谱增强的对话系统</strong>：</p>
<ol>
<li><strong>背景</strong>：现代对话系统，特别是那些基于Transformer的模型，通常依赖大量的对话数据进行训练。</li>
<li><strong>方法</strong>：利用知识图谱为对话系统提供背景知识，使模型能够更好地理解和响应用户的问题。</li>
</ol>
</li>
<li><p><strong>结构化知识的迁移学习</strong>：</p>
<ol>
<li><strong>背景</strong>：迁移学习目的是将在一个任务上学到的知识迁移到另一个任务上。知识图谱可能为此提供有用的结构化信息。</li>
<li><strong>方法</strong>：使用知识图谱为源任务和目标任务提供桥梁，从而促进知识的迁移。</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>​    </p>
<h2 id="论文2：-Improving-Multi-hop-Knowledge-Base-Question-Answering-by-Learning-Intermediate-Supervision-Signals-通过学习中间监督信号来改进多跳知识库问答"><a href="#论文2：-Improving-Multi-hop-Knowledge-Base-Question-Answering-by-Learning-Intermediate-Supervision-Signals-通过学习中间监督信号来改进多跳知识库问答" class="headerlink" title="论文2： Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals (通过学习中间监督信号来改进多跳知识库问答)"></a>论文2： Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals (通过学习中间监督信号来改进多跳知识库问答)</h2><h1 id="Basic-Information-1"><a href="#Basic-Information-1" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals (通过学习中间监督信号来改进多跳知识库问答)</li>
<li>Authors: Gaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, Ji-Rong Wen</li>
<li>Affiliation: School of Information, Renmin University of China (中国人民大学信息学院)</li>
<li>Keywords: Knowledge Base Question Answering, Teacher-student Network, Intermediate Supervision Signals</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://doi.org/10.1145/3437963.3441753">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/RichardHGL/WSDM2021_NSM">GitHub</a></li>
</ul>
<h1 id="论文简要-1"><a href="#论文简要-1" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>通过学习中间监督信号，提出了一种新颖的师生模型方法来改进多跳知识库问答任务，其中学生网络旨在找到正确答案，而教师网络则试图学习中间监督信号以提高学生网络的推理能力。通过双向推理，教师网络可以产生更可靠的中间监督信号，从而缓解虚假推理问题。实验证明了该方法在多跳知识库问答任务上的有效性。</li>
</ul>
<h1 id="背景信息-1"><a href="#背景信息-1" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 知识库问答（KBQA）是一个具有挑战性的任务，旨在从给定的知识库中找到自然语言问题的答案。多跳知识库问答是解决复杂问题的一种方法，需要进行多次推理过程。然而，多跳KBQA算法在中间步骤缺乏监督信号，只能从最终答案中获得反馈，导致学习不稳定或无效。</li>
<li>过去方案: 以往的研究中，一些方法将多跳KBQA作为强化学习任务来解决，但这些方法要么需要专家经验，要么在中间步骤仍然缺乏有效的监督信号。</li>
<li>论文的Motivation: 为了解决多跳KBQA中缺乏中间监督信号的问题，本文提出了一种新颖的师生模型方法。学生网络旨在找到正确答案，而教师网络则试图学习中间监督信号以提高学生网络的推理能力。通过双向推理，教师网络可以产生更可靠的中间监督信号，从而缓解虚假推理问题。这是本研究的动机和创新点。</li>
</ul>
<h1 id="方法-1"><a href="#方法-1" class="headerlink" title="方法:"></a>方法:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081546155.png" alt="image-20230923133816656"></p>
<h3 id="整体流程与公式解释："><a href="#整体流程与公式解释：" class="headerlink" title="整体流程与公式解释："></a>整体流程与公式解释：</h3><ol>
<li><p><strong>问题的背景</strong>:<br>多跳知识库问答 (KBQA) 的困难之处在于通常只知道答案，但不知道中间的推理步骤。为了解决这个问题，这篇论文提出了一个教师-学生模型。</p>
</li>
<li><p><strong>学生模型 - Neural State Machine (NSM)</strong>:</p>
<ul>
<li><p><strong>指令组件</strong>:</p>
<ol>
<li><p>从问题中生成指令向量 ( i^{(k)} )，这个向量表示在第 ( k ) 步推理时，模型应该关注问题的哪一部分。</p>
<p>[<br>$i^{(k)} &#x3D; \sum_{j&#x3D;1}^{l} \alpha^{(k)}_j h_j$<br>]</p>
<p>其中，权重 (\alpha^{(k)}_j) 用来决定在第 ( k ) 步时，应该对问题的哪一部分给予多少注意力。</p>
</li>
<li><p>使用权重生成问题的表示 ( q^{(k)} )，它结合了之前的指令和问题的原始表示。</p>
<p>[<br>$q^{(k)} &#x3D; W^{(k)} [i^{(k-1)}; q] + b^{(k)}$<br>]</p>
</li>
</ol>
</li>
<li><p><strong>推理组件</strong>:</p>
<ol>
<li><p>初始化实体表示 ( e^{(0)} )。这基于与实体 ( e ) 相关的所有关系来进行。</p>
<p>[<br>$e^{(0)} &#x3D; \sigma \left( \sum_{\langle e’, r, e \rangle \in N} r \cdot W^T \right)$<br>]</p>
</li>
<li><p>通过比较指令向量 ( $i^{(k)} $) 和关系 ( r )，生成匹配向量 ( $m^{(k)}_{\langle e’, r, e \rangle}$ )。</p>
<p>[<br>$m^{(k)}_{\langle e’, r, e \rangle} &#x3D; \sigma(i^{(k)} \odot W^R r)$<br>]</p>
</li>
<li><p>更新实体表示 ( e^{(k)} )，以反映与实体 ( e ) 相关的关系。</p>
<p>[<br>$e^{(k)} &#x3D; FFN([e^{(k-1)}; \tilde{e}^{(k)}])$<br>]</p>
</li>
<li><p>计算实体分布 ( p^{(k)} )，表示在第 ( k ) 步推理后，每个实体作为答案的可能性。</p>
<p>[<br>$p^{(k)} &#x3D; softmax(E^{(k)T} w)$<br>]</p>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>教师模型</strong>:</p>
<ul>
<li><p>教师模型的目标是学习在中间步骤的可靠实体分布。</p>
</li>
<li><p>为了达到这个目标，模型使用了前向和后向推理：</p>
<ol>
<li><p><strong>推理损失</strong>:</p>
<p>[<br>$L_f &#x3D; DKL(p^{(n)}_f, p^*_f) \quad \text{和} \quad L_b &#x3D; DKL(p^{(n)}_b, p^*_b)$<br>]</p>
<p>这些损失度量了教师模型在正向和反向推理中预测的实体分布与真实答案之间的差异。</p>
</li>
<li><p><strong>一致性损失</strong>:</p>
<p>[<br>$L_c &#x3D; \sum_{k&#x3D;1}^{n-1} DJS(p^{(k)}_f, p^{(n-k)}_b)$<br>]</p>
<p>这个损失度量了正向和反向推理过程中中间实体分布之间的一致性。</p>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>训练过程</strong>:</p>
<ul>
<li><p><strong>教师网络的训练</strong>: 教师模型的整体损失为 ( $L_t &#x3D; L_f + \lambda_b L_b + \lambda_c L_c$ )。</p>
</li>
<li><p><strong>学生网络的训练</strong>:</p>
<ol>
<li>使用教师模型在中间步骤生成的实体分布作为监督信号 ($ p^{(k)}_t &#x3D; \frac{1}{2}(p^{(k)}_f + p^{(n-k)}_b) $)。</li>
<li>学生模型的损失为 ($ L_s &#x3D; L_1 + \lambda L_2$ )，其中 ($ L_1 $) 和 ($L_2 $) 分别度量学生模型的最终和中间实体分布与教师模型之间的差异。</li>
</ol>
</li>
</ul>
</li>
</ol>
<ul>
<li>a. 理论背景:<ul>
<li>本文介绍了多跳知识库问答（KBQA）任务，该任务旨在从问题中提到的实体到知识库（KB）中多跳的答案。多跳KBQA中缺乏中间步骤的监督信号被认为是一个主要挑战，因为现有的算法只能从最终答案中获得反馈，导致学习不稳定或无效。为了解决这个挑战，本文提出了一种新颖的师生方法用于多跳KBQA。学生网络负责找到正确答案，而教师网络学习中间监督信号以提高学生网络的推理能力。教师网络利用正向和反向推理来增强中间实体分布的学习，产生更可靠的监督信号，缓解虚假推理的问题。通过在三个基准数据集上进行大量实验，证明了所提出方法的有效性。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>本文提出的多跳KBQA任务的方法基于师生框架。学生网络使用神经状态机（NSM）模型实现，通过将知识库视为图形来适应多跳KBQA任务。NSM模型在多跳推理过程中维护逐渐学习的实体分布。为了在中间推理步骤提供监督信号，通过修改NSM的架构，开发了一个教师网络来包含双向推理机制。教师网络在中间推理步骤学习更可靠的实体分布，这些分布被用作学生网络的监督信号。</li>
</ul>
</li>
</ul>
<h1 id="结果-1"><a href="#结果-1" class="headerlink" title="结果:"></a>结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081547746.png" alt="image-20230923135348128"></p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>实验在三个基准数据集上进行：MetaQA、WebQuestionsSP和Complex WebQuestions 1.1。MetaQA数据集包含电影领域的单跳和多跳问题，最多需要3跳推理。WebQuestionsSP数据集使用Freebase作为知识库，需要最多2跳推理。Complex WebQuestions 1.1数据集是WebQuestionsSP的扩展，需要最多4跳推理。评估采用了GraftNet中的训练&#x2F;开发&#x2F;测试划分。表1提供了子图中实体的平均数量和至少一个答案的覆盖率。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>实验结果表明，所提出的模型在多跳KBQA任务的有效性方面优于先前的方法。模型在三个基准数据集上进行了评估。在CWQ数据集上，模型在推理步骤为4时表现最佳，而在其他数据集上，推理步骤为3被认为是最佳的。教师网络在找到中间实体方面优于学生网络，尽管在第二跳上的性能稍差。一次性评估还表明，所提出的方法运行良好，并且相对于基本的NSM模型有了显著的改进。</li>
</ul>
</li>
</ul>
<ol>
<li><p><strong>论文题目是什么？</strong></p>
<ul>
<li>“Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals”</li>
</ul>
</li>
<li><p><strong>论文试图解决什么问题？</strong></p>
<ul>
<li>论文主要解决多跳知识库问题回答任务中的挑战，特别是当问题需要从多个事实或实体中获取答案时。文章提出通过学习中间的监督信号来增强模型的性能。</li>
</ul>
</li>
<li><p><strong>这是否是一个新的问题？</strong></p>
<ul>
<li>不完全是。知识库问题回答(KB-QA)已经被研究了很长时间，但多跳KB-QA，尤其是如何有效地学习中间的监督信号，仍然是一个相对较新和具有挑战性的问题。</li>
</ul>
</li>
<li><p><strong>这篇文章要验证一个什么科学假设？</strong></p>
<ul>
<li>文章的核心假设是：通过学习中间实体的监督信号，可以显著提高多跳知识库问题回答的性能。</li>
</ul>
</li>
<li><p><strong>有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</strong></p>
<ul>
<li>文章引用了多项关于知识库问题回答、多跳推理和监督学习的相关研究。这些研究可以归类为：<ul>
<li>知识库问题回答技术。</li>
<li>多跳推理方法。</li>
<li>中间或间接监督的学习方法。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>论文中提到的解决方案之关键是什么？</strong></p>
<p>论文的核心解决方案是提高多跳知识库问题回答的性能，特别是当问题需要从多个事实或实体中获取答案时。为了达到这一目标，文章提出了一种新的学习策略，该策略旨在自动挖掘训练数据中的中间监督信号。以下是这一解决方案的关键组成部分：</p>
<ol>
<li><strong>中间监督信号的自动挖掘</strong>:<ul>
<li>传统的多跳KB-QA方法通常只关注最终的答案，而不考虑中间的推理步骤。这篇论文的核心思想是，通过学习中间的实体和关系，模型可以更好地理解如何进行多步推理。</li>
<li>为了实现这一目标，作者提出了一种策略，该策略可以自动从训练数据中挖掘中间的监督信号，这些信号可以指导模型如何进行推理。</li>
</ul>
</li>
<li><strong>推理路径的建模</strong>:<ul>
<li>除了最终答案外，模型还被训练为预测从问题到答案的完整推理路径。这意味着模型不仅要知道答案是什么，还要知道如何得到这个答案。</li>
<li>通过这种方式，模型可以学习更为复杂和精细的推理策略，从而在多跳KB-QA任务中取得更好的性能。</li>
</ul>
</li>
<li><strong>多任务学习</strong>:<ul>
<li>模型被设计为同时学习多个任务：预测最终答案、预测中间的实体以及预测推理路径。</li>
<li>这种多任务学习策略确保了模型可以从多个角度理解问题，从而获得更为丰富和准确的答案。</li>
</ul>
</li>
<li><strong>利用现有的训练数据</strong>:<ul>
<li>一个重要的优点是，提出的策略不需要额外的标记数据。所有的中间监督信号都是从现有的训练数据中自动挖掘的，这意味着这种方法可以轻松地扩展到其他数据集或应用。</li>
</ul>
</li>
</ol>
</li>
<li><p><strong>论文中的实验是如何设计的？</strong></p>
<ul>
<li><ul>
<li><strong>实验设置</strong>:<ul>
<li><strong>数据集</strong>：论文选择了MetaQA和WebQuestionsSP两个数据集进行实验。这两个数据集都是为多跳知识库问题回答设计的，因此非常适合评估所提方法的性能。</li>
<li><strong>评估指标</strong>：使用准确率作为主要的评估指标，以衡量模型预测答案的准确性。</li>
</ul>
</li>
<li><strong>基线模型</strong>:<ul>
<li>为了证明所提方法的有效性，作者选择了多个先进的多跳KB-QA方法作为基线模型，包括MINERVA、CONV-KB和S-CONV-KB等。</li>
<li>这些基线模型有助于展示提出的方法在多跳KB-QA任务上的相对性能。</li>
</ul>
</li>
<li><strong>消融实验</strong>:<ul>
<li>为了验证所提方法的每个组件的有效性，作者进行了一系列消融实验。</li>
<li>通过移除模型的某些部分（如中间监督信号或推理路径建模），并观察性能如何变化，可以确定每个组件对总体性能的贡献。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>用于定量评估的数据集是什么？代码有没有开源？</strong></p>
<ul>
<li>使用的数据集包括MetaQA和WebQuestionsSP。论文中没有明确提到代码是否开源。</li>
</ul>
</li>
<li><p><strong>论文中的实验及结果有没有很好地支持需要验证的科学假设？</strong></p>
<ul>
<li>是的，实验结果显示，通过学习中间的监督信号，所提方法在多个数据集上均超过了其他先进的KB-QA方法，从而验证了文章的假设。</li>
</ul>
</li>
<li><p><strong>这篇论文与大模型结合有什么可行的研究方向？</strong></p>
</li>
</ol>
<p>​	</p>
<h3 id="1-问题：融合预训练模型的表示学习与本文的多跳推理"><a href="#1-问题：融合预训练模型的表示学习与本文的多跳推理" class="headerlink" title="1. 问题：融合预训练模型的表示学习与本文的多跳推理"></a>1. 问题：融合预训练模型的表示学习与本文的多跳推理</h3><p><strong>具体方法</strong>:</p>
<ul>
<li><strong>表示增强</strong>: 使用大型模型（如GPT-4）对问答对进行编码，然后结合论文中的方法进行多跳推理。</li>
<li><strong>知识图谱嵌入</strong>: 利用大型模型预训练的知识，对知识图谱进行嵌入学习，并与论文方法结合，提高问答效果。</li>
</ul>
<h3 id="2-问题：如何利用大模型的生成能力增强多跳问答"><a href="#2-问题：如何利用大模型的生成能力增强多跳问答" class="headerlink" title="2. 问题：如何利用大模型的生成能力增强多跳问答"></a>2. 问题：如何利用大模型的生成能力增强多跳问答</h3><p><strong>具体方法</strong>:</p>
<ul>
<li><strong>答案生成与验证</strong>: 使用大模型生成答案，并利用论文中的方法进行答案验证和筛选，确保答案的准确性。</li>
<li><strong>中间推理路径生成</strong>: 除了最终答案，还可以使用大模型生成中间的推理路径，与论文方法结合，提供更加详尽的解释。</li>
</ul>
<h3 id="3-问题：如何结合大模型和知识图谱进行更深层次的语义推理"><a href="#3-问题：如何结合大模型和知识图谱进行更深层次的语义推理" class="headerlink" title="3. 问题：如何结合大模型和知识图谱进行更深层次的语义推理"></a>3. 问题：如何结合大模型和知识图谱进行更深层次的语义推理</h3><p><strong>具体方法</strong>:</p>
<ul>
<li><strong>语义角色标注</strong>: 利用大模型的语义理解能力，对问题进行语义角色标注，然后结合论文中的方法，进行深层次的语义推理。</li>
<li><strong>关系预测与验证</strong>: 使用大模型预测可能的关系，然后通过论文中的方法进行验证，增强多跳推理的准确性。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/10/08/9%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%91%A8%E6%8A%A5/" data-id="clnh6662b000270u70xgbfv2c" data-title="9月第二周周报" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dbgpt-kagnet/" rel="tag">dbgpt kagnet</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-9月第一周位置插值" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/08/9%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E4%BD%8D%E7%BD%AE%E6%8F%92%E5%80%BC/" class="article-date">
  <time class="dt-published" datetime="2023-10-08T07:31:32.000Z" itemprop="datePublished">2023-10-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/10/08/9%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E4%BD%8D%E7%BD%AE%E6%8F%92%E5%80%BC/">9月第一周</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="论文1-：EXTENDING-CONTEXT-WINDOW-OF-LARGE-LAN-GUAGE-MODELS-VIA-POSITION-INTERPOLATION"><a href="#论文1-：EXTENDING-CONTEXT-WINDOW-OF-LARGE-LAN-GUAGE-MODELS-VIA-POSITION-INTERPOLATION" class="headerlink" title="论文1 ：EXTENDING CONTEXT WINDOW OF LARGE LAN-GUAGE MODELS VIA POSITION INTERPOLATION"></a>论文1 ：EXTENDING CONTEXT WINDOW OF LARGE LAN-GUAGE MODELS VIA POSITION INTERPOLATION</h2><blockquote>
<ol>
<li><strong>标题 (Title)</strong>: EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION</li>
<li><strong>中文标题 (Chinese Title)</strong>: 通过位置插值扩展大型语言模型的上下文窗口</li>
<li><strong>时间 (Date)</strong>: 28 Jun 2023</li>
<li><strong>作者 (Authors)</strong>: Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian</li>
<li><strong>机构 (Institution)</strong>: Meta Platforms Inc.</li>
<li><strong>研究主题 (Research Theme)</strong>: 扩展大型语言模型的上下文窗口，并提出一个名为“位置插值”的方法。</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.15595v2">Paper</a>, [GitHub: None]</li>
</ol>
</blockquote>
<h3 id="论文简要"><a href="#论文简要" class="headerlink" title="论文简要 :"></a>论文简要 :</h3><ul>
<li>本研究提出了一种名为Position Interpolation (PI)的方法，通过线性插值将RoPE-based预训练LLMs（如LLaMA模型）的上下文窗口大小扩展到32768，只需进行最小的微调（1000步），同时在需要长上下文的各种任务上展现出强大的实证结果，包括密钥检索、语言建模和从LLaMA 7B到65B的长文档摘要。通过Position Interpolation扩展的模型在其原始上下文窗口内相对保持较好的质量。该方法通过线性缩放输入位置索引以匹配原始上下文窗口大小，而不是超出训练的上下文长度，这可能导致完全破坏自注意机制的灾难性高注意力分数。我们的理论研究表明，插值的上限至少比外推的上限小约600倍，进一步证明了其稳定性。通过Position Interpolation扩展的模型保留其原始架构，并可以重用大部分现有的优化和基础设施。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081543130.png" alt="image-20230914151507583"></p>
<blockquote>
<p>图片理解：</p>
<ol>
<li><strong>左上角</strong>：这显示了LLM模型的“正常使用”。这里，输入位置指数（蓝色点）都在模型预先训练的范围内，也就是说，它们都在0到2048的范围内。</li>
<li><strong>右上角</strong>：这显示了“长度外推”。这意味着模型现在尝试处理超出其预先训练的范围的位置，即4096。这些新位置（红色点）是模型在预训练阶段未见过的。</li>
<li><strong>左下角</strong>：这是“位置插值”的关键。为了处理更多的位置，我们不是直接外推位置（这可能会导致问题），而是缩小位置指数。这里，我们将位置指数从[0, 4096]的范围缩小到[0, 2048]的范围，使它们与预训练的范围相匹配。这意味着原来的0-4096位置现在被“压缩”到0-2048的范围内，因此蓝色和绿色点都落在了预训练的范围内。</li>
</ol>
<h3 id="几个探究问题："><a href="#几个探究问题：" class="headerlink" title="几个探究问题："></a>几个探究问题：</h3><p><strong>问题1</strong>: 什么是LLM模型的“正常使用”？<br>正常使用是指LLM模型在其预训练的上下文窗口长度内进行操作。例如，对于一个预先训练为2048上下文窗口长度的Llama模型，输入位置指数（蓝点）都在这个预先训练的范围内。</p>
<hr>
<p><strong>问题2</strong>: 什么是“长度外推”？<br> 长度外推是指当模型需要操作超出其预训练上下文窗口范围的位置时。例如，在上述Llama模型中，模型可能需要处理高达4096的位置，这些位置（红点）是在预训练阶段未见过的。</p>
<hr>
<p><strong>问题3</strong>: 位置插值是如何工作的？<br>位置插值的核心思想是，我们不是尝试外推位置编码到未见过的范围，而是直接缩小位置指数。例如，我们将位置指数（蓝点和绿点）从[0, 4096]范围缩小到[0, 2048]范围，强制它们落在预训练的范围内。</p>
<hr>
<p><strong>问题4</strong>: 为什么我们要使用位置插值而不是简单地外推位置？<br>外推位置编码可能会导致问题，因为模型在预训练阶段没有见过这些位置。而位置插值方法则利用了位置编码可以应用于非整数位置的事实，将输入的位置编码调整到模型在预训练阶段已经见过的范围内。</p>
</blockquote>
<h3 id="背景信息-业务优先-自研大模型"><a href="#背景信息-业务优先-自研大模型" class="headerlink" title="背景信息: (业务优先&#x2F;自研大模型)"></a>背景信息: (业务优先&#x2F;自研大模型)</h3><ul>
<li><strong>上下文窗口的挑战</strong>：大型语言模型（LLM）通常具有预设的上下文窗口大小，这在某些应用场景中限制了其能力。当模型需要处理长对话、长文档摘要或长期规划时，预设的窗口大小经常成为瓶颈。这意味着我们需要某种方法来扩展现有的LLM的上下文窗口，而不是重新训练一个新模型。</li>
<li><strong>直接的方法并不高效</strong>：一个直观的方法是微调现有的预训练Transformer，使其具有更长的上下文窗口。然而，这种方法在实践中效果并不理想，因为模型对长上下文窗口的适应性很慢。</li>
<li><strong>现有扩展方法的局限性</strong>：尽管有一些现有的技术可以进行Transformer的长度外推，但这些方法并不适用于所有的预训练LLM。很多LLM使用的位置编码方法具有弱的外推特性，这限制了它们在长上下文窗口中的应用。</li>
<li><strong>位置插值的创新</strong>：为了解决上述问题，作者提出了位置插值这一新方法。与其尝试外推位置编码（这可能会导致问题），不如调整位置指数来适应现有的模型。这种方法的关键思想是：为了容纳更多的输入标记，我们在相邻的整数位置上插入位置编码，利用位置编码可以应用于非整数位置的事实。</li>
<li><strong>实证验证</strong>：作者不仅从理论上验证了他们的方法，还进行了实证验证。结果显示，位置插值是一种高效且有效的方法，可以在非常短的时间内对模型进行微调，使其适应大大扩展的上下文窗口。</li>
</ul>
<blockquote>
<p>背景技术之一RoPE:</p>
<ol>
<li><p><strong>背景</strong>：Transformer模型需要注入显式的位置信息来表示输入的顺序，这通常是通过位置编码来实现的。</p>
</li>
<li><p><strong>RoPE的定义</strong>：RoPE是LLaMA模型使用的位置编码。对于给定的位置索引 ( m ) 和嵌入向量 ( x )，RoPE定义了一个向量值的复数函数 ( f(x, m) )，该函数通过复数乘法来结合位置和内容信息。其中，( i ) 是虚数单位，而 ( $\theta_j$ ) 定义了一个特定的频率，它随着嵌入的维度变化。</p>
</li>
<li><p><strong>自注意力分数</strong>：使用RoPE，自注意力分数 ( a(m, n) ) 只依赖于相对位置 ( m - n )。这是通过三角函数来实现的。具体来说，查询q向量 ( q ) 和key向量 ( k ) 的RoPE编码通过点积来计算它们之间的相似度。然后，这个相似度又被表示为一个只依赖于相对位置的函数 ( a(m - n) )。</p>
</li>
<li><p><strong>RoPE的应用</strong>：在每一层，RoPE都应用于查询和密钥的嵌入来计算注意力分数。</p>
</li>
</ol>
<p>RoPE的关键思想是通过复数乘法结合位置和内容信息，并确保注意力机制只依赖于输入之间的相对位置。这种方法与传统的sine和cosine位置编码方法不同，因为它不是简单地将位置信息添加到内容的嵌入中，而是通过复数乘法来结合这两者。</p>
<ol>
<li><strong>RoPE的外推问题</strong>：尽管RoPE的注意力分数只依赖于相对位置（这是我们想要的），但其外推性能并不好。具体地说，当直接扩展到训练中未见过的更大的上下文窗口时，困惑度可能会急剧上升，与未经训练的模型相当。</li>
<li><strong>理想的行为</strong>：理想情况下，如果一个模型在大小为2048的上下文窗口上训练，那么它在更长的上下文窗口上仍应表现得相当好。但实际上，模型的行为是灾难性的，即使证据就在问题的位置附近。</li>
<li><strong>问题的原因</strong>：尽管根据(Su et al., 2021)的3.4.3节，注意力分数随着相对距离|m − n|的增加而减少，但从很远的距离来的内容仍然很重要。这提示我们，该文中推导的上界可能太松了。</li>
<li><strong>三角函数的基函数</strong>：如果我们把所有的三角函数都看作基函数，并认为Eqn. 2是基函数的扩展，那么问题就变得明显了。尽管在[0, 2048]的范围内函数的大小可能很小，但在该区域之外的值可能会非常大。</li>
<li><strong>原因分析</strong>：三角函数族（对于足够大的d）是一个通用的逼近器，可以拟合任意的函数。因此，总是存在一些系数（即k和q），它们在[0, 2048]范围内对应于小的函数值，但在该区域之外的值则大得多。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081543302.png" alt="image-20230914155452906"></p>
<ol>
<li><p><strong>左图 (Extrapolation)</strong>:</p>
</li>
<li><p>​	这张图展示了一个拟合的注意力分数函数（红线）。</p>
</li>
</ol>
<ul>
<li>它使用的是公式3，并且给出了一个特定的设置，（这是LLaMA 7B的设置）。</li>
<li><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081544208.png" alt="image-20230915145143066"></li>
<li>图中的点表示随机输入点，这些点是通过最小二乘法来拟合红线的。</li>
<li>结果的红线大致在[-1, 1]的范围内。</li>
</ul>
<ol start="3">
<li><strong>中图 (Problem of Extrapolation)</strong>:</li>
</ol>
<ul>
<li>这张图强调了外推的问题。</li>
<li>尽管拟合函数在[0, L]范围内似乎受到很好的约束（其中L&#x3D;2048），但在这个范围之外，它的值可能超过8000。</li>
<li>这种巨大的值会在注意力计算中引发灾难性的问题。</li>
<li>作者还强调，这里并没有特意挑选某种特定的行为：几乎从[0, L]范围内随机生成的每一组输入点得到的学习曲线都存在外推问题。</li>
</ul>
<ol start="4">
<li><strong>右图 (Advantage of Interpolation)</strong>:</li>
</ol>
<ul>
<li>这张图展示了插值的稳定性。</li>
<li>图中的曲线表示在垂直的虚线之间（即整数位置差异）是平滑且行为良好的。</li>
<li>这意味着，与外推相比，插值方法在处理超出预训练范围的位置时更为稳定。</li>
</ul>
</blockquote>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h3><blockquote>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081544506.png" alt="image-20230915153634972"></p>
<p>我们将此变换称为位置编码位置插值。在这一步中，我们在计算 RoPE 之前将位置索引从 [0, L’) 减少到 [0, L) 以匹配原始索引范围。因此，作为 RoPE 的输入，任意两个标记之间的最大相对距离从 L’ 减少到 L。由于我们在扩展前后对齐位置索引和相对距离的范围，我们减轻了由于上下文窗口扩展而导致的注意力分数计算的影响，这允许模型更容易适应。</p>
</blockquote>
<p>具体方法流程</p>
<blockquote>
<p>“位置插值” (Position Interpolation) 是一个更改传统 RoPE (Rotary Positional Embeddings) 方法的提议。为了更直观地解释这一过程，我将逐步描述其实现：</p>
<ol>
<li><p><strong>定义新的注意力分数</strong>:<br>传统的 RoPE 中的注意力分数是基于当前的位置编码来计算的。在位置插值方法中，注意力分数 ( $\tilde{a}(s)$ ) 被定义为 ($ a(Ls&#x2F;L’)$ )，其中 ( L’ ) 是更长的上下文窗口，( L ) 是原始的上下文窗口。这意味着，我们是在一个更长的上下文窗口中，通过插值将注意力分数调整到原始窗口的大小内。</p>
</li>
<li><p><strong>调整位置编码</strong>:<br>为了实现插值，我们首先需要缩放位置编码的范围。这通过将原始的位置编码 ( x ) 转化为 ( $f’(x, m) &#x3D; f(x, m \times L&#x2F;L’)$ ) 来完成。这样，当我们处理更长的上下文窗口 ( L’ ) 时，所有的位置编码都会被缩放到原始窗口 ( L ) 的范围内。</p>
</li>
<li><p><strong>线性插值</strong>:<br>为了确保插值的稳定性，提议使用线性插值方法来估算在两个已知位置之间的任何位置的注意力分数。这是通过以下公式完成的：</p>
<p>[<br>$a_{\text{linear}}(s) :&#x3D; (1 - \lambda(s))a(s_1) + \lambda(s)a(s_2)$<br>]</p>
<p>其中 ($ \lambda(s) &#x3D; \frac{s - s_1}{s_2 - s_1}$ )。这个公式基本上是一个权重的平均值，取决于 ( s ) 与 ( s_1 ) 和 ( s_2 ) 的相对距离。</p>
</li>
<li><p><strong>进一步的微调</strong>:<br>一旦实现了位置插值，可以进一步微调模型，以适应新的位置编码。这通常是通过使用如 “下一个令牌预测” 这样的任务来完成的。</p>
</li>
<li><p><strong>正则化</strong>:<br>为了进一步提高插值的稳定性和准确性，可以考虑在训练过程中对查询&#x2F;键产品应用正则化。</p>
</li>
</ol>
<p>位置插值的核心思想是，当我们处理更长的上下文窗口时，不是简单地扩展位置编码，而是将它们缩放到原始窗口的范围内。这样，我们可以确保模型在处理更长的文本时，仍然能够稳定地计算注意力分数。</p>
</blockquote>
<ul>
<li>a. 理论背景:<ul>
<li>本文介绍了扩展预训练大型语言模型（LLM）上下文窗口的挑战，并提出了一种称为位置插值（PI）的方法。该方法通过将输入位置索引进行降尺度处理，以匹配原始的上下文窗口大小，而不是超出训练过的上下文长度。这种方法可以最小化微调的情况下扩展上下文窗口，最多可达32768个标记。作者通过对密码检索、语言建模和长文档摘要等各种任务的实验，证明了PI的有效性。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>本文介绍了一种名为位置插值（PI）的方法，用于扩展预训练LLM的上下文窗口。PI不是通过外推，而是通过将位置索引进行降尺度处理，以匹配原始的上下文窗口大小。这是通过在相邻整数位置插值位置编码来实现的。作者在理论上证明了插值位置编码的上限要比外推位置编码小得多，使其更加稳定。实证上，PI被证明非常有效和高效，只需要短时间的微调，模型就能完全适应大幅扩展的上下文窗口。</li>
</ul>
</li>
</ul>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h3><ul>
<li>a. 详细的实验设置:</li>
<li><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081544198.png" alt="image-20230915155305482"><ul>
<li>作者使用了两个数据集（book corpus和cleaned Arxiv Math proof-pile dataset）来评估扩展模型和基准模型的长序列语言建模性能。对于book corpus数据集，作者使用了包含100个文档的整个测试集。对于proof-pile数据集，作者使用了至少包含32768个SentencePiece标记的128个文档的随机子样本。作者使用滑动窗口方法，以256的步长在不同的上下文窗口大小下评估困惑度。</li>
<li><strong>困惑度(perplexity)的值</strong>：每一个数字都表示了在特定上下文窗口大小下模型的困惑度。困惑度是评估语言模型性能的常用指标，值越低表示模型的性能越好。例如，对于”7B 2048 None”模型，在4096的上下文窗口大小下，它的困惑度大于10^3，这意味着其性能在这种扩展上下文中下降了。</li>
</ul>
</li>
<li>b. 详细的实验结果:</li>
<li><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081544146.png" alt="image-20230915155319124"><ul>
<li>实验结果表明，使用位置插值方法扩展的模型在更长的上下文窗口大小下取得了显著改进的困惑度。作者观察到，模型在更长的上下文窗口下的困惑度呈现出一致的改善趋势，表明模型能够有效利用更长的上下文窗口进行语言建模任务。相比之下，通过直接微调方法扩展的模型在更长的上下文窗口下的困惑度出现回归或轻微改善，表明其在利用更长的上下文窗口方面的能力有限。在一些情况下，扩展模型在原始上下文窗口上的困惑度有轻微下降，这是由于位置插值引起的位置编码范围较窄所致。作者还通过密码检索任务测量了模型的有效上下文窗口大小，并发现通过位置插值扩展的模型成功地实现了其期望的上下文窗口大小扩展目标，而通过直接微调扩展的模型的有效上下文窗口大小增加很小。实验结果表明，与基准模型相比，扩展模型在困惑度和有效上下文窗口大小方面取得了更好的性能。</li>
</ul>
</li>
</ul>
<h2 id="论文2：YaRN-Efficient-Context-Window-Extension-of-Large-Language-Models-YaRN-大型语言模型的高效上下文窗口扩展-23年9月"><a href="#论文2：YaRN-Efficient-Context-Window-Extension-of-Large-Language-Models-YaRN-大型语言模型的高效上下文窗口扩展-23年9月" class="headerlink" title="论文2：YaRN: Efficient Context Window Extension of Large Language Models (YaRN: 大型语言模型的高效上下文窗口扩展)23年9月"></a>论文2：YaRN: Efficient Context Window Extension of Large Language Models (YaRN: 大型语言模型的高效上下文窗口扩展)23年9月</h2><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: YaRN: Efficient Context Window Extension of Large Language Models (YaRN: 大型语言模型的高效上下文窗口扩展)</li>
<li>Authors: Bowen Peng, Jeffrey Quesnelle, Honglu Fan, Enrico Shippole</li>
<li>Affiliation: Nous Research (Nous研究)</li>
<li>Keywords: Rotary Position Embeddings, context window extension, language models, LLaMA, fine-tuning</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.00071v1">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/jquesnelle/yarn">GitHub Code</a></li>
<li><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081544580.png" alt="image-20230916105228832"></li>
</ul>
<h1 id="论文简要-1"><a href="#论文简要-1" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>本研究提出了一种名为YaRN的方法，通过使用RoPE（Rotary Position Embeddings）来高效地扩展大型语言模型的上下文窗口，相比之前的方法，YaRN所需的标记数量和训练步骤减少了10倍和2.5倍。使用YaRN，我们展示了LLaMA模型可以有效地利用和推广到比其原始预训练允许的上下文长度更长的范围，并且在上下文窗口扩展方面超过了先前的最新技术。此外，我们还证明了YaRN具有超越微调数据集有限上下文的能力。</li>
</ul>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: Transformer-based Large Language Models (LLMs)在自然语言处理（NLP）任务中表现出色，但它们的上下文窗口长度限制了它们的能力。RoPE（Rotary Position Embeddings）被用来编码位置信息，但这些模型无法推广到超过它们训练时的序列长度。</li>
<li>过去方案: 过去的方法如ALiBi能够进行有限的推广，但无法推广到显著超过预训练长度的序列。其他方法如Position Interpolation（PI）和\NTK-aware\ interpolation通过微调和插值来扩展上下文长度，但仍存在一些限制。</li>
<li>论文的Motivation: 鉴于现有方法的限制，本研究旨在提出一种高效的方法来扩展大型语言模型的上下文窗口，以便更好地利用和推广到更长的上下文范围，从而提高模型的性能和应用范围。</li>
</ul>
<p><strong>2.1 Rotary Position Embeddings (RoPE)</strong></p>
<ul>
<li><p>输入令牌序列为 ($w_1, w_2, \ldots, w_L$ )，它们的嵌入向量表示为 ($ x_1, \ldots, x_L \in R^{|D|}$ )，其中 ($|D|$ ) 是隐藏状态的维度。</p>
</li>
<li><p>公式(1):<br>[<br>$q_m &#x3D; f_q(x_m, m) \in R^{|L|}$<br>]<br>[<br>$k_n &#x3D; f_k(x_n, n) \in R^{|L|}$<br>]<br>这里，( q_m ) 和 ( k_n ) 分别是查询和键向量。这些向量与输入令牌的嵌入和它们的位置索引相对应。</p>
</li>
<li><p>在RoPE中，一个关键的想法是将实向量空间与复向量空间相互对应。这通过公式(2)和(3)来完成，其中实部和复部被交错地映射到复数空间。</p>
</li>
<li><p>公式(4)描述了如何从嵌入向量 ( x_m, x_n ) 生成查询和键向量：<br>[<br>$f_q(x_m, m) &#x3D; W_q x_m e^{im\theta}$<br>]<br>[<br>$f_k(x_n, n) &#x3D; W_k x_n e^{in\theta}$<br>]<br>这里，( \theta_d &#x3D; b^{-2d&#x2F;|D|} ) 并且 ( b &#x3D; 10000 )。</p>
</li>
</ul>
<p><strong>2.2 Positional Interpolation (PI)</strong></p>
<ul>
<li><p>为了解决上下文窗口长度固定的问题，有研究提出了位置插值(PI)方法。这个方法的核心是在预训练的限制内插值位置指数，并利用少量的微调来扩展上下文窗口。</p>
</li>
<li><p>公式(9)描述了如何对RoPE进行修改以实现位置插值：<br>[<br>$f’<em>{q}(x_m, m, \theta_d) &#x3D; f</em>{q}(x_m, \frac{mL’}{L}, \theta_d)$<br>]<br>[<br>$f’<em>{k}(x_n, n, \theta_d) &#x3D; f</em>{k}(x_n, \frac{nL’}{L}, \theta_d)$<br>]<br>这里，( L’ ) 是超出预训练限制的新的上下文窗口。</p>
</li>
</ul>
<p><strong>2.3 Additional Notation</strong></p>
<ul>
<li><p>这部分提供了一些额外的符号和函数来描述如何对RoPE进行插值。</p>
</li>
<li><p>公式(10)为：<br>[<br>$f’<em>{q,k}(x_m, m, \theta_d) &#x3D; f</em>{q,k}(x_m, g(m), h(\theta_d))$<br>]<br>这里，函数 ( g(m) ) 和 ( h(\theta_d) ) 定义了如何进行插值。</p>
</li>
<li><p>对于位置插值，我们有:<br>[<br>$s &#x3D; \frac{L’}{L}$<br>]<br>[<br>$g(m) &#x3D; s \cdot m$<br>]<br>[<br>$h(\theta_d) &#x3D; \theta_d$<br>]</p>
</li>
</ul>
<p><strong>2.4 Related work</strong></p>
<ul>
<li>这部分简要介绍了与RoPE相关的其他工作，并指出了他们的限制和不同之处。</li>
</ul>
<p>总结一下，这篇文章的主要贡献在于提出了对Rotary Position Embeddings (RoPE)进行位置插值的方法，从而能够扩展上下文窗口长度。这种方法允许在不需要大量数据的情况下进行微调，从而在更长的序列上使用预训练的语言模型。</p>
<h1 id="方法-1"><a href="#方法-1" class="headerlink" title="方法:"></a>方法:</h1><h3 id="1-“NTK-aware”-插值"><a href="#1-“NTK-aware”-插值" class="headerlink" title="1. “NTK-aware” 插值"></a>1. <strong>“NTK-aware” 插值</strong></h3><p><strong>背景</strong>：RoPE从信息编码的角度看，当输入维度较低，相应的嵌入缺乏高频组件时，深度神经网络难以学习高频信息。</p>
<p><strong>解决方案</strong>：不是简单地通过一个固定的缩放因子 ( s ) 均匀地缩放RoPE的所有维度，而是将插值压力分散到多个维度上，使高频缩放较少，低频缩放较多。</p>
<ul>
<li>( $b’ &#x3D; b \cdot s^{\frac{|D|}{|D|-2}}$ )（公式16）</li>
</ul>
<p>其中，( b’ ) 是新的基数，( s ) 是缩放因子，( |D| ) 是维度。</p>
<h3 id="2-“NTK-by-parts”-插值"><a href="#2-“NTK-by-parts”-插值" class="headerlink" title="2. “NTK-by-parts” 插值"></a>2. <strong>“NTK-by-parts” 插值</strong></h3><p><strong>背景</strong>：当上下文大小为 ( L ) 时，某些维度的波长 ( $\lambda$ ) 大于在预训练中看到的最大上下文长度。</p>
<p><strong>解决方案</strong>：选择不对波长超过上下文长度的高频维度进行插值。这可以避免模型对近距离token的位置顺序感到困惑。</p>
<ul>
<li>对于某一维度，它的波长可以通过 ($ \lambda_d &#x3D; \frac{2\pi b}{2^{\frac{d}{|D|}}}$ )（公式19）计算。</li>
<li>为了决定何时应用插值，定义了一个ramp函数 ( $\gamma_d(r)$ )（公式22）。</li>
</ul>
<h3 id="3-“Dynamic-NTK”-插值"><a href="#3-“Dynamic-NTK”-插值" class="headerlink" title="3. “Dynamic NTK” 插值"></a>3. <strong>“Dynamic NTK” 插值</strong></h3><p><strong>背景</strong>：我们希望模型在更长的上下文大小上逐渐降低性能，而不是在超出训练上下文限制时立即失败。</p>
<p><strong>解决方案</strong>：动态计算缩放因子 ( s )：</p>
<ul>
<li>($ s &#x3D; \left{\begin{array}{ll}<br>\frac{L’}{L} &amp; \text{if } \frac{L’}{L} &gt; 1 \<br>1 &amp; \text{otherwise}<br>\end{array}\right. $)（公式26）</li>
</ul>
<h3 id="4-YaRN-方法"><a href="#4-YaRN-方法" class="headerlink" title="4. YaRN 方法"></a>4. <strong>YaRN 方法</strong></h3><p><strong>背景</strong>：由于插值，长距离之间的平均最小距离会随着令牌数的增加而减少。</p>
<p><strong>解决方案</strong>：在应用softmax之前，通过将中间的注意力矩阵乘以一个温度 ( t &gt; 1 ) 来增加注意力的“温度”。这可以通过简单地按常数因子 ( $\sqrt{t}$ ) 缩放RoPE嵌入的长度来实现。</p>
<ul>
<li>($ \sqrt{t} \approx 0.1 \ln(s) + 1$ )（公式27）</li>
</ul>
<p>这个公式是通过实验得到的，描述了需要的温度与缩放因子 ( s ) 之间的关系。</p>
<h3 id="5-外推和迁移学习"><a href="#5-外推和迁移学习" class="headerlink" title="5. 外推和迁移学习"></a>5. <strong>外推和迁移学习</strong></h3><p>描述了如何使用YaRN方法进行迁移学习，从 ( s &#x3D; 16 ) 到 ( s &#x3D; 32 ) 进行训练，即使在只进行了200步的训练之后，也可以成功地外推到更长的上下文长度。</p>
<blockquote>
<p>举例子：</p>
<p>文章所解决的问题：想要处理更长的文本序列，但模型在训练时只看到了一定长度的文本。我们需要一种方法来“教”模型处理超出其原始训练上下文的文本。</p>
<p>现在，将这个问题比作一辆只能在城市内行驶的汽车，而我们想要它能在高速公路上行驶。以下是作者提出的解决方案：</p>
<ol>
<li><p><strong>“NTK-aware” 插值</strong>:</p>
<ul>
<li>想象一下，城市车辆在高速公路上可能会错过一些高速信号（或高频信息）。</li>
<li>为了解决这个问题，我们不是简单地提高车速（或均匀地缩放所有信息），而是更多地关注那些容易错过的高速信号。</li>
</ul>
</li>
<li><p><strong>“NTK-by-parts” 插值</strong>:</p>
<ul>
<li>在城市中，有些路段的车流比其他路段更重要。在模型中，有些信息维度比其他维度更关键。</li>
<li>为了确保重要路段流畅，我们对这些关键的维度进行特殊处理。</li>
</ul>
</li>
<li><p><strong>“Dynamic NTK” 插值</strong>:</p>
<ul>
<li>当我们的车辆需要进入高速公路时，我们会动态地调整它的速度，使其能够更好地适应高速公路的环境。</li>
</ul>
</li>
<li><p><strong>YaRN 方法</strong>:</p>
<ul>
<li>这是一种综合方法。想象一下，我们现在希望汽车在城市和高速公路之间都能流畅行驶。</li>
<li>为了实现这一点，我们进行了一些调整，如调整车辆的温度控制，使其在不同的环境下都能运行得很好。</li>
</ul>
</li>
<li><p><strong>外推和迁移学习</strong>:</p>
<ul>
<li>假设我们的车辆已经在一个城市中训练过了，现在我们想让它在另一个城市中也能表现得很好。</li>
<li>这部分描述了如何只进行少量的额外训练，就能使模型在新的上下文中也能运行得很好。</li>
</ul>
</li>
</ol>
</blockquote>
<ul>
<li>a. 理论背景:<ul>
<li>本文讨论了基于Transformer的语言模型在上下文窗口方面的局限性，即决定了提供示例和进行上下文学习的空间量。作者提出了YaRN方法，使用旋转位置嵌入（RoPE）来扩展这些模型的上下文窗口。他们证明了YaRN可以使模型有效地利用和外推到更长的上下文长度，超过了先前的最先进方法。作者还强调了YaRN超越了微调数据集的有限上下文的能力。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>本文介绍了旋转位置嵌入（RoPE）作为他们工作的基础。RoPE可以有效地对Transformer-based语言模型进行位置信息编码。作者解释了原始Transformer架构使用的绝对正弦位置编码，后来改进为可学习的绝对位置编码。T5相对偏差、RoPE、XPos和ALiBi等相对位置编码方案进一步提高了Transformer的性能。然而，这些位置编码在训练过程中只能泛化到上下文窗口之内。</li>
</ul>
</li>
<li>c. YaRN方法:<ul>
<li>本文讨论了使用位置插值（PI）来扩展语言模型的上下文长度的概念。他们解释了PI涉及在预训练限制内插值位置索引以扩展上下文长度。这种方法在少量微调中表现良好。作者提出了对PI方法的改进，包括适用于无微调的预训练模型的“动态NTK”插值方法和在长上下文数据上进行微调时表现良好的“NTK-by-part”插值方法。</li>
</ul>
</li>
</ul>
<h1 id="结果-1"><a href="#结果-1" class="headerlink" title="结果:"></a>结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081544409.png" alt="image-20230916160532328"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081544446.png" alt="image-20230916160549288"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202310081544406.png" alt="image-20230916160603391"></p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>本文使用了Hugging Face Open LLM Leaderboard对不同的LLM进行了比较，使用了一套标准化的四个公共基准测试：25-shot ARC-Challenge、10-shot HellaSwag、5-shot MMLU和0-shot TruthfulQA。这些模型使用这套测试进行评估，并与Llama 2基线和其他模型的已建立分数进行比较。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>结果显示，YaRN模型与其相应的Llama 2基线之间的性能下降很小。YaRN s &#x3D; 16和s &#x3D; 32模型之间的平均得分下降了0.49％，表明从64k到128k的迭代扩展中性能损失可以忽略不计。总之，YaRN改进了现有的RoPE插值方法，可以作为PI的替代方法，没有任何不利因素，并且实施工作量很小。微调模型在多个基准测试中保留了其原始能力，同时能够关注非常大的上下文大小。YaRN还允许在较短的数据集上进行微调时进行高效的外推，并可以利用转移学习以加快收敛速度。</li>
</ul>
</li>
</ul>
<blockquote>
<p>法学模型怎么用？glm2能用？</p>
<p>百川模型？调研一下最新的模型</p>
<p>创新方向</p>
<p>外置知识库</p>
<p>输入的单词太短，外挂知识库，增加输入长度，解决问题，</p>
<p>有指标上的提升</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/10/08/9%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E4%BD%8D%E7%BD%AE%E6%8F%92%E5%80%BC/" data-id="clnh6662a000170u71ekof4rj" data-title="9月第一周" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%BD%8D%E7%BD%AE%E6%8F%92%E5%80%BC-yarn%E6%8F%92%E5%80%BC/" rel="tag">位置插值&#x2F;yarn插值</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-7月第二周cvil多模态知识蒸馏" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8cvil%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T08:16:36.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8cvil%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">7月第二周cvil多模态知识蒸馏</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="7月第二周cViL跨语言多模态知识蒸馏-论文1：cViL-Cross-Lingual-Training-of-Vision-Language"><a href="#7月第二周cViL跨语言多模态知识蒸馏-论文1：cViL-Cross-Lingual-Training-of-Vision-Language" class="headerlink" title="7月第二周cViL跨语言多模态知识蒸馏 论文1：cViL: Cross-Lingual Training of Vision-Language"></a>7月第二周cViL跨语言多模态知识蒸馏 论文1：cViL: Cross-Lingual Training of Vision-Language</h1><p>Models using Knowledge Distillation</p>
<h2 id="自然语言蒸馏"><a href="#自然语言蒸馏" class="headerlink" title="??自然语言蒸馏"></a>??自然语言蒸馏</h2><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: cViL: Cross-Lingual Training of Vision-Language Models using Knowledge Distillation (cViL: 跨语言视觉-语言模型的知识蒸馏训练)</li>
<li>Authors: Kshitij Gupta, Devansh Gautam, Radhika Mamidi</li>
<li>Affiliation: IIIT Hyderabad (印度国际信息技术研究所)</li>
<li>Keywords: Vision-Language Models, Cross-Lingual Training, Knowledge Distillation</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.03354v2">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/kshitij98/cViL">GitHub</a></li>
</ul>
<h1 id="论文简要"><a href="#论文简要" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>本文提出了一种使用知识蒸馏的方法，通过跨语言训练英语视觉-语言模型来训练目标语言的单语模型，并在日语和印地语的视觉问答任务中取得了相对提升4.4%和13.4%的最新成果。</li>
</ul>
<h1 id="创新点总结："><a href="#创新点总结：" class="headerlink" title="创新点总结："></a>创新点总结：</h1><blockquote>
<ol>
<li><p><strong>使用mBERT初始化模型和分词器</strong>：作者选择了mBERT而非传统的BERT来初始化模型和分词器。mBERT是一个多语言版本的BERT，能处理多种语言，这使得模型有能力从一开始就处理多语言数据。</p>
</li>
<li><p><strong>数据增强的应用</strong>：作者通过将大量的英语数据集翻译为其他语言，从而扩大了非英语语言的数据集。这有助于改善数据稀缺问题，提高模型的性能。</p>
</li>
<li><p><strong>多方面的知识蒸馏</strong>：作者提出了一套全面的知识蒸馏方案，包括CLS标记蒸馏、图像标记蒸馏、对象标签蒸馏、混合代码蒸馏和中间层蒸馏。这些不同的蒸馏方法都试图最小化教师模型和学生模型在相应部分的差异，从而更好地将教师模型的知识迁移到学生模型中。</p>
</li>
<li><p><strong>引入混合代码蒸馏</strong>：作者提出了一个创新的混合代码蒸馏方法，通过生成具有上下文信息的双语词对齐，然后在目标语言中随机替换一部分词为英语对齐词，创建了混合语言的数据集。</p>
</li>
<li><p><strong>中间层蒸馏</strong>：作者试验了在模型的中间层应用蒸馏损失，这是一种新的尝试，可以帮助模型更好地理解语言。</p>
</li>
</ol>
</blockquote>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 视觉-语言任务在研究界越来越受关注，但主要集中在英语上。本文旨在利用仅使用英语的视觉-语言模型来训练目标语言的单语模型。</li>
<li>过去方案: 过去的方法主要是通过在预训练数据中使用多种语言来训练多语言模型，但初始训练成本高，不适合训练多种语言的多种架构的多语言模型。</li>
<li>论文的Motivation: 为了解决这些问题，本文提出了一种直接在目标语言中使用英语模型作为监督来训练单语多模态模型的方法，并通过知识蒸馏技术将高资源英语模型的知识转移到其他语言，从而减少资源消耗。</li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307151433262.png" alt="image-20230715143350223" style="zoom:150%;" />

<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307151639382.png" alt="image-20230715163918321"></p>
<ol>
<li><p><strong>CLS标记蒸馏</strong>：公式为 </p>
<p>$$<br>L_{CLS} &#x3D; MSE(C_S, C_T)<br>$$</p>
<p>这里的 (C_S) 和 (C_T) 分别是学生网络和教师网络的CLS标记嵌入。MSE() 是均方误差损失函数，它计算的是学生网络和教师网络的CLS标记嵌入之间的均方误差。</p>
</li>
<li><p>？？什么技巧，具体怎么用 <strong>图像标记蒸馏</strong>：公式为 </p>
<p>$$L_{img} &#x3D; \frac{1}{p}\sum_{i&#x3D;1}^{p}MSE(I_{S_i}, I_{T_i})$$</p>
<p>这里的 (I_{S_i}) 和 (I_{T_i}) 分别是学生网络和教师网络的第i个图像标记嵌入，(p) 是输入中图像标记的数量。</p>
</li>
<li><p><strong>对象标签蒸馏</strong>：公式为 </p>
</li>
<li><p>$$<br>L_{tag} &#x3D; \frac{1}{t^2}\sum_{i&#x3D;1}^{t}\sum_{j&#x3D;1}^{t}A_{ij}MSE(O_{S_i}, O_{T_j})<br>$$</p>
<p>这里的 (O_{S_i}) 和 (O_{T_j}) 分别是学生网络和教师网络的第i个和第j个对象标签嵌入，(A_{ij}) 是一个二值矩阵，表示第i个和第j个对象标签是否匹配和对齐，(t) 是输入中对象标签的数量。</p>
</li>
<li><p><strong>混合代码蒸馏</strong>：公式为<br>$$<br>L_{CM} &#x3D; \frac{1}{n^2}\sum_{i&#x3D;1}^{n}\sum_{j&#x3D;1}^{n}B_{ij}MSE(H_{S_i}, H_{T_j})<br>$$</p>
<p>这里的 (H_{S_i}) 和 (H_{T_j}) 分别是学生网络和教师网络的第i个和第j个文本标记嵌入，(B_{ij}) 是一个二值矩阵，表示第i个和第j个文本标记是否匹配和对齐，(n) 是输入中文本标记的数量。</p>
</li>
<li><p><strong>中间层蒸馏</strong>：对于选定的每一层 (m)，公式为<br>$$<br>L_{distil} &#x3D; \sum_{m \in L}\lambda_m(L_{CLS} + L_{img} + L_{tag} + L_{CM})<br>$$</p>
<p>这里的 (L) 是需要应用损失的层的集合，(\lambda_m) 是第m层的权重，(L_{CLS})、(L_{img})、(L_{tag}) 和 (L_{CM}) 分别是CLS标记蒸馏、图像标记蒸馏、对象标签蒸馏和混合代码蒸馏的损失。</p>
</li>
</ol>
<ul>
<li>a. 理论背景:<ul>
<li>本文提出了一种在目标语言中使用仅英语模型训练单语视觉语言模型的流程。作者扩展了OSCAR+模型，该模型使用对象标签进行图像-文本对齐，并在不同语言的视觉问答数据集上进行训练。作者引入了一种新颖的知识蒸馏方法，使用平行句子将知识从英语模型转移到资源较少的目标语言模型。作者还发布了一个日语和印地语的视觉问答数据集。该流程在日语和印地语的准确性上分别比最先进的模型提高了4.4%和13.4%。模型架构基于OSCAR方法，并使用mBERT处理非英语语言。通过将大型英语数据集进行翻译来进行数据增强，以减轻数据稀缺和语言偏差。知识蒸馏过程旨在将图像-问题对映射到跨语言的语义空间中的相同位置。知识蒸馏的目标函数被最小化以训练学生模型。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>该方法涉及使用知识蒸馏技术训练学生模型。实验中使用的教师模型是预训练于OSCAR+语料库并针对英语视觉问答进行微调的OSCAR+B检查点。学生模型使用不同类型的蒸馏，包括分类令牌蒸馏、图像令牌蒸馏、对象标签蒸馏、混合代码蒸馏和中间层蒸馏。<strong>每种类型的蒸馏都涉及最小化特定的目标函数，以对齐学生模型和教师模型的嵌入。</strong>蒸馏损失应用于学生模型的一部分层。提出了两种训练方法：CVILAUG，涉及数据增强技术；CVILKD，涉及知识蒸馏技术。模型在英语的任务特定数据集上进行训练，然后翻译到目标语言进行评估。</li>
</ul>
</li>
</ul>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><ul>
<li><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307151442421.png" alt="image-20230715144236380"></p>
</li>
<li><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307151446190.png" alt="image-20230715144639144"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307151443893.png" alt="image-20230715144336857"></p>
</li>
<li><blockquote>
<ol>
<li><strong>日语视觉问题回答</strong>：作者的模型（CVILAUG和CVILKD）在日语Visual Genome VQA数据集上的性能超过了当前的最佳模型，包括PCATT、UNITERCC和UC2。作者还注意到，在覆盖数据集的情况下，模型的正确分类率为48.02%，但由于数据集覆盖率只有74.39%，最终的准确率降低到了35.72%。</li>
<li><strong>印地语视觉问题回答</strong>：作者的模型（CVILKD）在印地语VQA v2.0数据集上的性能显著优于没有使用知识蒸馏的基线模型，准确率相对提高了7.3%。在印地语VQA v1.0数据集上，作者的系统也显著优于当前的最佳模型，包括BUTD、Bi-linear Attention和Multimodal Fusion。</li>
<li><strong>训练成本</strong>：作者的模型能够在17小时内使用知识蒸馏从零开始训练，这明显比预训练步骤需要的资源要少。</li>
<li><strong>消融研究</strong>：通过移除每一个蒸馏目标并比较模型在日语Visual Genome VQA数据集上的性能，作者发现所有的蒸馏目标都对系统的性能有所贡献。中间层蒸馏对系统的性能影响最大，而对象标签蒸馏也起到了重要的作用。</li>
</ol>
</blockquote>
</li>
<li><p>. 详细的实验设置:</p>
<ul>
<li>模型在4个Nvidia GeForce RTX 2080 Ti GPU上进行训练，每个GPU的批量大小为32个问题-答案对。使用了具体参数的AdamW优化器，以及dropout和attention dropout。在将文本输入模型之前，将其转换为小写。采用了混合精度训练以提高效率。对于CVILAUG和CVILKD两个系统，详细描述了训练方法。</li>
</ul>
</li>
<li><p>b. 详细的实验结果:</p>
<ul>
<li>研究的主要结果是对系统在最先进模型上的性能进行比较。模型在日语和印地语视觉问答数据集上的准确性优于基线，并取得了比以前模型更好的准确性。还讨论了系统在不同问题类型上的性能。通过t-SNE可视化了图像-问题对的学习特征。比较了模型生成的图像和单词令牌的嵌入，显示嵌入对齐良好，并形成了每个对象类别的可区分聚类。进行了消融研究，以调查不同蒸馏目标的贡献。移除了每个蒸馏目标，并比较了系统在日语视觉基因组数据集的测试集上的性能。根据提供的表格，不同模型在VQA数据集上的准确性显示。具有所有蒸馏目标的CVILKD模型达到了最高的35.72的准确性。消融研究表明，每个蒸馏目标对系统的性能都很重要。最关键的蒸馏目标是中间层蒸馏，其次是对象标签蒸馏。基线模型的准确性为33.75。</li>
</ul>
</li>
</ul>
<h1 id="论文2：MMLU-Measuring-massive-multitask-language-understanding-in-Chinese"><a href="#论文2：MMLU-Measuring-massive-multitask-language-understanding-in-Chinese" class="headerlink" title="论文2：MMLU: Measuring massive multitask language understanding in Chinese"></a>论文2：MMLU: Measuring massive multitask language understanding in Chinese</h1><h1 id="Basic-Information-1"><a href="#Basic-Information-1" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: CMMLU: Measuring massive multitask language understanding in Chinese (CMMLU：测量中文大规模多任务语言理解)</li>
<li>Authors: Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin</li>
<li>Affiliation: MBZUAI (Mohamed bin Zayed University of Artificial Intelligence) (MBZUAI)</li>
<li>Keywords: large language models, Chinese benchmark, language understanding, performance evaluation, knowledge and reasoning abilities</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.09212v1">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/haonan-li/CMMLU">GitHub</a></li>
</ul>
<h1 id="论文简要-1"><a href="#论文简要-1" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>本文介绍了CMMLU，这是一个全面的中文评估基准，旨在评估大规模语言模型在不同主题和环境下的性能。通过对18个先进的多语言和中文导向的语言模型进行评估，发现大多数模型在给定上下文示例和思维链提示的情况下，难以达到50%的平均准确率，而随机基准为25%。此外，通过广泛的实验，还分析了影响模型性能的因素，并提出了增强语言模型的方向。</li>
</ul>
<h1 id="背景信息-1"><a href="#背景信息-1" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 随着大型语言模型（LLMs）的能力不断提升，评估其性能变得越来越重要和具有挑战性。然而，评估这些模型中编码的知识和推理能力变得越来越具有挑战性，尤其是对于生成流畅和合理回答的LLMs的普及。</li>
<li>过去方案: 为了解决这个问题，研究人员从不同的角度创建了各种基准，如MMLU和CLUE。然而，这些基准主要针对英语，限制了对其他语言的LLMs进行评估的能力。</li>
<li>论文的Motivation: 鉴于中文是全球使用人数最多的语言，为了评估中文语言模型，需要提出一个全面的中文评估套件。本文的动机就是设计和介绍了CMMLU，这是一个专门针对中文语言和文化背景的综合评估套件，旨在评估LLMs的高级知识和推理能力。CMMLU涵盖了广泛的主题，包括自然科学、社会科学、工程和人文学科，并通过对ChatGPT和其他先进的中文导向LLMs进行评估，揭示了LLMs在中文知识和语言理解方面的改进空间。</li>
</ul>
<h1 id="方法-1"><a href="#方法-1" class="headerlink" title="方法:"></a>方法:</h1><ul>
<li>a. 理论背景:<ul>
<li>本文介绍了CMMLU，这是一个综合性的中文基准，用于评估大型语言模型（LLMs）在各个学科中的性能。评估了18个先进的多语言和中文导向的LLMs，结果显示大多数模型难以达到50%的平均准确率，表明LLMs需要改进。本文还进行了实验，以确定影响模型性能的因素，并提出了增强LLMs的方向。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>本文提出了CMMLU作为一个综合性的中文评估套件，涵盖了广泛的学科，并在中文语言和文化背景下评估LLMs。文中强调了LLMs在中文知识和语言理解方面的改进空间。</li>
</ul>
</li>
</ul>
<h1 id="结果-1"><a href="#结果-1" class="headerlink" title="结果:"></a>结果:</h1><ul>
<li>a. 详细的实验设置:<ul>
<li>本研究使用的数据集包含67个学科的11,528个多项选择题。这些问题由四名注释员手动从免费资源中收集，每小时50元人民币的费率。收集过程大约耗时250小时，并努力防止问题出现在语言模型的训练集中。数据集被分为包含5个问题的few-shot开发集和包含100多个问题的测试集。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>本研究评估了18个不同大小、语言导向和阶段（预训练或微调）的先进语言模型（LLMs），以评估它们在知识为中心的基准测试中的性能。评估包括零样本和少样本设置，并根据模型理解和利用知识的能力对模型进行评估。</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8cvil%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" data-id="cllkmcmex0005ksu72xzt0ecu" data-title="7月第二周cvil多模态知识蒸馏" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%B7%A8%E8%AF%AD%E8%A8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" rel="tag">跨语言多模态知识蒸馏</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-8月第三周" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%89%E5%91%A8/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T08:05:27.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%89%E5%91%A8/">8月第三周HomoDistil</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <ul>
<li><ul>
<li><h1 id="8月第三周周报"><a href="#8月第三周周报" class="headerlink" title="8月第三周周报"></a>8月第三周周报</h1><h2 id="论文1：HomoDistil-Homotopic-Task-Agnostic-Distillation-of-Pre-trained-Transformers"><a href="#论文1：HomoDistil-Homotopic-Task-Agnostic-Distillation-of-Pre-trained-Transformers" class="headerlink" title="论文1：HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers"></a>论文1：HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers</h2><p>base ICLR 2023</p>
<p>Chen Liang⋆∗, Haoming Jiang⋄, Zheng Li⋄, Xianfeng Tang⋄, Bin Yin⋄ &amp; Tuo Zhao⋆ ⋆Georgia Institute of Technology, ⋄ Amazon{cliang73,tourzhao}@gatech.edu,{jhaoming,amzzhe,xianft,alexbyin}@amazon.com</p>
<p>同主题蒸馏</p>
<h3 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h3><p>知识蒸馏已被证明是一个强大的模型压缩方法，以促进在实践中部署预训练的语言模型。本文侧重于与任务无关的蒸馏。它产生了一个紧凑的预训练模型，可以很容易地在计算成本和内存足迹较小的各种任务上进行微调。尽管具有实际的好处，但与任务无关的蒸馏具有挑战性。由于教师模型比学生模型具有更大的容量和更强的表示能力，因此学生很难在大量开放域训练数据上产生与教师匹配的预测。这种较大的预测差异通常会降低知识蒸馏的好处。为了应对这一挑战，我们提出了同主题蒸馏（HomoDistil），这是一种新的与任务无关的蒸馏方法，配备了迭代修剪。具体来说，我们从教师模型初始化学生模型，并迭代地修剪学生的神经元，直到达到目标宽度。这种方法在整个蒸馏过程中保持了教师和学生的预测之间的微小差异，保证了知识转移的有效性。大量实验表明，HomoDistil 在现有基线上取得了显着的改进。</p>
<h3 id="摘要总结："><a href="#摘要总结：" class="headerlink" title="摘要总结："></a>摘要总结：</h3><ul>
<li>(1): 本文介绍一种新型的迭代剪枝算法-Homotopic Distillation。</li>
<li>(2): 该算法采用结构剪枝技术，通过迭代剪枝来优化模型空间和时间效率。其实现步骤具体如下：</li>
<li>(3): 在优化模型时，每一次迭代会剪枝掉最不重要的神经元并对剪枝后的模型进行蒸馏。每个参数的重要性得分则可以根据公式进行计算。另外，我们采用SGD算法对学生模型进行权重更新，通过优化其知识蒸馏损失来提高模型的精度和泛化效果。</li>
<li>(4): 在BERT-base的自然语言理解和问答任务上，该算法在GLUE基准和SQuaD v1.1和v2.0数据集上都取得了优秀的结果，证明了Homotopic Distillation的有效性和可行性。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308201445125.png" alt="1"></p>
<p>HomoDistil 动机说明（用剪枝给知识蒸馏做初始化，并迭代式地获得最终的学生模型结构）</p>
<blockquote>
<p>随着预训练大模型规模的不断增加，任务特定蒸馏（下游微调蒸馏）的成本越来越高，任务无关蒸馏变得越来越重要。然而，由于教师模型的模型容量和表示能力远超学生模型，因此学生很难在大量开放域训练数据上模仿教师的预测。本文提出了同源蒸馏（Homotopic Distillation, HomoDistil）来缓解这一问题，该方法充分利用了蒸馏和剪枝的优势，将两者有机结合在了一起。</p>
<p>左图:在 HomoDistil 中，学生从老师初始化，并通过蒸馏过程迭代修剪。矩形的宽度表示层的宽度。颜色的深度代表训练的充分性。</p>
<p>右图：本文用教师模型初始化学生模型，以缓解两者在蒸馏过程中的容量和能力差异，并通过基于蒸馏损失的重要性得分的迭代剪枝，来逐步将学生模型修剪至最终想要的目标结构。在整个蒸馏+剪枝的过程中，教师和学生一直保持着较小的预测差异，这有助于知识更有效的传递。其核心动机如图 1 所示。</p>
</blockquote>
<p>该方法可从「蒸馏损失函数」和「迭代剪枝细节」两部分进行介绍。</p>
<h2 id="2-1-蒸馏损失函数"><a href="#2-1-蒸馏损失函数" class="headerlink" title="2.1 蒸馏损失函数"></a>2.1 蒸馏损失函数</h2><p>本文采用了与 TinyBERT [1] 的通用蒸馏阶段类似的蒸馏损失函数进行任务无关的蒸馏。整体损失函数可以分为三部分：</p>
<p>a）任务损失：设 是学生模型在开放域数据上预训练的任务损失（例如 BERT 的掩码语言建模损失 ）；</p>
<p>b）概率蒸馏损失：即 Hinton [2] 经典 KD 论文中的 KL 散度损失；</p>
<p>c）Transformer 蒸馏损失：具体包括教师和学生的中间层及嵌入层的隐层表示的差异损失，以及中间层的注意力得分差异损失。<br>当然，让我们详细解释上述段落中提到的公式和概念，并尝试回答可能的问题。我们这里讨论的是任务无关的知识蒸馏（Task-Agnostic Distillation），其目的是通过捕捉教师模型的某些内部表示来训练学生模型。</p>
<h3 id="1-知识蒸馏损失"><a href="#1-知识蒸馏损失" class="headerlink" title="1. 知识蒸馏损失 \"></a>1. 知识蒸馏损失 \</h3><p>$$<br>L_{\text{MLM}} + \alpha_1 D_{\text{KL}}<br>$$</p>
<h4 id="掩码语言建模损失-L-text-MLM"><a href="#掩码语言建模损失-L-text-MLM" class="headerlink" title="掩码语言建模损失 ( $L_{\text{MLM}} $)"></a>掩码语言建模损失 ( $L_{\text{MLM}} $)</h4><p>这部分损失与BERT等预训练模型的训练方式有关，其中模型试图预测句子中掩盖（或隐去）的词汇。它的目的是确保学生模型能够理解和生成自然语言。</p>
<h4 id="1-2-Kullback-Leibler-KL-散度损失-D-text-KL"><a href="#1-2-Kullback-Leibler-KL-散度损失-D-text-KL" class="headerlink" title="1.2 Kullback-Leibler (KL) 散度损失 ( $D_{\text{KL}}$ )"></a>1.2 Kullback-Leibler (KL) 散度损失 ( $D_{\text{KL}}$ )</h4><p>KL散度量化了教师模型和学生模型之间概率分布的差异。这有助于学生模型学习教师模型的概率预测行为。</p>
<h3 id="2-Transformer蒸馏损失"><a href="#2-Transformer蒸馏损失" class="headerlink" title="2. Transformer蒸馏损失"></a>2. Transformer蒸馏损失</h3><h4 id="2-1-隐藏表示的中间层蒸馏损失-L-text-hidn"><a href="#2-1-隐藏表示的中间层蒸馏损失-L-text-hidn" class="headerlink" title="2.1 隐藏表示的中间层蒸馏损失 ( $L_{\text{hidn}}$ )"></a>2.1 隐藏表示的中间层蒸馏损失 ( $L_{\text{hidn}}$ )</h4><p>学生模型和教师模型的隐藏表示在k-th中间层被比较。为了维度匹配，引入了一个可学习的线性投影矩阵( W_k^{\text{hidn}} )，使得学生模型的隐藏表示与教师模型在同一空间内。</p>
<ul>
<li>$ H_k^t  和 H_k^s $: 分别表示教师和学生模型在k-th中间层的隐藏表示。</li>
<li>${MSE}(H_k^t, H_k^s W_k^{\text{hidn}})$: 指的是教师和学生隐藏表示之间的均方误差。</li>
</ul>
<h4 id="2-2-嵌入层蒸馏损失-L-text-emb"><a href="#2-2-嵌入层蒸馏损失-L-text-emb" class="headerlink" title="2.2 嵌入层蒸馏损失 ( $L_{\text{emb}}$ )"></a>2.2 嵌入层蒸馏损失 ( $L_{\text{emb}}$ )</h4><p>嵌入层损失类似于中间层损失，但是作用在嵌入层。通过均方误差比较教师和学生模型的嵌入表示，并引入一个线性投影矩阵( $W_{\text{emb}}$ )来进行维度匹配。</p>
<h4 id="2-3-注意力蒸馏损失-L-text-attn"><a href="#2-3-注意力蒸馏损失-L-text-attn" class="headerlink" title="2.3 注意力蒸馏损失 ($ L_{\text{attn}}$ )"></a>2.3 注意力蒸馏损失 ($ L_{\text{attn}}$ )</h4><p>这一损失比较了教师和学生模型在k-th层的注意力分数矩阵。通过捕捉教师模型的注意力模式，有助于学生模型学习如何将注意力分配给不同的输入部分。</p>
<h3 id="3-总损失-L-text-total"><a href="#3-总损失-L-text-total" class="headerlink" title="3. 总损失 ($ L_{\text{total}}$ )"></a>3. 总损失 ($ L_{\text{total}}$ )</h3><p>所有这些损失项的加权和形成了总损失，通过优化这个总损失来训练学生模型。权重参数( \alpha_1, \alpha_2, \alpha_3, \alpha_4 ) 允许我们调整每个组件在总损失中的相对重要性。<strong>公式5</strong></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211622828.png" alt="image-20230819113820853"></p>
<h2 id="2-2-迭代剪枝细节"><a href="#2-2-迭代剪枝细节" class="headerlink" title="2.2 迭代剪枝细节"></a><strong>2.2 迭代剪枝细节</strong></h2><h3 id="1-学生模型的初始化"><a href="#1-学生模型的初始化" class="headerlink" title="1. 学生模型的初始化"></a>1. 学生模型的初始化</h3><p>学生模型从预训练的教师模型初始化，即$\theta_s^{(0)} &#x3D; \theta_t$。这意味着学生模型一开始的权重与教师模型相同。</p>
<h3 id="2-SGD型算法的更新"><a href="#2-SGD型算法的更新" class="headerlink" title="2. SGD型算法的更新"></a>2. SGD型算法的更新</h3><p>在第(t)次迭代中，学生模型根据总损失($L_{\text{total}}$)（由方程5定义）使用SGD（随机梯度下降）类型的算法进行更新。<strong>公式6</strong>更新规则为：</p>
<p>$\theta_s^{(t)} \leftarrow \theta_s^{(t-1)} - \eta \nabla_{\theta_s^{(t-1)}} L_{\text{total}}(\theta_s^{(t-1)}, \theta_t),$</p>
<p>其中($\eta$)是步长，控制每次迭代的更新幅度。</p>
<h3 id="3-计算重要性得分"><a href="#3-计算重要性得分" class="headerlink" title="3. 计算重要性得分"></a>3. 计算重要性得分</h3><p>接下来计算所有参数的重要性得分($S_j^{(t)}$)。重要性得分与参数的梯度和值有关，可以用来确定哪些神经元可以被剪枝，而不会显著影响模型性能。得分定义为：<br><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211622525.png" alt="image-20230819115152088"></p>
<h3 id="4-计算权重矩阵的重要性得分"><a href="#4-计算权重矩阵的重要性得分" class="headerlink" title="4. 计算权重矩阵的重要性得分"></a>4. 计算权重矩阵的重要性得分</h3><p>对于学生模型中的任何权重矩阵($W^{(t)}$)，其对应的重要性得分表示为($S^{(t)}_W$。然后定义每一列的重要性得分为$N^{(t)}_W$，<strong>公式7</strong> 其中</p>
<p>$N^{(t)}_{W_i} &#x3D; \left| S^{(t)}_W[:,i] \right|_1 \quad \forall i &#x3D; 1, \ldots, ds.$</p>
<p>这一步骤有助于确定哪些列（神经元）对模型最重要。</p>
<h3 id="5-计算二进制掩码"><a href="#5-计算二进制掩码" class="headerlink" title="5. 计算二进制掩码"></a>5. 计算二进制掩码</h3><p>接下来，计算与权重矩阵相关的二进制掩码($M^{(t)}_W$)，也就是<strong>公式8</strong> 其中</p>
<p>$M^{(t)}<em>{W[:,i]} &#x3D;<br>\begin{cases}<br>      1 &amp; \text{if } N^{(t)}</em>{W_i} \text{ is in the top } r^{(t)} \text{ of } N^{(t)}_W, \<br>      0 &amp; \text{otherwise}, , \forall i &#x3D; 1, \ldots, ds.<br>\end{cases}$</p>
<p>这个掩码用于确定哪些列保留，哪些列剪除。</p>
<h3 id="6-剪枝的时间表"><a href="#6-剪枝的时间表" class="headerlink" title="6. 剪枝的时间表"></a>6. 剪枝的时间表</h3><p>剪枝的速率(r(t))由一个三次递减函数控制，确保稀疏性缓慢增加，列逐渐剪除。参数(rf)代表最终稀疏性，(T)是总训练迭代次数，($0 \leq ti &lt; tf \leq T$)是超参数。</p>
<p>算法结果：</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211619521.png" alt="image-20230819135528828"></p>
<blockquote>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ol>
<li>($\theta_t$)：教师模型的参数。</li>
<li>($T, t_i, t_f, r_f, \alpha_1, \alpha_2, \alpha_3, \alpha_4$)：超参数。</li>
</ol>
<h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><ol>
<li>($\theta(T)_s$)：学生模型的参数。</li>
</ol>
<h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><ol>
<li><p><strong>初始化</strong>：学生模型的参数($\theta(0)_s$)被初始化为教师模型的参数($\theta_t$)。</p>
</li>
<li><p><strong>循环训练</strong>：算法进行(T)次迭代，其中每次迭代执行以下步骤：<br>a. <strong>计算总损失</strong>：根据等式5计算总损失($L_{\text{total}}$)。<br>b. <strong>参数更新</strong>：使用梯度下降法更新学生模型的参数。更新规则为($\theta(t)_s \leftarrow \theta(t-1)_s - \eta \nabla \theta(t-1)<em>s L</em>{\text{total}}$)，其中($\eta$)是学习率。<br>c. <strong>计算重要性得分</strong>：根据等式6计算重要性得分(S(t))。<br>d. <strong>计算权重和掩码</strong>：对于学生模型的每个权重矩阵(W(t))：</p>
<ul>
<li>计算各列的重要性得分(N(t)_W)，遵循等式7。</li>
<li>计算二进制掩码(M(t)_W)，遵循等式8。</li>
<li>使用掩码更新权重矩阵：($W(t) \leftarrow W(t) \odot M(t)_W$)。</li>
</ul>
</li>
</ol>
</blockquote>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>数据:<strong>语料库</strong>：</p>
<ul>
<li><strong>Wikipedia 2</strong>：包含2500M单词的英语维基百科语料库。</li>
<li><strong>BookCorpus</strong>（Zhu et al., 2015）：包含800M单词。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211619940.png" alt="image-20230819135306819"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211619689.png" alt="image-20230819142515987"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211619638.png" alt="image-20230819142537042"></p>
<h1 id="论文2-HOW-I-LEARNED-TO-STOP-WORRYING-AND-LOVERETRAINING"><a href="#论文2-HOW-I-LEARNED-TO-STOP-WORRYING-AND-LOVERETRAINING" class="headerlink" title="论文2 HOW I LEARNED TO STOP WORRYING AND LOVERETRAINING"></a>论文2 HOW I LEARNED TO STOP WORRYING AND LOVERETRAINING</h1><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: How I Learned to Stop Worrying and Love Retraining (我如何学会不再担心并热爱重新训练)</li>
<li>Authors: Max Zimmer, Christoph Spiegel, Sebastian Pokutta</li>
<li>Affiliation: Department for AI in Society, Science, and Technology, Zuse Institute Berlin, Germany (柏林祖斯研究所，德国)</li>
<li>Keywords: Neural Network Pruning, Retraining, Learning Rate Schedule, Budgeted Training</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=xxx">Paper</a> , [GitHub: None]</li>
<li>ICLR 2023</li>
</ul>
<h1 id="论文简要"><a href="#论文简要" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>本研究提出了一种简单而高效的方法，通过使用线性学习率调度来大幅缩短神经网络修剪后的重新训练阶段，并在初始稠密训练阶段上实施预算控制，从而在质量和速度上超越了现有的复杂方法。</li>
</ul>
<p>论文动机：</p>
<ol>
<li><strong>剪枝的必要性和挑战</strong>：由于现代神经网络通常具有大量的参数，因此需要高内存和计算资源。通过剪枝可以显著减少这些需求，但同时也带来了许多挑战，例如剪枝后的性能下降以及剪枝过程的计算复杂性。</li>
<li><strong>迭代大小剪枝（IMP）的问题</strong>：虽然IMP是一种有效的剪枝方法，但其计算复杂性和潜在的次优状态引起了关注。作者意图重新审视并改进这种方法，使其更加实用和有效。</li>
<li><strong>重新思考剪枝和再训练的方法</strong>：作者试图通过在预算训练的背景下重新思考IMP的再训练阶段，找到一种可以大大缩短运行时间并保持性能的方法。</li>
<li><strong>寻找更有效的学习率计划</strong>：通过提出自适应线性学习速率重启（ALLR），作者试图解决选择线性计划的初始值的问题，并考虑剪枝的影响和整体再训练时间。</li>
<li><strong>挑战现有观点</strong>：作者挑战了一般认为剪枝稳定方法优于依赖“硬”剪枝和再训练的方法的观点，通过实验和新方法来证明他们的观点。</li>
<li><strong>提供实用的解决方案</strong>：通过提出预算IMP（BIMP）和其他改进，作者意图提供一种实际、高效的解决方案，以便在不牺牲性能的情况下更有效地剪枝神经网络。</li>
</ol>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 近年来，神经网络修剪已成为压缩模型的重要方法，但修剪后的网络需要经过重新训练来恢复性能，而这一过程通常非常耗时。</li>
<li>过去方案: 传统的修剪方法通常需要多次迭代的重新训练，计算资源消耗较大，而现有的稳定修剪方法在训练过程中引入了复杂的参数化，也无法很好地解决这个问题。</li>
<li>论文的Motivation: 针对修剪后的重新训练阶段的学习率调度问题，以及初始稠密训练阶段的预算控制问题，本研究提出了一种简单而高效的方法，通过线性学习率调度和预算控制，实现了修剪后网络的高质量和快速训练。</li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211150540.png" alt="image-20230821115029487"></p>
<p>提出了几种不同的学习率调度策略来改进迭代式裁剪-重训循环(IMP)中的重训阶段,包括:</p>
<ul>
<li>LRW:学习率回绕(Learning Rate Rewinding),使用原训练的后几个学习率进行重训</li>
<li>SLR:学习率重新启动(Restarting),使用缩放后的原学习率调度进行重训</li>
<li>CLR:周期性学习率重新启动,使用基于余弦的周期学习率进行重训</li>
<li>LLR:线性学习率重新启动,使用线性衰减的学习率进行重训</li>
<li>ALLR:自适应线性学习率重新启动,根据裁剪影响自适应地确定初始学习率</li>
</ul>
<h3 id="裁剪稳定性"><a href="#裁剪稳定性" class="headerlink" title="裁剪稳定性"></a>裁剪稳定性</h3><p>提出了 Budgeted IMP (BIMP) 方法,在给定训练epoch次数budget内,首先dense训练,然后进行IMP裁剪和重训,避免额外的隐性偏置,与传统裁剪稳定性方法进行公平比较。</p>
<h3 id="裁剪方式"><a href="#裁剪方式" class="headerlink" title="裁剪方式"></a>裁剪方式</h3><p>简单采用基于权重大小的全局裁剪方式。</p>
<p>总体来说,主要贡献在于提出了重新思考重训的学习率调度策略,并在公平的基础上比较了裁剪稳定性方法,为神经网络裁剪的高效训练提供了新思路。</p>
<ul>
<li>a. 理论背景:<ul>
<li>本文讨论了现代神经网络架构中过参数化的问题以及修剪模型以压缩这些模型的潜在好处。介绍了迭代幅度修剪（IMP）的概念以及与重新训练阶段相关的挑战。作者提出重新思考重新训练阶段，将其置于预算训练的背景下，并证明可以使用简单的线性学习率调度显著缩短重新训练时间。他们还提出了一种自适应选择学习率初始值的方法。此外，他们建议在IMP的初始密集训练阶段施加预算，以高效地生成稀疏的训练网络。作者强调他们的研究结果质疑了修剪稳定方法通常优于依赖“硬”修剪和重新训练方法的观点。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>本文讨论了修剪神经网络的不同重新训练调度方法。作者提出了几种方法，包括速率回退（LRW）、缩放学习率重启（SLR）、循环学习率重启（CLR）、线性学习率重启（LLR）和自适应线性学习率重启（ALLR）。LRW使用每个周期的最后学习率重新训练修剪后的网络。SLR将原始调度压缩到重新训练时间框架内，并包含一个短暖身阶段。CLR基于1周期学习率调度，并包含一个短暖身阶段。LLR在重新训练期间使用线性学习率调度，而ALLR根据修剪的影响和可用的重新训练时间动态调整重新训练调度的初始值。作者根据Li等人（2020）关于在固定迭代预算内训练神经网络的研究结果提出了这些方法。他们认为重新训练应该考虑预算训练的方面，并且从预算训练中得出的经验通常适用于修剪的背景下。</li>
</ul>
</li>
</ul>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211154782.png" alt="image-20230821115453710"></p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>实验结果显示，修剪后的重新训练类似于预算训练。线性和余弦学习率调度优于常数和阶梯调度，其中LLR略优于CLR。然而，对于较短的重新训练时间和中等稀疏度，CLR和LLR等固定重启调度与FT和LRW相比不太竞争。ALLR在小的重新训练预算范围内始终改进了以前的方法。实验结果还展示了ALLR在不同重新训练周期下实现高测试准确率的有效性。此外，对于CIFAR-10上的ResNet-56和ImageNet上的ResNet-50的结果显示了BIMP与修剪稳定方法在不同稀疏度水平上的比较。BIMP在准确性、加速比和稀疏度方面与其他方法相比具有竞争力。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>实验结果显示，BIMP方法在测试性能、理论加速比和实现的稀疏度方面优于许多其他修剪稳定方法。对于ImageNet，DNW方法与BIMP相当或更好，但训练时间大约是BIMP的两倍。大多数其他方法与BIMP相比似乎处于劣势，而DPF是一个强有力的竞争对手。BIMP在相同数量的训练周期内实现了这些结果，并且没有一些其他方法的计算开销。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%89%E5%91%A8/" data-id="cllkmcmf4000eksu756mw56ms" data-title="8月第三周HomoDistil" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%89%AA%E6%9E%9D-%E8%92%B8%E9%A6%8F/" rel="tag">剪枝+蒸馏</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-8月第二周剪枝相关" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T08:03:58.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3/">8月第二周剪枝相关</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="8月第二周-以前的剪枝相关论文"><a href="#8月第二周-以前的剪枝相关论文" class="headerlink" title="8月第二周 以前的剪枝相关论文"></a>8月第二周 以前的剪枝相关论文</h1><h1 id="论文1：Densely-Connected-Convolutional-Networks-密集连接卷积网络"><a href="#论文1：Densely-Connected-Convolutional-Networks-密集连接卷积网络" class="headerlink" title="论文1：Densely Connected Convolutional Networks (密集连接卷积网络)"></a>论文1：Densely Connected Convolutional Networks (密集连接卷积网络)</h1><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Densely Connected Convolutional Networks (密集连接卷积网络)</li>
<li>Authors: Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger</li>
<li>Affiliation: First author’s affiliation: Cornell University (第一作者所属机构：康奈尔大学)</li>
<li>Keywords: convolutional networks, deep learning, dense connectivity, vanishing-gradient problem, feature reuse (卷积网络，深度学习，密集连接，梯度消失问题，特征重用)</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.06993">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/liuzhuang13/DenseNet">GitHub Code</a></li>
<li>16年8月</li>
</ul>
<h1 id="论文简要"><a href="#论文简要" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604156.png" alt="b8624ba6215bcbdf5fbd0071bbd3eccf_0_Figure_1"></p>
<p>图1：DenseNet在ResNet的基础上(ResNet介绍)，进一步扩展网络连接，该层前面所有层的feature map都是这层的输入，该层的feature map是后面所有层的输入。网络结构如上图:</p>
<ul>
<li>动机：随着CNN网络层数的不断增加，gradient vanishing和modeldegradation问题出现在了人们面前，BatchNormalization的广泛使用在一定程度上缓解了gradient vanishing的问题，而ResNet和HighwayNetworks通过构造恒等映射设置旁路，进一步减少了gradient vanishing和model degradation的产生。</li>
</ul>
<p>创新点总结:</p>
<p>(1)相比ResNet拥有更少的参数数量<br>(2)旁路加强了特征的重用.<br>(3)网络更易于训练，并具有一定的正则效果<br>(4)缓解了gradient vanishing和model degradation的问题</p>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 卷积神经网络（CNNs）已成为视觉目标识别的主要机器学习方法。然而，随着网络层数的增加，信息和梯度在网络中传递时可能会消失，这成为一个新的研究问题。</li>
<li>过去方案: 过去的研究提出了一些解决梯度消失问题的方法，如ResNets和Highway Networks，通过建立早期层与后期层之间的短路径来保留信息。然而，这些方法仍存在一些问题，如参数冗余和训练困难。</li>
<li>CNN史上的一个里程碑事件是ResNet模型的出现，ResNet可以训练出更深的CNN模型，从而实现更高的准确度。ResNet模型的核心是通过建立前面层与后面层之间的“短路连接”（shortcuts，skip connection），这有助于训练过程中梯度的反向传播，从而能训练出更深的CNN网络。</li>
<li>DenseNet模型，它的基本思路与ResNet一致，但是它建立的是前面所有层与后面层的密集连接（dense connection），它的名称也是由此而来。DenseNet的另一大特色是通过特征在channel上的连接来实现特征重用（feature reuse）。这些特点让DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能，DenseNet也因此斩获CVPR 2017的最佳论文奖。</li>
<li>论文的Motivation: 基于过去方法的观察，本文提出了一种新的网络架构，即DenseNet，通过密集连接每一层，实现了信息的最大流动和特征的重用。这种架构具有更好的参数效率和训练性能。</li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><p>Residual Connection 能够让模型趋向于宽网络，Densenet论文核心思想:对每一层的前面所有层都加一个单独的 shortcut到该层，使得任意两层网络都可以直接”沟通”。即下图：</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604162.png" alt="b8624ba6215bcbdf5fbd0071bbd3eccf_2_Figure_2"></p>
<h3 id="1-密集块（Dense-Block）受到GoogleNet的启发，论文提出了Dense-Block，即在每个Block内，所有layer都保持dense-connectivity，而在Block之间是没有dense-connectivity，而是通过transition-layer连接的"><a href="#1-密集块（Dense-Block）受到GoogleNet的启发，论文提出了Dense-Block，即在每个Block内，所有layer都保持dense-connectivity，而在Block之间是没有dense-connectivity，而是通过transition-layer连接的" class="headerlink" title="1. 密集块（Dense Block）受到GoogleNet的启发，论文提出了Dense Block，即在每个Block内，所有layer都保持dense connectivity，而在Block之间是没有dense connectivity，而是通过transition layer连接的"></a>1. 密集块（Dense Block）受到GoogleNet的启发，论文提出了Dense Block，即在每个Block内，所有layer都保持dense connectivity，而在Block之间是没有dense connectivity，而是通过transition layer连接的</h3><p>DenseNet的核心是一种名为“密集块”的结构。在密集块中，每一层都与之前所有层直接连接。这意味着第 ( i ) 层的输入不仅是第 ( i-1 ) 层的输出，还包括第 ( i-2 ) 层、第 ( i-3 ) 层等的输出。每一层的输出又成为其后所有层的输入。</p>
<p>这种密集连接的数学表达可以用下式表示：</p>
<p>$$<br>x_{l} &#x3D; H_{l}([x_{0}, x_{1}, \ldots, x_{l-1}])<br>$$</p>
<p>其中，( x_{l} ) 是第 ( l ) 层的输出，( H_{l} ) 是第 ( l ) 层的转换函数（如卷积、激活函数等），方括号表示将所有输入连接在一起。</p>
<p>H()就是Composite function，,每个Composite function的结构如下:<br>即单个Block内，层与层之间的非线性转换函数：</p>
<p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812120140992.png" alt="image-20230812120140992"></p>
<h3 id="2-过渡层（Transition-Layer）"><a href="#2-过渡层（Transition-Layer）" class="headerlink" title="2. 过渡层（Transition Layer）"></a>2. 过渡层（Transition Layer）</h3><p>由于密集连接可能导致特征图尺寸和通道数急剧增加，DenseNet引入了“过渡层”来控制网络的复杂性。过渡层通常包括卷积层和池化层，用于减少特征图的尺寸和通道数。结构如下</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604167.png" alt="image-20230812120040464"></p>
<h3 id="3-增长率（Growth-Rate）"><a href="#3-增长率（Growth-Rate）" class="headerlink" title="3. 增长率（Growth Rate）"></a>3. 增长率（Growth Rate）</h3><p>增长率是DenseNet中的一个关键参数，用于控制每一层添加的特征数量。假设增长率为 ( k )，则每一层产生 ( k ) 个新特征图，并将其与之前的特征图连接在一起。增长率有助于平衡网络的容量和计算复杂性。</p>
<p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812120217759.png" alt="image-20230812120217759"></p>
<h3 id="4-Bottleneck-Layer"><a href="#4-Bottleneck-Layer" class="headerlink" title="4. Bottleneck Layer"></a>4. Bottleneck Layer</h3><p>为了进一步减少计算负担（即使是每层只产生k个feature maps，但还是很多），DenseNet通常在每个卷积层之前添加一个“瓶颈层”（Bottleneck Layer）。瓶颈层包括1x1的卷积操作，用于减少特征图的通道数，然后再传递到更复杂的卷积层。</p>
<p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812120300514.png" alt="image-20230812120300514"></p>
<h3 id="5-合适的损失函数和优化器"><a href="#5-合适的损失函数和优化器" class="headerlink" title="5. 合适的损失函数和优化器"></a>5. 合适的损失函数和优化器</h3><p>与其他深度学习模型一样，DenseNet也需要选择合适的损失函数和优化器来进行训练。常用的损失函数如交叉熵损失，优化器如Adam或SGD等。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>DenseNet通过引入密集连接实现了特征的有效传播和重用，减轻了梯度消失问题，降低了参数数量。其独特的密集块、过渡层、增长率和瓶颈层等设计使其在多项计算机视觉任务上表现出色。</p>
<ul>
<li>a. 理论背景:<ul>
<li>本文介绍了密集卷积网络（DenseNets），它以前馈方式将每一层与其他每一层相连接。这种密集连接模式具有几个优点，包括缓解梯度消失问题、增强特征传播、鼓励特征重用和减少参数数量。DenseNets在目标识别基准任务上取得了显著的改进，同时需要更少的计算量。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>DenseNets通过特征重用来利用网络的潜力，从而得到易于训练和参数高效的紧凑模型。它们将任意一层与所有后续层直接连接，改善了层之间的信息流动。每一层的复合函数包括批归一化、修正线性单元和卷积。为了便于下采样，网络被划分为具有过渡层的密集连接的密集块。网络的增长率决定了每一层对网络全局状态的贡献程度。还使用瓶颈层和压缩因子来提高计算效率和模型紧凑性。</li>
</ul>
</li>
</ul>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果:"></a>实验结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604154.png" alt="image-20230812121120350"></p>
<ul>
<li><p>使用的是DenseNet-BC</p>
</li>
<li><p>使用4个Dense Block</p>
</li>
<li><p>在送入第一个Dense Block前，会先送到一个7x7x2k的stride&#x3D;2的卷积层</p>
</li>
<li><p>o所有的layers的feature map都设置为k</p>
</li>
<li><p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812121225490.png" alt="image-20230812121225490"></p>
</li>
<li><p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812121245625.png" alt="image-20230812121245625"></p>
</li>
<li><p>a. 详细的实验设置:</p>
<ul>
<li>DenseNet-BC在CIFAR数据集上取得了3.46%的错误率（C10+）和17.18%的错误率（C100+），优于现有的最先进结果。</li>
<li>DenseNet-BC在SVHN数据集上取得了比wide ResNet更好的结果，L&#x3D;100，k&#x3D;24，使用了dropout。</li>
</ul>
</li>
<li><p>b. 详细的实验结果:</p>
<ul>
<li>250层的DenseNet-BC在SVHN上的性能与较短的对应模型相比没有进一步提高。</li>
<li>DenseNet-BC在ImageNet分类任务上与最先进的ResNet架构相比，性能相当，但需要更少的参数和计算量。</li>
</ul>
</li>
</ul>
<h3 id="Q1-论文试图解决什么问题？"><a href="#Q1-论文试图解决什么问题？" class="headerlink" title="Q1 论文试图解决什么问题？"></a>Q1 论文试图解决什么问题？</h3><p>论文介绍了一种名为密集连接卷积网络（Dense Convolutional Network，简称 DenseNet）的新型结构。这一结构旨在解决卷积神经网络（CNN）中的一些核心问题，如梯度消失问题、特征传播的困难、特征重用的不足以及参数数量的增多。</p>
<h3 id="Q2-这是否是一个新的问题？"><a href="#Q2-这是否是一个新的问题？" class="headerlink" title="Q2 这是否是一个新的问题？"></a>Q2 这是否是一个新的问题？</h3><p>DenseNet提供了一种新的解决方案，通过密集连接每一层到其它所有层来解决这些问题。</p>
<h3 id="Q3-这篇文章要验证一个什么科学假设？"><a href="#Q3-这篇文章要验证一个什么科学假设？" class="headerlink" title="Q3 这篇文章要验证一个什么科学假设？"></a>Q3 这篇文章要验证一个什么科学假设？</h3><p>文章的科学假设是，通过在网络中引入密集连接，可以增强特征传播，减轻梯度消失问题，增强特征重用，并显著减少参数数量。这种连接方式被认为可以提高准确性并降低训练成本。</p>
<h3 id="Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？"><a href="#Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？" class="headerlink" title="Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？"></a>Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h3><p>相关研究包括不同类型的卷积神经网络结构，如LeNet、VGG等。这些研究可以归类为深度学习和计算机视觉领域内的网络架构研究。此论文的作者，如Gao Huang、Zhuang Liu、Laurens van der Maaten和Kilian Q. Weinberger等，都是该领域值得关注的研究员。</p>
<h3 id="Q5-论文中提到的解决方案之关键是什么？"><a href="#Q5-论文中提到的解决方案之关键是什么？" class="headerlink" title="Q5 论文中提到的解决方案之关键是什么？"></a>Q5 论文中提到的解决方案之关键是什么？</h3><p>解决方案的关键在于每一层与其它所有层之间的密集连接。这种连接方式使得每一层的特征图都作为其后每一层的输入，并且自身的特征图也作为所有后续层的输入。这增强了特征传播和重用，降低了参数数量。</p>
<h3 id="Q6-论文中的实验是如何设计的？"><a href="#Q6-论文中的实验是如何设计的？" class="headerlink" title="Q6 论文中的实验是如何设计的？"></a>Q6 论文中的实验是如何设计的？</h3><p>论文评估了DenseNet在四个竞争激烈的对象识别基准任务（CIFAR-10、CIFAR-100、SVHN和ImageNet）上的性能。实验设计用于比较DenseNet与现有最先进方法的性能，以证明其优越性和效率。</p>
<h3 id="Q7-用于定量评估的数据集是什么？代码有没有开源？"><a href="#Q7-用于定量评估的数据集是什么？代码有没有开源？" class="headerlink" title="Q7 用于定量评估的数据集是什么？代码有没有开源？"></a>Q7 用于定量评估的数据集是什么？代码有没有开源？</h3><p>用于定量评估的数据集包括CIFAR-10、CIFAR-100、SVHN和ImageNet。论文中提到了代码和预训练模型已在GitHub上开源。</p>
<h3 id="Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？"><a href="#Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？" class="headerlink" title="Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？"></a>Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？</h3><p>是的，论文中的实验结果显示DenseNets在多个基准任务上实现了显著改进，证明了其科学假设的有效性，即密集连接可以提高性能并降低计算成本。</p>
<h3 id="Q9-这篇论文到底有什么贡献？"><a href="#Q9-这篇论文到底有什么贡献？" class="headerlink" title="Q9 这篇论文到底有什么贡献？"></a>Q9 这篇论文到底有什么贡献？</h3><p>这篇论文的主要贡献是引入了DenseNet结构，这一新型结构通过密集连接解决了传统卷积神经网络中的一些问题，提高了准确性并降低了训练成本。它为深度学习和计算机视觉领域提供了一种新的研究方向。</p>
<h3 id="Q10-下一步呢？有什么工作可以继续深入？"><a href="#Q10-下一步呢？有什么工作可以继续深入？" class="headerlink" title="Q10 下一步呢？有什么工作可以继续深入？"></a>Q10 下一步呢？有什么工作可以继续深入？</h3><p>下一步的工作可能包括将DenseNet结构应用于更多类型的任务和领域，如自然语言处理、医学图像分析等。此外，也可以探索更多优化DenseNet结构的方法，如更高效的训练策略、更精细的连接模式等，以进一步提高其性能和通用性。</p>
<h3 id="密集连接放到transformer"><a href="#密集连接放到transformer" class="headerlink" title="密集连接放到transformer"></a>密集连接放到transformer</h3><h1 id="论文2-Pruning-Filters-for-Efficient-ConvNets-高效卷积神经网络的滤波器剪枝"><a href="#论文2-Pruning-Filters-for-Efficient-ConvNets-高效卷积神经网络的滤波器剪枝" class="headerlink" title="论文2 Pruning Filters for Efficient ConvNets (高效卷积神经网络的滤波器剪枝)"></a>论文2 Pruning Filters for Efficient ConvNets (高效卷积神经网络的滤波器剪枝)</h1><h1 id="Basic-Information-1"><a href="#Basic-Information-1" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Pruning Filters for Efficient ConvNets (高效卷积神经网络的滤波器剪枝)</li>
<li>Authors: Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf</li>
<li>Affiliation: University of Maryland (马里兰大学), NEC Labs America (NEC美国实验室)</li>
<li>Keywords: CNNs, computation costs, parameter storage costs, pruning filters, convolutional layers, accuracy, acceleration, sparsity, BLAS libraries</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.08710">Paper</a>, [GitHub: None]</li>
<li>16年8月</li>
</ul>
<h2 id="核心思想：用weight值的大小来评判filter的重要性，对于一个filter，对所有weight的绝对值求和-求L1范数-，作为该filter的评价指标，排序结果并将一层中值低的filter裁掉。属于Filters剪枝"><a href="#核心思想：用weight值的大小来评判filter的重要性，对于一个filter，对所有weight的绝对值求和-求L1范数-，作为该filter的评价指标，排序结果并将一层中值低的filter裁掉。属于Filters剪枝" class="headerlink" title="核心思想：用weight值的大小来评判filter的重要性，对于一个filter，对所有weight的绝对值求和(求L1范数)，作为该filter的评价指标，排序结果并将一层中值低的filter裁掉。属于Filters剪枝"></a>核心思想：用weight值的大小来评判filter的重要性，对于一个filter，对所有weight的绝对值求和(求L1范数)，作为该filter的评价指标，排序结果并将一层中值低的filter裁掉。属于Filters剪枝</h2><h1 id="论文简要-1"><a href="#论文简要-1" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>本研究提出了一种用于卷积神经网络（CNNs）的加速方法，通过剪枝对输出准确性影响较小的滤波器，从而显著降低计算成本。</li>
</ul>
<h1 id="背景信息-1"><a href="#背景信息-1" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: CNNs在各种应用中取得了成功，但计算和参数存储成本也显著增加。过去的研究主要集中在减少这些开销，包括剪枝和压缩各层的权重，以减少计算和存储成本，同时保持原始准确性。</li>
<li>过去方案: 过去的剪枝方法主要集中在剪枝权重，但这种方法在卷积层中可能无法充分减少计算成本，因为剪枝网络中存在不规则的稀疏性。此外，这些方法通常需要支持稀疏卷积库，而本研究提出的方法不需要，并且可以与现有的高效BLAS库一起使用。</li>
<li>论文的Motivation: 鉴于过去的剪枝方法存在的问题，本研究提出了一种新的滤波器剪枝方法，通过剪枝对输出准确性影响较小的滤波器，从而显著降低计算成本。与剪枝权重不同，这种方法不会导致稀疏连接模式，因此不需要稀疏卷积库的支持，并且可以与现有的高效BLAS库一起使用。通过实验证明，即使是简单的滤波器剪枝技术也可以在保持准确性的同时，将VGG-16的推理成本降低高达34%，将ResNet-110的推理成本降低高达38%。</li>
</ul>
<h1 id="方法-1"><a href="#方法-1" class="headerlink" title="方法:"></a>方法:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604181.png" alt="003b8469928619fec974c021af486830_2_Figure_1"></p>
<p>1.PRUNING FILTERS AND FEATURE MAPS<br>n表示的是第个卷积层的输入通道数，ni+1表示的是第i+1个filter，(hwi)表示的是输入featuremap的尺寸大小，单个filter(3D)的大<br>小表示为(k，k，ni)，总的filters表示为(ni+1，k，k，ni)，那么可以得到:一个feature map经过总的filters得到输出feature map后，需要进<br>行的计算量是ni<em>ni+1 <em>k</em>k</em>hi+1 <em>Wi+1，对应下图的kernel matrix。<br>那么，根据图片所示，<br>如果剪掉一个 feature map，可以直接减少ni</em>k<em>hi+1</em>Wi+1个运算(总的有ni+1个卷积核，输出通道数少了一个)。<br>输出 feature map 的减少一个，就会导致后面附加移除ni+2<em>k”</em>hi+2*wi+2个运算(输入通道数少一个，共有ni+1个输入通道<br>数)。所以减少m个 feature maps 可以减少m&#x2F;ni+1的计算量。图中的 kernel matrix每一列表示一个3Dfilter，一个3Dfilter会有一个feature map输出。</p>
<ol start="2">
<li>DETERMINING WHICH FILTERS TO PRUNE WITHIN A SINGLE LAYER<br>第i个卷积层剪掉m个卷积核的过程算法流程如下:<br>1.对每个卷积核，计算它的权重绝对值之和s。<br>2.根据s结果排序。<br>3.将m个权重绝对值之和最小的卷积核以及对应的 feature maps 剪掉。下一个卷积层中与剪掉的 feature maps 相关的核也要移除。<br>4.一个对干策i层和第i+1层的新的权重矩阵被创建，并目剩下的权重参数被复制到新模型中。 <img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604492.png" alt="image-20230812124443795"></li>
<li>DETERMINING SINGLE LAYER’S SENSITIVITY TO PRUNING<br>为了弄清楚每层的敏感度，论文对每一层独立剪枝并在验证集上对剪枝后的网络进行评估，Fiqure2(b)展示了结果，很明显斜率比较平缓的层对剪枝的敏感度更高，我们根据经验来决定对每一层的卷积核进行剪枝，对于深度网络(如VGG-16或ResNets)，我们观察到同一stage(相同尺寸的特征图)对应的层对剪枝的敏感度相似，为了避免引入layer-wise meta-parameters，我们对于同一stage的所有层使用相同的剪枝比例。对于那些敏感度高的层，我们剪枝时比例很小，甚至完全不进行剪枝。</li>
<li>Puring filters across multiple layers<br>之前的工作是逐层剪枝，然后重复训练来恢复精度。然而，理解如何能一次性对多层进行剪枝是非常有必要的:<br>1.对于深度网络，逐层剪枝再训练太耗时;<br>2.整体剪枝的方法提供给网络稳健性的一个全面视野，从而导致产生一个更小的网络;<br>3.对于复杂的网络，一个整体的方法很有必要，比如对于ResNet，对恒等映射特征图或者每个残差模块的第二个层剪枝会导致额外层的修剪:为了对多层同时剪枝，我们考虑了两个策略:<br>  4.每一层独立剪枝，即在计算(求权重绝对值之和)时不考虑上一层的修剪情况，所以计算时下图中的黄点仍然参与计算;<br>   5.贪心策略，计算时不计算已经修剪过的，即黄点不参与计算;</li>
</ol>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604736.png" alt="image-20230812124802841"></p>
<h3 id="1-连续层的剪枝："><a href="#1-连续层的剪枝：" class="headerlink" title="1. 连续层的剪枝："></a>1. <strong>连续层的剪枝</strong>：</h3><ul>
<li>这里涉及的是一种剪枝策略，考虑了卷积神经网络中连续层之间的关系。剪枝一个层的滤波器可能会影响下一个层的计算，因为滤波器之间存在依赖关系。</li>
</ul>
<h3 id="2-独立剪枝策略："><a href="#2-独立剪枝策略：" class="headerlink" title="2. 独立剪枝策略："></a>2. <strong>独立剪枝策略</strong>：</h3><ul>
<li>这个策略在计算每个滤波器的重要性时不考虑前一层已经被剪枝的滤波器。</li>
<li>例如，假设我们正在查看三个连续的层，用 ��,��+1,��+2<em>n**i</em>,<em>n**i</em>+1,<em>n**i</em>+2 来表示每层的滤波器数量。在计算第 �+1<em>i</em>+1 层的滤波器重要性时，这个策略不考虑第 �<em>i</em> 层哪些滤波器被剪除（蓝色标记）。</li>
<li>由于不考虑前一层的剪枝，所以某些卷积核权重（黄色标记）仍然会被包括在计算中。</li>
</ul>
<h3 id="3-贪婪剪枝策略："><a href="#3-贪婪剪枝策略：" class="headerlink" title="3. 贪婪剪枝策略："></a>3. <strong>贪婪剪枝策略</strong>：</h3><ul>
<li>与独立剪枝策略不同，贪婪剪枝策略在计算滤波器重要性时会考虑前一层已经剪除的滤波器。</li>
<li>这意味着如果前一层的某个特征映射已经被剪除，那么在下一层的计算中，与该特征映射相对应的卷积核不会被计算。</li>
</ul>
<h3 id="4-结果的卷积核矩阵："><a href="#4-结果的卷积核矩阵：" class="headerlink" title="4. 结果的卷积核矩阵："></a>4. <strong>结果的卷积核矩阵</strong>：</h3><ul>
<li>无论采用哪种策略，剪枝过程都将导致新的卷积核矩阵大小为 (��+1−1)×(��+2−1)(<em>n**i</em>+1−1)×(<em>n**i</em>+2−1)。实测第二种策略精度会更高</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604371.png" alt="image-20230812125105617"></p>
<p>这段话讨论了在卷积神经网络中剪枝残差块的特殊情况。残差块是一种特定的网络结构，它包括一个或多个卷积层和一个称为“快捷路径”或“投影快捷路径”的特殊连接。我们来详细解释这段文字的含义：</p>
<h3 id="1-残差块和投影快捷路径："><a href="#1-残差块和投影快捷路径：" class="headerlink" title="1. 残差块和投影快捷路径："></a>1. <strong>残差块和投影快捷路径</strong>：</h3><ul>
<li>残差块包括一系列卷积层，这些层的输出与输入相加，形成“残差”连接。</li>
<li>投影快捷路径是一种特殊的连接，它可以改变输入的维度以匹配残差块内层的输出维度。通常通过卷积操作实现。</li>
</ul>
<h3 id="2-剪枝残差块的第一层："><a href="#2-剪枝残差块的第一层：" class="headerlink" title="2. 剪枝残差块的第一层："></a>2. <strong>剪枝残差块的第一层</strong>：</h3><ul>
<li>这段文字指出，残差块的第一层可以“无限制”地剪枝。也就是说，您可以自由选择要剪除的滤波器，而不必担心会影响其他部分的网络。</li>
</ul>
<h3 id="3-剪枝残差块的第二层："><a href="#3-剪枝残差块的第二层：" class="headerlink" title="3. 剪枝残差块的第二层："></a>3. <strong>剪枝残差块的第二层</strong>：</h3><ul>
<li><p>残差块的第二层的剪枝与投影快捷路径的剪枝结果有关。</p>
</li>
<li><p>这里的逻辑是，由于投影快捷路径直接与残差块的第二层连接，因此如果投影快捷路径中的某个滤波器被剪除（例如，由于被认为不重要），那么残差块的第二层中与该滤波器相对应的滤波器也应被剪除。</p>
</li>
<li><p>这些要剪除的滤波器被标记为绿色。</p>
</li>
<li><p>a. 理论背景:</p>
<ul>
<li>本文讨论了卷积神经网络（CNNs）的计算和参数存储成本的增加以及需要高效和小型网络尺寸的需求。还强调了减少处理大量图像的网络服务的推理时间的重要性。</li>
</ul>
</li>
<li><p>b. 技术路线:</p>
<ul>
<li>本文提到了减少CNNs中计算成本的模型压缩和技术的先前研究。其中包括权重修剪、低秩逼近、基于FFT的卷积和量化等技术。还提到了其他方法，这些方法专注于去除冗余的特征图或使用稀疏约束训练紧凑的CNNs。</li>
</ul>
</li>
</ul>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><p><img src="/../../../../doc/%E5%9D%9A%E6%9E%9C%E4%BA%91%E6%96%87%E6%A1%A3/202308/%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3.assets/image-20230812124123590.png" alt="image-20230812124123590"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211604674.png" alt="image-20230812124135465"></p>
<ul>
<li>b. 详细的实验结果:<ul>
<li>本文提出的滤波器修剪技术在VGG-16和ResNet-110模型上的性能进行了实验。结果显示，该技术在CIFAR10数据集上可以将推理成本降低高达34%（对于VGG-16）和38%（对于ResNet-110），同时保持接近原始准确性。即使是相对于AlexNet或VGGNet而言参数和推理成本较少的ResNets，也可以在不牺牲太多准确性的情况下实现30%的浮点运算（FLOP）的减少。此外，本文还发现，修剪第一层残差块比修剪第二层更有效地减少总体FLOP。最小滤波器修剪优于随机滤波器修剪和具有最大ℓ1范数的滤波器修剪。最小滤波器修剪还优于使用各种标准（如σmean-mean、σmean-ℓ1、σmean-ℓ2和σvar-ℓ2）的基于激活的特征图修剪。综上所述，本文提出的方法可以在不显著损失准确性的情况下，将VGGNet和深度ResNets的FLOP减少约30%。</li>
</ul>
</li>
</ul>
<h3 id="Q1-论文试图解决什么问题？-1"><a href="#Q1-论文试图解决什么问题？-1" class="headerlink" title="Q1 论文试图解决什么问题？"></a>Q1 论文试图解决什么问题？</h3><p>论文关注卷积神经网络（CNN）的计算和参数存储成本的增加问题。作者提出了一种剪枝方法，通过从CNN中剪除滤波器来加速计算，同时保持原始精度。</p>
<h3 id="Q2-这是否是一个新的问题？-1"><a href="#Q2-这是否是一个新的问题？-1" class="headerlink" title="Q2 这是否是一个新的问题？"></a>Q2 这是否是一个新的问题？</h3><p>该问题不是全新的。在卷积神经网络的训练和部署中，计算和存储效率一直是一个关键问题。过去已有研究人员尝试通过剪枝和压缩权重来减轻这些开销。</p>
<h3 id="Q3-这篇文章要验证一个什么科学假设？-1"><a href="#Q3-这篇文章要验证一个什么科学假设？-1" class="headerlink" title="Q3 这篇文章要验证一个什么科学假设？"></a>Q3 这篇文章要验证一个什么科学假设？</h3><p>文章的科学假设是，与基于权重大小的剪枝方法相比，通过剪除滤波器来减少计算成本更为有效，因为后者可以减少卷积层的计算成本，而不仅仅是全连接层。</p>
<h3 id="Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？-1"><a href="#Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？-1" class="headerlink" title="Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？"></a>Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h3><ul>
<li>早期的工作由Le Cun等人于1989年引入，提出了名为“Optimal Brain Damage”的剪枝方法，采用理论支持的显着性度量来剪枝权重。</li>
<li>Hassibi和Stork于1993年提出了“Optimal Brain Surgeon”，以根据二阶导数信息确定并去除不重要的权重。</li>
<li>Mariet和Sra（2016）通过识别不需要重新训练的神经元的子集来减少网络冗余。但是，这种方法仅在全连接层上操作，并引入稀疏连接。</li>
<li>过去的工作还提出了通过将权重矩阵表示为两个较小矩阵的低秩乘积来近似卷积操作，以降低卷积层的计算成本（参见Denil et al. (2013); Jaderberg et al. (2014); Zhang et al. (2015b;a); Tai et al. (2016); Ioannou et al. (2016)）。</li>
<li>其他减少卷积开销的方法包括使用基于FFT的卷积（Mathieu等人）。</li>
</ul>
<p>这些研究可以归类为：</p>
<ul>
<li>基于理论的权重剪枝方法</li>
<li>网络冗余减少</li>
<li>低秩近似和特殊卷积方法</li>
</ul>
<h3 id="Q5-论文中提到的解决方案之关键是什么？-1"><a href="#Q5-论文中提到的解决方案之关键是什么？-1" class="headerlink" title="Q5 论文中提到的解决方案之关键是什么？"></a>Q5 论文中提到的解决方案之关键是什么？</h3><p>这篇论文提出的解决方案的关键要素包括：</p>
<ol>
<li><strong>剪枝滤波器</strong>：该方法专注于从CNN中剪除对输出精度影响较小的滤波器。</li>
<li><strong>减少计算成本</strong>：通过一起移除整个滤波器及其连接的特征映射，可以显著降低计算成本。</li>
<li><strong>避免稀疏连接</strong>：与剪枝权重不同，此方法不会导致稀疏连接模式。因此，它不需要稀疏卷积库的支持，可以与现有的用于密集矩阵乘法的高效BLAS库一起工作。</li>
<li><strong>简单的剪枝技术</strong>：通过重新训练网络，即使是简单的滤波器剪枝技术也可以在几乎恢复到原始精度的同时，为VGG-16和ResNet-110等模型减少多达34%和38%的推理成本。</li>
</ol>
<p>这些关键要素共同实现了在保持精度的同时提高卷积神经网络的计算效率。</p>
<h3 id="Q6-论文中的实验是如何设计的？-1"><a href="#Q6-论文中的实验是如何设计的？-1" class="headerlink" title="Q6 论文中的实验是如何设计的？"></a>Q6 论文中的实验是如何设计的？</h3><p>实验部分描述了论文中进行的实验设计：</p>
<ol>
<li><strong>剪枝的网络类型</strong>：实验中剪枝了两种类型的网络：简单的CNN（例如VGG-16在CIFAR-10上）和残差网络（例如ResNet-56&#x2F;110在CIFAR-10上，ResNet-34在ImageNet上）。</li>
<li><strong>挑战</strong>：与常用于演示模型压缩的AlexNet或VGG（在ImageNet上）不同，VGG（在CIFAR-10上）和残差网络在全连接层中的参数较少。因此，从这些网络中剪去大量参数是具有挑战性的。</li>
<li><strong>实现</strong>：滤波器剪枝方法在Torch7中实现。剪枝滤波器后，将创建一个具有较少滤波器的新模型，并将修改层的剩余参数以及未受影响层的参数复制到新模型中。</li>
<li><strong>后续处理</strong>：如果剪除了卷积层，则后续批量归一化层的权重也会被移除。</li>
<li><strong>基线精度</strong>：为获得每个网络的基线精度，从头开始训练每个模型，并遵循与ResNet相同的预处理和超参数。</li>
<li><strong>重新训练</strong>：使用恒定学习速率进行重新训练，并减小剪枝后网络的学习速率。</li>
</ol>
<h3 id="Q7-用于定量评估的数据集是什么？代码有没有开源？-1"><a href="#Q7-用于定量评估的数据集是什么？代码有没有开源？-1" class="headerlink" title="Q7 用于定量评估的数据集是什么？代码有没有开源？"></a>Q7 用于定量评估的数据集是什么？代码有没有开源？</h3><p>用于定量评估的数据集包括CIFAR-10和ImageNet。开源</p>
<h3 id="Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？-1"><a href="#Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？-1" class="headerlink" title="Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？"></a>Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？</h3><p>实验结果部分给出了针对不同模型和剪枝方案的详细结果。以下是一些关键发现：</p>
<ul>
<li><strong>VGG-16剪枝</strong>：针对CIFAR-10数据集的VGG-16模型进行剪枝后，可以实现34.2%的FLOP（浮点操作数）剪减和64.0%的参数剪减，同时保持了类似的错误率（6.75%原始错误率与6.60%剪枝后错误率）。</li>
<li><strong>ResNet-56剪枝</strong>：针对CIFAR-10数据集的ResNet-56模型进行剪枝后，可以实现27.6%的FLOP剪减和13.7%的参数剪减，同时保持了相似的错误率。</li>
<li><strong>ResNet-110剪枝</strong>：针对CIFAR-10数据集的ResNet-110模型进行剪枝后，可以实现38.6%的FLOP剪减和32.4%的参数剪减，同时保持了相似的错误率。</li>
<li><strong>ResNet-34剪枝</strong>：针对ImageNet数据集的ResNet-34模型进行剪枝后，可以实现24.2%的FLOP剪减和10.8%的参数剪减，但错误率略有上升。</li>
</ul>
<p>这些实验结果支持了论文的科学假设，表明通过剪除对输出精度影响较小的滤波器，可以显著降低计算成本，同时基本保持原始精度。同时，该方法不导致稀疏连接模式，与现有的高效BLAS库兼容。</p>
<h3 id="Q9-这篇论文到底有什么贡献？-1"><a href="#Q9-这篇论文到底有什么贡献？-1" class="headerlink" title="Q9 这篇论文到底有什么贡献？"></a>Q9 这篇论文到底有什么贡献？</h3><p>这篇论文的主要贡献如下：</p>
<ol>
<li><strong>滤波器剪枝方法</strong>：提出了一种剪除CNN中滤波器的方法，以降低计算和存储成本，而不损失原始精度。</li>
<li><strong>有效的实现</strong>：该方法不引入稀疏连接模式，可以与现有的密集矩阵乘法库一起工作。</li>
<li><strong>广泛适用</strong>：该方法在VGG和ResNet等不同类型的网络上进行了验证。</li>
<li><strong>实验验证</strong>：通过广泛的实验，展示了该方法在不同网络和数据集上的有效性。</li>
</ol>
<h3 id="Q10-下一步呢？有什么工作可以继续深入？-1"><a href="#Q10-下一步呢？有什么工作可以继续深入？-1" class="headerlink" title="Q10 下一步呢？有什么工作可以继续深入？"></a>Q10 下一步呢？有什么工作可以继续深入？</h3><ol>
<li><strong>更复杂的剪枝策略</strong>：探索基于更复杂的标准和方法的剪枝策略，以实现更精确的滤波器选择。</li>
<li><strong>不同类型的网络</strong>：将方法扩展到更多种类和结构的卷积神经网络上。</li>
<li><strong>实时和嵌入式应用</strong>：针对实时和嵌入式应用的特定需求，优化和调整剪枝方法。</li>
<li><strong>与其他压缩技术结合</strong>：探索将滤波器剪枝与其他模型压缩和优化技术结合的可能性，以实现更全面的优化。</li>
</ol>
<h1 id="论文3：Filter-Pruning-via-Geometric-Median-for-Deep-Convolutional-Neural-Networks-Acceleration-基于几何中位数的深度卷积神经网络加速的滤波器剪枝"><a href="#论文3：Filter-Pruning-via-Geometric-Median-for-Deep-Convolutional-Neural-Networks-Acceleration-基于几何中位数的深度卷积神经网络加速的滤波器剪枝" class="headerlink" title="论文3：Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration (基于几何中位数的深度卷积神经网络加速的滤波器剪枝)"></a>论文3：Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration (基于几何中位数的深度卷积神经网络加速的滤波器剪枝)</h1><h1 id="Basic-Information-2"><a href="#Basic-Information-2" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration (基于几何中位数的深度卷积神经网络加速的滤波器剪枝)</li>
<li>Authors: Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, Yi Yang</li>
<li>Affiliation: CAI, University of Technology Sydney (悉尼科技大学)</li>
<li>Keywords: filter pruning, deep convolutional neural networks, acceleration, geometric median</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.00250">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/he-y/filter-pruning-geometric-median">GitHub</a></li>
<li>18年11月</li>
</ul>
<h1 id="论文简要-2"><a href="#论文简要-2" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125926446.png" alt="image-20230812125926446"></p>
<ul>
<li>上图1是典型的滤波器剪枝的方法。现有的方法大都是基于范数的标准进行剪枝。小范数不重要，认为滤波器的范数(p-norm)越小，携带的信息越少，于是对网络贡献越小，那么这些滤波器可以去掉而不会严重影响网络的性能。于是我们根据滤波器的p-norm进行重要性排序，规定一个阈值，并且删除p-norm小于阈值的滤波器。</li>
<li>这个浅蓝色区域是一种理想的滤波器范数分布，而基于这种范数标准的剪枝实际隐含了两个条件</li>
<li>:一是范数值的标准差尽可能的大，也就是分布相对稀疏，这样的目的是可以更容易的找到一个成值进行剪枝;</li>
<li>第二个条件是小范数的值应该尽可能的小，理想状况下是趋近于0，这样能保证小范数的滤波器对网络的贡献很小，去掉他对模型的影响不大。</li>
<li>本文提出了一种基于几何中位数的滤波器剪枝方法，用于压缩深度卷积神经网络模型，无需满足之前方法中的两个要求，通过在两个图像分类基准上的实验证明了其有效性和优势。</li>
</ul>
<h1 id="背景信息-2"><a href="#背景信息-2" class="headerlink" title="背景信息:"></a>背景信息:</h1><p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812130521561.png" alt="image-20230812130521561"></p>
<p>如这两幅图所示，浅蓝色区域仍是理想情况下的范数分布，绿色区域是实际的滤波器范数分布。a名我们看到绿色区域这种情况下的分布范数的标准差太小，这样出现的问题是很多滤波器有相似的重要性，很难确定一个合适的阈值(A smallnorm deviation leads to a small search space, which makes it diffcult to find an appropriate threshold to select fifilters toprune.)</p>
<p>b图我们看到最小范数的值仍然很大，也就是说他对网络的贡献度不能被忽视。</p>
<p>作者为了证明在有些场景中上面提到的两个条件并不能很好的满足，做了非常充分的实验证明，分别用resnet110在cifar10和resnet18在ILSVRC2012数据集上进行实验:</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812130609383.png" alt="image-20230812130609383"></p>
<p>绿色小竖线表示范数的观察值，蓝色线表示核的分布预测。b图我们看到在resnet110的第一层有大量的滤波器，它们的范数集中在10x6的量级上。c图展示滤波器的范数范围跨度只有0.3，远远小于a图第一层范数1.7的跨度，这说明同一个网络不同层的滤波器范数有的标差较大，有的就偏小。同样的看g图，resnet18最后一层的滤波器最小范数大约是0.8，与e图展示的第一层比起来并没有接近0，通过分析实际的预训练网络，发现这些网络确实出现了上述的情况。</p>
<ul>
<li>论文背景: 深度卷积神经网络的深度和宽度使得计算成本昂贵，难以在移动设备上部署。因此，有必要降低计算成本但保持高准确性的深度卷积神经网络模型。</li>
<li>过去方案: 过去的滤波器剪枝方法通常使用“较小范数-较不重要”准则来剪枝具有较小范数值的滤波器。然而，这种准则的有效性取决于两个要求：滤波器范数的偏差应该很大，滤波器的最小范数应该很小。然而，这两个要求并不总是满足的。</li>
<li>论文的Motivation: 鉴于过去方法的局限性，本文提出了一种新的滤波器剪枝方法，名为Filter Pruning via Geometric Median (FPGM)，通过剪枝具有冗余信息的滤波器来压缩CNN模型。与之前的方法不同，FPGM选择具有最可替代贡献的滤波器进行剪枝，而不是剪枝相对不重要的滤波器。通过计算同一层中滤波器的几何中位数，FPGM可以用其余滤波器来表示附近的滤波器，因此剪枝这些滤波器不会对模型性能产生重大负面影响。与基于范数的准则不同，FPGM的性能不会因为无法满足范数准则的要求而下降。</li>
</ul>
<h1 id="方法-2"><a href="#方法-2" class="headerlink" title="方法:"></a>方法:</h1><h3 id="基于几何中位数的滤波器剪枝方法"><a href="#基于几何中位数的滤波器剪枝方法" class="headerlink" title="基于几何中位数的滤波器剪枝方法"></a>基于几何中位数的滤波器剪枝方法</h3><p>作者摒弃了这种基于范数标准剪枝的方法，提出了一种基于几何中心的滤波器评价指标。在某一层中，从全局考虑所有filter的关系。<br>这种方法的提出灵感来源于几何中心的思想。什么是几何中心?就是在一个d维的空间中，给定一个点集，a1.a2.a3.….an，在该空间中找到一个点x*，使得该点到各个点的距离和最小，就是这个d维空间的几何中心。</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131322763.png" alt="image-20230812131322763"></p>
<p>基本思想:几何中心是对于欧几里得空间的点的中心的一个估计。受此启发，作者把滤波器抽象成欧氏空间中的点，对于网络中的每一层，在该层的滤波器空间中，计算GM，也就是第1层的数据中心。如果某个滤波器接近于这个GM，可以认为这个源波器的信息跟其他源波器重合，甚至是几余的，于是可以去掉这个源波器而不对网络产生大的影响。去掉它后它的功能可以被其他滤波器代替。沿用求取普通几何中心的思想，</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131413844.png" alt="image-20230812131413844"></p>
<p>归根结底，作者是想在某一层中找到与几何中心xGM相近的滤波器，然后将他们剪枝。为了方便求解，作者假设第层的几何中心就位于第层的滤波器中，因为真实目的是要找到与几何中心最近的滤波器。此时(2)的优化问题就变为，从第层中所有的滤波器中寻找一个filter,确保其与剩余的filters的距离之和最小。</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131533751.png" alt="image-20230812131533751"></p>
<p>公式5-9看不懂</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131557560.png" alt="image-20230812131557560"></p>
<p>算法流程如下：</p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812131517584.png" alt="image-20230812131517584"></p>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ul>
<li><strong>训练数据</strong>：( X )。</li>
<li><strong>剪枝率</strong>：( P_i )。剪枝率确定了每一层应剪去的滤波器数量的百分比。</li>
</ul>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><ol>
<li><strong>模型参数初始化</strong>：模型的权重参数被初始化为( W &#x3D; {W(i), 0 \leq i \leq L} )，其中( L )是网络的层数。</li>
</ol>
<h3 id="训练和剪枝过程"><a href="#训练和剪枝过程" class="headerlink" title="训练和剪枝过程"></a>训练和剪枝过程</h3><ol start="2">
<li><strong>开始训练周期</strong>：对于每个训练周期（从1到最大训练周期），执行以下步骤：<ul>
<li><strong>更新模型参数</strong>：根据训练数据( X )更新模型参数( W )。这一步通常涉及正向传播、损失计算、反向传播和权重更新。</li>
<li><strong>逐层剪枝</strong>：对于每一层（从1到( L )），执行以下剪枝步骤：<ul>
<li><strong>找到要剪去的滤波器</strong>：找到满足方程4的( N_{i+1} \times P_i )个滤波器。方程4通常涉及使用几何中位数来确定要剪去的滤波器。</li>
<li><strong>零化选定的滤波器</strong>：将选定的滤波器设置为零，从而实现剪枝。</li>
</ul>
</li>
<li><strong>重复以上步骤</strong>：继续训练和剪枝，直到达到最大训练周期。</li>
</ul>
</li>
</ol>
<h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><ol start="3">
<li><strong>获取紧凑模型</strong>：从最终的权重参数( W )中获取紧凑模型( W^* )。紧凑模型不包括已剪去（零化）的滤波器。</li>
</ol>
<h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><ul>
<li><strong>输出紧凑模型及其参数</strong>：最终剪枝后的模型及其参数( W^* )。</li>
</ul>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>基于几何中位数的滤波器剪枝（FPGM）是一个迭代过程，其中训练和剪枝在每个训练周期内交替进行。通过这种方式，算法可以在训练过程中逐渐剪去滤波器，从而在保持网络性能的同时减少模型大小。最终的紧凑模型包括剪枝后保留的所有滤波器和参数。</p>
<ul>
<li>a. 理论背景:<ul>
<li>本文提出了一种新的滤波器修剪方法，称为几何中位数滤波器修剪（FPGM），用于压缩深度卷积神经网络模型。与以往基于范数值修剪滤波器的方法不同，FPGM通过修剪冗余的滤波器来实现。在两个图像分类基准测试上的实验结果表明，FPGM在最小精度损失的情况下实现了显著的模型压缩。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>本文提出的FPGM方法通过计算同一层内滤波器的几何中位数（GM），并修剪与几何中位数最远的滤波器。与以往使用范数值作为标准的方法不同，FPGM不依赖于滤波器范数来选择修剪的滤波器。</li>
</ul>
</li>
</ul>
<h1 id="结果-2"><a href="#结果-2" class="headerlink" title="结果:"></a>结果:</h1><p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125737860.png" alt="image-20230812125737860"></p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125805978.png" alt="image-20230812125805978"></p>
<p><img src="/../../../../doc/%25E5%259D%259A%25E6%259E%259C%25E4%25BA%2591%25E6%2596%2587%25E6%25A1%25A3/202308/%25E5%2589%25AA%25E6%259E%259D%25E7%259B%25B8%25E5%2585%25B3.assets/image-20230812125815070.png" alt="image-20230812125815070"></p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>本文在两个基准测试（CIFAR-10和ILSVRC-2012）上评估了FPGM算法。CIFAR-10数据集包含10个不同类别的60,000个32x32彩色图像，其中包括50,000个训练图像和10,000个测试图像。ILSVRC-2012数据集是一个大规模数据集，包含1.28百万个训练图像和50,000个验证图像，共有1,000个类别。实验设置包括指定两个数据集的训练设置，包括参数设置和训练计划。修剪设置涉及为每个层设置修剪率，确定要修剪的滤波器数量。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>实验结果显示了FPGM的有效性和效率。在应用于CIFAR-10和ILSVRC-2012两个图像分类基准测试时，FPGM在显著减少模型FLOPs的同时，几乎没有损失精度。例如，在CIFAR-10上，FPGM在ResNet-110上减少了超过52%的FLOPs，并且相对精度提高了2.69%。在ILSVRC-2012上，FPGM在ResNet-101上减少了超过42%的FLOPs，而top-5精度没有下降。</li>
</ul>
</li>
</ul>
<h3 id="Q1-论文试图解决什么问题？-2"><a href="#Q1-论文试图解决什么问题？-2" class="headerlink" title="Q1 论文试图解决什么问题？"></a>Q1 论文试图解决什么问题？</h3><p>这篇论文试图解决深度卷积神经网络（CNN）的滤波器剪枝问题。以前的工作使用了“小范数-次要重要”的准则来剪去具有较小范数值的滤波器。本文分析了基于范数的准则，并指出其有效性取决于两个并不总是满足的要求：滤波器的范数偏差应该大，滤波器的最小范数应该小。为了解决这个问题，论文提出了一种名为“基于几何中位数的滤波器剪枝”（FPGM）的新方法。</p>
<h3 id="Q2-这是否是一个新的问题？-2"><a href="#Q2-这是否是一个新的问题？-2" class="headerlink" title="Q2 这是否是一个新的问题？"></a>Q2 这是否是一个新的问题？</h3><p>滤波器剪枝并不是一个全新的问题，已有许多工作致力于深度神经网络的压缩和加速。然而，本文提出了一种新的剪枝方法，并分析了以前基于范数的剪枝准则的局限性。</p>
<h3 id="Q3-这篇文章要验证一个什么科学假设？-2"><a href="#Q3-这篇文章要验证一个什么科学假设？-2" class="headerlink" title="Q3 这篇文章要验证一个什么科学假设？"></a>Q3 这篇文章要验证一个什么科学假设？</h3><p>这篇文章的科学假设是，通过使用几何中位数作为剪枝准则，可以剪去冗余的滤波器，从而压缩CNN模型，无论滤波器的范数偏差大小或滤波器的最小范数大小如何。这种方法的目的是克服基于范数的剪枝方法的局限性。</p>
<h3 id="Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？-2"><a href="#Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？-2" class="headerlink" title="Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？"></a>Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h3><p>相关的研究主要集中在加速卷积神经网络（CNN）的方法上，可以大致分为四个类别：</p>
<ol>
<li><strong>矩阵分解</strong>：使用数学方法分解矩阵以降低计算复杂度。</li>
<li><strong>低精度权重</strong>：使用低精度数值表示权重以减少存储和计算需求。</li>
<li><strong>知识蒸馏</strong>：通过训练较小的模型来模仿大模型的行为。</li>
<li><strong>剪枝</strong>：移除神经网络中不必要的连接。剪枝方法可以进一步分为权重剪枝和滤波器剪枝。其中，滤波器剪枝不仅减少了设备上的存储使用，还降低了计算成本以加速推理。</li>
</ol>
<h3 id="Q5-论文中提到的解决方案之关键是什么？-2"><a href="#Q5-论文中提到的解决方案之关键是什么？-2" class="headerlink" title="Q5 论文中提到的解决方案之关键是什么？"></a>Q5 论文中提到的解决方案之关键是什么？</h3><p>论文提出了一种名为“基于几何中位数的滤波器剪枝”（FPGM）的新方法。关键之处在于：</p>
<ul>
<li><strong>基于几何中位数</strong>：使用几何中位数作为剪枝准则，可以剪去冗余的滤波器。</li>
<li><strong>克服范数局限性</strong>：与基于范数的方法不同，FPGM无需满足滤波器的范数偏差大或最小范数小的要求。</li>
<li><strong>压缩和加速</strong>：该方法旨在压缩CNN模型并加速推理过程。</li>
</ul>
<h3 id="Q6-论文中的实验是如何设计的？-2"><a href="#Q6-论文中的实验是如何设计的？-2" class="headerlink" title="Q6 论文中的实验是如何设计的？"></a>Q6 论文中的实验是如何设计的？</h3><p>论文的实验部分评估了基于几何中位数的滤波器剪枝（FPGM）方法在不同类型的网络和数据集上的效果。以下是实验设计的关键部分：</p>
<ul>
<li><p><strong>网络结构</strong>：对单分支网络（VGGNet）和多分支网络（ResNet）进行了评估。</p>
</li>
<li><p><strong>数据集</strong>：实验在两个基准数据集上进行，即CIFAR-10和ILSVRC-2012。</p>
</li>
<li><p>训练设置</p>
<p>：</p>
<ul>
<li>在CIFAR-10上，参数设置与先前的工作相同。</li>
<li>在ILSVRC-2012上，使用默认参数设置，并与PyTorch官方示例中的数据增强策略相同。</li>
</ul>
</li>
<li><p><strong>剪枝设置</strong>：分析了从头开始训练的模型与预训练模型的差异。对于从头开始的模型，使用正常的训练计划，无需额外的微调。</p>
</li>
</ul>
<h3 id="Q7-用于定量评估的数据集是什么？代码有没有开源？-2"><a href="#Q7-用于定量评估的数据集是什么？代码有没有开源？-2" class="headerlink" title="Q7 用于定量评估的数据集是什么？代码有没有开源？"></a>Q7 用于定量评估的数据集是什么？代码有没有开源？</h3><p>用于定量评估的数据集是CIFAR-10和ILSVRC-2012。开源</p>
<h3 id="Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？-2"><a href="#Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？-2" class="headerlink" title="Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？"></a>Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？</h3><p>成功评估基于几何中位数的滤波器剪枝方法的有效性和效率</p>
<h3 id="Q9-这篇论文到底有什么贡献？-2"><a href="#Q9-这篇论文到底有什么贡献？-2" class="headerlink" title="Q9 这篇论文到底有什么贡献？"></a>Q9 这篇论文到底有什么贡献？</h3><p>这篇论文的主要贡献如下：</p>
<ol>
<li><p><strong>分析现有方法的局限性</strong>：分析了基于范数的剪枝准则的局限性，并提出了两个不总是满足的要求。</p>
</li>
<li><p><strong>提出新的剪枝方法</strong>：介绍了一种新的滤波器剪枝方法，即基于几何中位数的滤波器剪枝（FPGM），旨在压缩CNN模型并加速推理。</p>
</li>
<li><p><strong>实验验证</strong>：通过在多个网络结构和数据集上的实验，评估了所提方法的有效性和效率。</p>
<h3 id="Q10-下一步呢？有什么工作可以继续深入？-2"><a href="#Q10-下一步呢？有什么工作可以继续深入？-2" class="headerlink" title="Q10 下一步呢？有什么工作可以继续深入？"></a>Q10 下一步呢？有什么工作可以继续深入？</h3><ol>
<li><strong>扩展到其他网络结构</strong>：将FPGM方法扩展到其他复杂的网络结构，并评估其在不同任务和领域中的适用性。</li>
<li><strong>与其他压缩技术的结合</strong>：探讨如何将FPGM与其他模型压缩和加速技术（如量化、知识蒸馏等）结合使用。</li>
</ol>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%89%AA%E6%9E%9D%E7%9B%B8%E5%85%B3/" data-id="cllkmcmf5000gksu7f0li9ao8" data-title="8月第二周剪枝相关" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AF%86%E9%9B%86%E8%BF%9E%E6%8E%A5%E3%80%81%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%89%AA%E6%9E%9D/" rel="tag">密集连接、滤波器剪枝</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-8月第一周自然语言蒸馏" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%92%B8%E9%A6%8F/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T07:58:35.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%92%B8%E9%A6%8F/">8月第一周自然语言大模型蒸馏</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="8月第一周自然语言蒸馏论文"><a href="#8月第一周自然语言蒸馏论文" class="headerlink" title="8月第一周自然语言蒸馏论文"></a>8月第一周自然语言蒸馏论文</h1><h1 id="Title-Knowledge-Distillation-of-Large-Language-Models-大型语言模型的知识蒸馏"><a href="#Title-Knowledge-Distillation-of-Large-Language-Models-大型语言模型的知识蒸馏" class="headerlink" title="Title: Knowledge Distillation of Large Language Models (大型语言模型的知识蒸馏)"></a>Title: Knowledge Distillation of Large Language Models (大型语言模型的知识蒸馏)</h1><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Knowledge Distillation of Large Language Models (大型语言模型的知识蒸馏)</li>
<li>Authors: Yuxian Gu, Li Dong, Furu Wei, Minlie Huang</li>
<li>Affiliation: The CoAI Group, Tsinghua University (清华大学)</li>
<li>Keywords: Knowledge Distillation, Large Language Models, Generative Language Models, Reverse Kullback-Leibler Divergence, Optimization Approach</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.08543v1">Paper</a>, <a target="_blank" rel="noopener" href="https://aka.ms/MiniLLM">GitHub Code</a></li>
</ul>
<h1 id="论文简要-第一篇自然语言蒸馏-bert蒸馏？"><a href="#论文简要-第一篇自然语言蒸馏-bert蒸馏？" class="headerlink" title="论文简要 :第一篇自然语言蒸馏 bert蒸馏？"></a>论文简要 :第一篇自然语言蒸馏 bert蒸馏？</h1><ul>
<li>本文提出了一种名为MINILLM的方法，用于从生成型大型语言模型中蒸馏出较小的语言模型。通过使用逆向Kullback-Leibler散度作为目标函数，并引入有效的优化方法，MINILLM模型在指令跟随任务中表现出更高的生成质量、更低的暴露偏差、更好的校准性和更高的长文本生成性能。</li>
</ul>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 随着大型语言模型的快速发展，知识蒸馏（KD）成为减少计算资源需求的一种常用技术。然而，过去的知识蒸馏方法主要应用于白盒分类模型或训练小模型以模仿ChatGPT等黑盒模型API。如何有效地从白盒生成型语言模型中蒸馏知识仍然未被充分探索，而随着生成型语言模型的繁荣，这变得越来越重要。</li>
<li>过去方案: 过去的知识蒸馏方法主要应用于白盒分类模型或训练小模型以模仿黑盒模型API。白盒知识蒸馏方法主要研究了小型语言理解模型，而对于生成型语言模型的白盒知识蒸馏尚未被探索。</li>
<li>论文的Motivation: 随着生成型语言模型的兴起，白盒知识蒸馏对于研究社区和行业部门变得更有价值，因为学生模型可以从白盒教师模型中获得更好的信号，从而潜在地实现更高的性能。然而，白盒知识蒸馏方法主要研究了小型语言理解模型，而对于生成型语言模型的白盒知识蒸馏尚未被探索。因此，本文旨在研究生成型语言模型的白盒知识蒸馏，并提出了一种新的方法MINILLM，通过使用逆向Kullback-Leibler散度作为目标函数，实现了更好的生成质量和性能。</li>
</ul>
<h1 id="方法-创新点1：正则化整体样本"><a href="#方法-创新点1：正则化整体样本" class="headerlink" title="方法: 创新点1：正则化整体样本"></a>方法: 创新点1：正则化整体样本</h1><h2 id="可能的工作：在视觉蒸馏上使用正则化减少过拟合增加泛化能力，针对序列蒸馏改进"><a href="#可能的工作：在视觉蒸馏上使用正则化减少过拟合增加泛化能力，针对序列蒸馏改进" class="headerlink" title="可能的工作：在视觉蒸馏上使用正则化减少过拟合增加泛化能力，针对序列蒸馏改进"></a>可能的工作：在视觉蒸馏上使用正则化减少过拟合增加泛化能力，针对序列蒸馏改进</h2><p>可能的工作2：法学大模型对垂直领域大模型建设，提出了一个新的方法，包括微调，提示词，langchain，本文提到的蒸馏</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308061123535.png" alt="image-20230806112311078"></p>
<blockquote>
<ol>
<li>序列级知识蒸馏（KD）(左图): 这种方法强迫学生模型直接从教师模型生成的序列中学习。前向 KLD (Kullback-Leibler Divergence，库尔巴克-莱布勒散度)用作衡量教师输出和学生输出分布之间的不相似度。<strong>这种方法就像是死记硬背，学生模型本质上是试图复制教师的确切输出序列。</strong>提示 𝒙 输入到两个模型中，然后计算教师输出 𝒚 (从 𝑝 中采样)和学生预测输出之间的差异，以调整学生模型参数（𝜃）。然而，这种方法可能导致学生在语言生成任务中过度估计某些区域，特别是当学生模型缺乏捕获所有教师模型输出分布细微差异的能力时。</li>
<li>MINILLM (右图): 相比之下，这种方法允许学生模型通过教师模型的反馈来改善其生成能力。与单纯模仿教师输出不同，这种方法旨在帮助学生模型更好地泛化。这是通过最小化反向 KLD 来实现的，本质上是使学生模型的输出分布尽可能地接近教师的，但这样可以使学生模型也能从自己的错误中学习。<strong>提示 𝒙 输入到两个模型中，然后计算教师输出 (从 𝑞! 中采样)和学生预测输出之间的差异，为学生模型提供学习的机会。</strong></li>
</ol>
</blockquote>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308061238004.png" alt="image-20230806123825947"></p>
<blockquote>
<p>输入包括以下元素：</p>
<ul>
<li>条件生成数据集D，包含提示和真实响应</li>
<li>预训练语料库DPT，包含长文档纯文本</li>
<li>教师模型，其输出分布为p</li>
<li>初始学生模型，其输出分布为qθ0</li>
</ul>
<p>输出为一个学生模型，其输出分布为qθ。</p>
<p>这个算法的步骤如下：</p>
<ol>
<li>首先在数据集D上对学生模型进行微调，并选择具有最低验证损失的模型θ。</li>
<li>重复以下步骤直到收敛：<ul>
<li>从数据集D中抽取一批提示，并从其中收集响应以得到S &#x3D; {(xm, ym)}M m&#x3D;1。</li>
<li>从DPT中抽取一个小批量D’PT &#x3D; {dm}M m&#x3D;1。</li>
<li>计算主损失梯度(∇J)Main，这个值取决于学生模型qθ和教师模型pe之间的概率分布差异，详见等式5和等式6。</li>
<li>计算正则化损失梯度(∇J)Reg，这个值是在所有可能的输出yt中，学生模型qθ和教师模型p的分布差异的梯度，详见等式3。</li>
<li>计算预训练损失的梯度∇LPT，这个值是学生模型qθ在预训练语料库D’PT上的对数概率的梯度。</li>
<li>更新模型参数θ，根据计算出的梯度值，来调整模型的参数。</li>
</ul>
</li>
</ol>
<p>这个算法的核心思想是在模型参数更新过程中引入了一个正则化项，旨在减小学生模型和教师模型在相同输入下的输出分布差异，从而使学生模型更好地模仿教师模型。此外，为了保持模型的泛化能力，还计算了在预训练语料库上的损失，这有助于防止模型过度拟合教师模型的输出。</p>
</blockquote>
<p>步骤：</p>
<p>在这篇论文的实验设置中，研究者们首先在指令-响应数据集 D 上对一个大模型进行微调，将其作为教师模型 p。然后，他们比较了不同的知识蒸馏方法，以在教师模型的指导下对学生模型进行蒸馏，并通过评估蒸馏模型的指令执行性能来进行比较。</p>
<p>他们蒸馏了三种不同大小的模型：GPT-2，OPT和LLaMA，并分别使用GPT-2-1.5B，OPT-13B和LLaMA-13B作为各模型类型的教师模型。他们还使用GPT-J作为教师模型的结果。</p>
<p>他们从 databricks-dolly-15k 数据集中构建了训练数据，该数据集由 15K个人工编写的指令-响应对组成。他们随机分配了 14K个样本作为训练集 D，并留下了 500个样本用于验证和测试。对于 DPT，他们为 GPT-2 系列使用了 OpenWebText，而对于其他模型，他们使用了 RoBERTa 训练语料库。</p>
<p>实验中使用了两个指标来评估模型生成的响应：Rouge-L 分数和GPT-4反馈。</p>
<p>他们在主要实验中考虑了三个基线：</p>
<ol>
<li>SFT w&#x2F;o KD 直接在 D 上对学生模型进行微调，使用金色响应进行监督。</li>
<li>KD 使用教师分布作为每个标记步骤的监督，对 D 上的学生模型进行微调，也称为单词级别的 KD。</li>
<li>SeqKD 在教师生成的数据上对学生模型进行微调。</li>
</ol>
<p>总结实验步骤：</p>
<ol>
<li>在指令-响应数据集上微调大模型作为教师模型。</li>
<li>对多种大小的学生模型进行知识蒸馏，并评估其在教师模型指导下在数据集上的性能。</li>
<li>从 databricks-dolly-15k 数据集中构建训练数据。</li>
<li>使用 OpenWebText 和 RoBERTa 训练语料库进行预训练。</li>
<li>使用 Rouge-L 和 GPT-4 反馈对模型生成的响应进行评估。</li>
<li>比较与直接微调学生模型（SFT w&#x2F;o KD）、单词级别知识蒸馏（KD）以及在教师生成数据上微调学生模型（SeqKD）等基线方法的性能。</li>
<li>报告结果并进行分析。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308061243956.png" alt="image-20230806124343892"></p>
<ul>
<li>a. 理论背景:<ul>
<li>本文讨论了使用知识蒸馏（KD）来减少大型语言模型（LLM）的计算需求。先前的KD方法主要集中在白盒分类模型或训练小模型以模仿黑盒模型API，而作者提出了一种名为MINILLM的方法，从生成性更大的语言模型中提炼出较小的语言模型。作者认为标准的KD目标对于生成性LLM来说是次优的，并提出了最小化反向Kullback-Leibler散度（KLD）的方法。他们还引入了一种优化方法来学习这个目标。实验结果表明，MINILLM模型生成的响应更精确，整体质量更高，暴露偏差更低，校准性更好，并具有更高的长文本生成性能。该方法适用于不同模型家族，参数大小范围从120M到13B。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>MINILLM方法用于语言模型（LLM）中的知识蒸馏（KD）。与序列级KD不同，MINILLM专注于最小化反向Kullback-Leibler散度（KLD），并鼓励学生模型在其自身能力范围内生成教师模型偏好的样本。使用策略梯度定理进行优化，通过从学生模型中进行采样来计算目标函数的梯度。为了解决高方差和奖励欺骗等问题，提出了三种策略：单步正则化、教师混合采样和长度归一化。提供了MINILLM的训练算法，包括在数据集上微调学生模型、采样提示和响应以及计算梯度。还描述了实验设置和评估指标。</li>
</ul>
</li>
</ul>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><ul>
<li>a. 详细的实验设置:<ul>
<li>表1中的评估结果显示，标准的知识蒸馏（KD）方法，SFT w&#x2F;o KD、KD和SeqKD，成功地从教师模型中提取了知识，实现了更好的Rouge-L和GPT-4反馈分数。然而，MINILLM方法在几乎所有情况下都优于基线方法，表明其在提取具有高整体性能的小模型方面的有效性。MINILLM还生成了与真实值具有高重叠度的更精确的响应，并展示了良好的超出分布的泛化能力。MINILLM的改进在不同的模型大小和家族中保持一致，证明了其在大型语言模型（LLM）时代的可扩展性和泛化能力。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>实验结果显示，微调模型的ExAccErr在生成过程中不断增加，而MiniLLM具有较低的ExAccErr，并且错误在长文本生成中停止累积。在两个文本分类数据集SST2和BoolQ上的校准测试显示，使用KD和SeqKD训练的模型与教师模型相比校准性较差。另一方面，MINILLM缩小了学生和教师之间ECE分数的差距。模型的性能根据真实响应的长度而变化，在期望短响应的提示上，所有方法都获得了较低的分数。MINILLM通过保持生成响应中不同的4-gram比例来保持生成多样性，并且不会在测试集上导致语言建模损失的显著增加。消融研究表明，教师混合采样和长度归一化对于稳定训练至关重要，而单步正则化减少了训练过程的方差。在教师混合探索中，α值的选择会影响性能，一般而言，α &#x3D; 0.2是合适的。添加预训练损失有助于在规范NLP任务上保持能力，而不会对指令遵循任务的性能产生显著影响。</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%92%B8%E9%A6%8F/" data-id="cllkmcmf6000iksu7egm425wk" data-title="8月第一周自然语言大模型蒸馏" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-7月第四周周报依赖图通用剪枝" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%91%A8%E6%8A%A5%E4%BE%9D%E8%B5%96%E5%9B%BE%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T07:57:10.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%91%A8%E6%8A%A5%E4%BE%9D%E8%B5%96%E5%9B%BE%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/">7月第四周周报依赖图通用剪枝</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="7月第四周依赖图论文通用剪枝"><a href="#7月第四周依赖图论文通用剪枝" class="headerlink" title="7月第四周依赖图论文通用剪枝"></a>7月第四周依赖图论文通用剪枝</h1><h1 id="论文1-DepGraph-Towards-Any-Structural-Pruning"><a href="#论文1-DepGraph-Towards-Any-Structural-Pruning" class="headerlink" title="论文1 DepGraph: Towards Any Structural Pruning"></a>论文1 DepGraph: Towards Any Structural Pruning</h1><h1 id="DepGraph：面向任何结构修剪"><a href="#DepGraph：面向任何结构修剪" class="headerlink" title="DepGraph：面向任何结构修剪"></a>DepGraph：面向任何结构修剪</h1><h1 id="Basic-Information"><a href="#Basic-Information" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: DepGraph: Towards Any Structural Pruning (DepGraph: 通向任意结构剪枝)</li>
<li>Authors: Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, Xinchao Wang</li>
<li>Affiliation: National University of Singapore, Zhejiang University, Huawei Technologies Ltd.</li>
<li>Keywords: structural pruning, neural networks, dependency graph, automatic method, generalizability</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.12900v2">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/VainF/Torch-Pruning">GitHub</a></li>
</ul>
<h1 id="论文简要"><a href="#论文简要" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li><p>提出了一种非深度图算法DepGraph，实现了架构通用的结构化剪枝，适用于CNNs, Transformers, RNNs, GNNs等网络。DepGraph能够自动地分析复杂的结构耦合，从而正确地移除参数实现网络加速。基于DepGraph算法，我们开发了PyTorch结构化剪枝框架 <a href="https://link.zhihu.com/?target=https://github.com/VainF/Torch-Pruning">Torch-Pruning</a>。不同于依赖Masking实现的“模拟剪枝”，该框架能够实际地移除参数和通道，降低模型推理成本。在DepGraph的帮助下，研究者和工程师无需再与复杂的网络结构斗智斗勇，可以轻松完成复杂模型的一键剪枝。</p>
<p>论文标题：DepGraph: Towards Any Structural Pruning<br>论文链接：<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2301.12900">https://arxiv.org/abs/2301.12900</a><br>项目地址：<a href="https://link.zhihu.com/?target=https://github.com/VainF/Torch-Pruning">https://github.com/VainF/Torch-</a></p>
</li>
</ul>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 近年来，边缘计算应用的出现使得深度神经网络压缩变得必要。剪枝作为一种网络压缩方法已被证明是高效且实用的。然而，现有的剪枝方法往往依赖于手动设计的分组方案，限制了其在新的网络架构上的通用性。</li>
<li>过去方案: 传统的剪枝方法可以大致分为结构剪枝和非结构剪枝两种。结构剪枝通过物理上移除参数来改变神经网络的结构，而非结构剪枝则通过将特定权重置零来实现。然而，现有的结构剪枝方法往往需要针对不同网络进行个案分析，且无法直接推广到其他网络架构，限制了其在实际应用中的使用。</li>
<li>论文的Motivation: 鉴于现有结构剪枝方法的局限性，本文旨在提出一种通用的剪枝方案，能够适用于任意网络架构。为了解决结构剪枝中的依赖性问题，作者引入了Dependency Graph (DepGraph)方法，通过显式建模不同层之间的依赖关系，实现了全自动的剪枝过程。通过在多个网络架构上的实验证明了DepGraph的有效性和通用性。</li>
</ul>
<h1 id="论文摘要"><a href="#论文摘要" class="headerlink" title="论文摘要"></a>论文摘要</h1><blockquote>
<p>结构剪枝能够通过从神经网络中移除结构性分组的参数来加速模型。然而，不同模型中的参数分组模式各不相同，这使得依赖于手动设计的分组方案的特定架构的剪枝器无法泛化到新的架构。在这项工作中，我们研究了一个极具挑战性但鲜少被探索的任务，即<strong>任意结构剪枝</strong>，以解决诸如CNNs、RNNs、GNNs和Transformers等任意架构的通用结构剪枝。实现这个目标的最大难题在于<strong>结构耦合</strong>，它不仅强制不同的层同时被剪枝，还期望所有被移除的参数在重要性上保持一致，从而避免剪枝后出现结构问题和显著的性能下降。为解决这个问题，我们提出了一种通用且全自动的方法，依赖图（Dependency Graph，DepGraph），用以明确地建模层与层之间的依赖关系，以及为剪枝全面地组合耦合参数。在这项工作中，我们对多种架构和任务进行了广泛的评估，包括用于图像的ResNe(X)t、DenseNet、MobileNet和Vision transformer，用于图的GAT，用于3D点云的DGCNN，以及用于语言的LSTM，并表明，即使采用简单的基于规范的判别准则，所提出的方法也能够持续产生令人满意的性能。</p>
</blockquote>
<blockquote>
<ol>
<li>什么是结构耦合（structural coupling），并且它如何影响神经网络的剪枝？</li>
<li>结构耦合是指<strong>神经网络中不同层之间的关联性</strong>，这种关联性强制同时剪枝不同的层，同时也期望所有被剪枝的参数都一致地不重要，这样可以避免剪枝后出现结构问题和明显的性能下降。具体来说，如果一个参数被剪枝，那么它的耦合参数也应该被剪枝，以保持网络的结构完整性。</li>
</ol>
</blockquote>
<h2 id="图片1：不同神经网络模型结构耦合和剪枝策略的对比"><a href="#图片1：不同神经网络模型结构耦合和剪枝策略的对比" class="headerlink" title="图片1：不同神经网络模型结构耦合和剪枝策略的对比"></a>图片1：不同神经网络模型结构耦合和剪枝策略的对比</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221403214.png" alt="image-20230722140353693"></p>
<blockquote>
<p>a: 在”a”部分（CNNs）中，例如，为了剪枝Conv2，块内的所有其他层（如Conv1, BN1, BN2）也必须被剪枝。这强调了在剪枝过程中，如果要剪枝某个层，则必须同时剪枝块内的所有其他层。</p>
<p>b: b部分中，如果要对Transform的MLP模块进行剪枝，也就必须要对Norm和多头注意力进行剪枝，这种情况的出现是由于神经网络中的结构耦合。在神经网络中，不同层的参数是彼此依赖的，这种依赖关系迫使一旦一个层（如MLP）被剪枝，那么它的耦合层（如Norm和多头注意力机制）也必须被剪枝。各层之间的信息流动是相互依赖的。如果你剪掉了一个层，那么它的输出就不再能够作为下一层的输入。因此，下一层也就失去了它的输入源，相当于也被”剪掉”了。而这些层可能又是其他层的输入源，因此，剪枝一个层可能会导致多个层被剪掉。这就是为什么在剪枝MLP的同时，必须剪掉Norm和多头注意力机制。</p>
<p>c 一样的道理 RNNS</p>
<p>d GNNS  进行剪枝时，不仅要考虑节点或边的重要性，还需要考虑剪枝操作对整个网络结构的影响。例如，如果剪枝的节点或边是信息传播路径的一部分，那么剪枝可能会阻断信息的流动，从而影响网络的性能。</p>
</blockquote>
<h1 id="相关工作部分"><a href="#相关工作部分" class="headerlink" title="相关工作部分"></a>相关工作部分</h1><blockquote>
<ol>
<li><strong>结构剪枝和非结构剪枝</strong>：结构剪枝的目标是物理上移除一组参数，从而减少神经网络的大小。相比之下，非结构剪枝涉及将特定的权重置零，但不改变网络结构。非结构剪枝在实践中易于实施，并且本质上能够适应各种网络。然而，它通常需要专门的AI加速器或软件进行模型加速。相反，结构剪枝通过从网络中物理上移除参数来改善推理开销，从而在更广泛的应用领域中找到应用。</li>
<li><strong>剪枝分组参数</strong>：在复杂的网络结构中，参数组之间可能存在依赖性，需要同时进行剪枝。例如，当剪枝两个连续的卷积层时，从第一层移除一个过滤器需要剪枝下一层的关联核。虽然手动分析参数依赖性是可行的，但当应用于复杂的网络时，这个过程可能非常耗费人力。此外，这样的手动方案本质上不能转移到新的架构，这严重限制了剪枝的应用。最近，一些初步的工作已经被提出来解密层与层之间的复杂关系。不幸的是，现有的技术仍然依赖于经验规则或预定义的架构模式，使它们对所有结构剪枝应用的通用性不足。</li>
<li>作者指出了他们的工作目标：提出一种通用的方法来解决这个挑战，证明有效地处理参数依赖性可以使结构剪枝在各种网络中通用，从而在多个任务上得到满意的性能。</li>
</ol>
<p>在卷积神经网络（CNNs）中，一层的输出通常作为下一层的输入，这种依赖关系也适用于卷积层。如果你移除了Conv2的一个过滤器，那么Conv2的输入（也就是Conv1的输出）就会少了一个特征映射。为了保持网络的结构完整性，你需要同时剪掉Conv1中对应的输出通道。</p>
<p><strong>具体来说，Conv1的每个过滤器都会输出一个特征映射，这些特征映射被堆叠在一起，形成Conv2的输入。每个Conv2的过滤器都会在这些输入特征映射上进行卷积操作。如果你移除了Conv2的一个过滤器，那么那个过滤器对应的输入特征映射就不再需要了。这个输入特征映射是Conv1的一个输出特征映射，所以对应的Conv1的过滤器也就可以被剪掉了。</strong></p>
<p>这就是为什么在剪枝Conv2的时候，也需要剪掉Conv1。这是由于Conv1和Conv2之间的结构耦合，也就是他们的输出和输入之间的依赖关系。</p>
</blockquote>
<h1 id="方法-21、29"><a href="#方法-21、29" class="headerlink" title="方法:21、29"></a>方法:21、29</h1><ol>
<li><h2 id="Dependency-in-Neural-Networks神经网络中的依赖"><a href="#Dependency-in-Neural-Networks神经网络中的依赖" class="headerlink" title="Dependency in Neural Networks神经网络中的依赖"></a>Dependency in Neural Networks神经网络中的依赖</h2></li>
<li><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221459077.png" alt="image-20230722145939026"></p>
</li>
<li><p>在不同结构中具有相互依赖性的分组参数。必须同时修剪所有高亮显示的参数。</p>
</li>
<li><blockquote>
<p>在许多神经网络优化任务中，结构修剪是一种常用的技术，通过移除不重要的神经元来使网络更加高效。在这里，作者使用了一个由三个连续层组成的线性神经网络作为例子，每一层都由二维权重矩阵（wl, wl+1和wl+2）参数化。</p>
<p>当通过修剪神经元来瘦身网络时，参数之间的依赖性就显现出来了。在这里，wl和wl+1的依赖性是这样的：如果你想修剪连接wl和wl+1的第k个神经元，那么你就必须同时移除wl[k, :] 和 wl+1[:, k]。这就是所谓的“依赖性”，因为这两个参数（即wl和wl+1）必须一起修剪，不能单独修剪。</p>
<p>在现有的文献中，研究人员通常使用手动设计的、模型特定的方案来处理层依赖性并对深度神经网络进行结构修剪。然而，这个过程中存在着各种类型的依赖性，如作者所示。手动分析所有的依赖性并不可行，尤其是当简单的依赖性可能会嵌套或组合形成更复杂的模式时。</p>
</blockquote>
</li>
</ol>
<h2 id="Dependency-Graph依赖图"><a href="#Dependency-Graph依赖图" class="headerlink" title="Dependency Graph依赖图"></a>Dependency Graph依赖图</h2><p>1.分组：为了执行结构修剪（结构修剪是一种优化神经网络的方法，通过删除不重要的神经元或连接来减少网络的复杂性），我们首先需要将网络层按照它们的依赖关系进行分组。在这里，他们提出了一个分组矩阵G，它是一个L×L的矩阵，其中L是网络层的数量。在这个矩阵中，如果第i层和第j层之间存在依赖关系，那么Gij就是1。</p>
<p>然而，现代的深度网络可能由成千上万个具有复杂连接的层组成，这使得从神经网络中获取分组模式变得非常困难。也就是说，得到的分组矩阵G可能会非常大而复杂，其中Gij的值不仅取决于第i层和第j层，还会受到它们之间所有中间层的影响。在大多数情况下，这种非局部的、隐式的关系不能用简单的规则来处理。</p>
<p>因此，作者提出了一个新的解决方案：<strong>他们不直接估计分组矩阵G，而是提出了一个更易于估计的依赖模型方法，即依赖图。依赖图是从分组矩阵G中有效地提取出来的，它能更好地处理和展示层之间复杂的依赖关系。</strong></p>
<p> 1 分组矩阵的定义：<br>$$<br>G \in {0,1}^{L \times L}<br>$$</p>
<ol start="2">
<li><p>如果第i层和第j层之间存在依赖关系，那么：<br>$$<br>G_{ij} &#x3D; 1<br>$$</p>
</li>
<li><p>如果不存在依赖关系，则：</p>
</li>
</ol>
<p>$$<br>G_{ij} &#x3D; 0<br>$$</p>
<ol start="4">
<li>对角线元素的设置，即自依赖性：</li>
</ol>
<p>$$<br>Diag(G) &#x3D; \mathbf{1}_{1 \times L}<br>$$</p>
<p>其中$\mathbf{1}_{1 \times L}$是长度为L的全1向量。</p>
<ol start="5">
<li>表示与第i层有依赖关系的所有层的集合：</li>
</ol>
<p>$$<br>g(i) &#x3D; {j | G_{ij} &#x3D; 1}<br>$$</p>
<p>2 Dependency Graph ：介绍了一种新的图形数据结构——依赖图Dependency Graph，这是一种有效的分组矩阵G的压缩方式。依赖图D记录了具有直接连接的相邻层之间的本地依赖关系，与分组矩阵G的区别在于，它仅记录了直接连接的相邻层之间的依赖关系。依赖图D可以被视为分组矩阵G的传递性简化，即<strong>包含与G相同的顶点，但尽可能少的边</strong>。</p>
<p>在介绍Dependency Graph之前，首先用一个例子解释了依赖关系的冗余。考虑一个依赖组g &#x3D; {w1, w2, w3}，它有依赖关系w1 ⇔ w2，w2 ⇔ w3，和w1 ⇔ w3。经过仔细观察，可以发现这个依赖模型存在一些冗余。例如，依赖关系w1 ⇔ w3可以通过递归过程从w1 ⇔ w2和w2 ⇔ w3推导出来。开始时，我们以w1为起点，检查它与其他层（如w1 ⇔ w2）的依赖关系。然后，w2提供了一个新的起点，用于递归地扩展依赖关系，这反过来又“触发”了w2 ⇔ w3。这个递归过程最终以一个传递关系w1 ⇔ w2 ⇔ w3结束。在这种情况下，我们只需要两个依赖关系就可以描述组g中的关系。</p>
<p>同样的，3.2节中讨论的分组矩阵G对于依赖性建模也是冗余的，因此可以被压缩为一个形式更紧凑，边更少，但保留相同信息的形式。这就引出了依赖图。</p>
<p>依赖图D的正式构造规则是：对于所有的Gij &#x3D; 1，在D中存在从顶点i到j的路径。因此，Gij可以通过检查D中顶点i和j之间是否存在路径来推导出来。</p>
<blockquote>
<p>依赖图Dependency Graph的构造过程分步骤解释：</p>
<ol>
<li><strong>定义所有节点</strong>：首先，我们在图中定义每一层网络为一个节点。这些节点相当于我们要考虑的网络层。</li>
<li><strong>检查并添加直接的边</strong>：然后，我们仔细观察网络中每一层之间的直接连接。如果第i层和第j层之间有直接连接（即第i层的输出是第j层的输入），我们就在依赖图中添加一条从i到j的边。这样，我们就得到了所有直接相邻层之间的依赖关系。</li>
</ol>
<p>现在我们来考虑一个更为复杂的依赖关系：如果有第i层→第j层→第k层的依赖关系。在原始的分组矩阵G中，我们会同时标记Gij和Gjk为1，还需要标记Gik为1，表示第i层和第k层也存在依赖关系。</p>
<p>然而，在依赖图D中，我们不需要直接标记Gik。只要有从节点i到节点k的路径（在这种情况下，路径就是i→j→k），就认为Gik存在。这样，依赖图D只需要记录直接的依赖关系，而所有间接的依赖关系可以通过检查路径的存在来推导。这大大减少了需要记录的依赖关系的数量。</p>
<p>依赖图D的主要目标就是将分组矩阵G压缩为更简洁的形式，便于处理复杂的依赖关系。这种压缩是有效的，因为我们只关心是否存在从一个节点到另一个节点的路径，而不关心路径的具体形状。这种方法可以在保留所有重要信息的同时，大大简化了依赖关系的表示。</p>
</blockquote>
<p>3 Network Decomposition 网络分解</p>
<blockquote>
<p>这一段的主要内容是说，在实际操作中，仅仅在网络层级别上建立依赖图可能存在一些问题，因为某些基础层（如全连接层）可能有两种不同的修剪方案，比如 w[k, :] 和 w[:, k]，它们分别压缩了输入和输出的维度。此外，网络还包含一些非参数化的操作，比如跳过连接（skip connections），它们也会影响层之间的依赖关系。</p>
<p>为了解决这些问题，作者提出了一种新的表示法，将网络 F(x; w) 分解为更细的基本组件，表示为 F &#x3D; {f1, f2, …, fL}，其中每个组件 f 可以是参数化的层（如卷积）或非参数化的操作（如残差加法）。他们不再专注于层级别的关系，而是集中于层的输入和输出之间的依赖关系。特别地，他们将组件 fi 的输入和输出分别表示为 f- i 和 f+ i。对于任何网络，最终的分解可以被形式化为 F &#x3D; {f- 1 , f+ 1 , …, f- L , f+ L }。这种表示法使得依赖关系的建模更为容易，并允许同一层有不同的修剪方案。</p>
</blockquote>
<p>4 Dependency Modeling 依赖模型</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221521004.png" alt="image-20230722152158933"></p>
<blockquote>
<p>层分组是通过从f+ 4开始在DepGraph上递归传播来实现的。在该示例中，由于上面所示的发散修剪方案，在卷积输入f-4和输出f+ 4之间不存在层内依赖性。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221523007.png" alt="image-20230722152330970"></p>
<p>算法1 解释</p>
<blockquote>
<p>用于构建所“依赖图”，它是一个二维矩阵，用于表示网络中的层之间的依赖关系。</p>
<p>以下是这个算法的步骤：</p>
<ol>
<li><p>输入一个神经网络F(x; w)。</p>
</li>
<li><p>将网络F分解为两组组件：f− 和 f+。这些组件分别代表每一层的输入和输出。</p>
</li>
<li><p>初始化依赖图DepGraph D，这是一个 2L x 2L 的零矩阵，其中 L 是网络中层的数量。</p>
</li>
<li><p>双重循环遍历所有的 i 和 j，对于每一对 (i, j)，根据公式计算 D(f- i, f+ j) 和 D(f+ j, f- i) 的值。</p>
<ul>
<li><p>如果 f- i 与 f+ j 是相连的，或者它们位于同一层并且有相同的剪枝方案（sch(f- i) &#x3D; sch(f+ j)），则值为1；</p>
</li>
<li><p>否则，值为0。</p>
</li>
</ul>
</li>
<li><p>返回依赖图D。</p>
</li>
</ol>
<p>简单来说，这个算法通过检查网络中每一层的输入和输出，建立了一个表征层间和层内依赖关系的二维矩阵。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221524708.png" alt="image-20230722152425671"></p>
<blockquote>
<p>这个算法是用来在神经网络中创建一组分组，每一组中的层有依赖关系。</p>
<p>以下是这个算法的步骤：</p>
<ol>
<li><p>输入依赖图DepGraph D。</p>
</li>
<li><p>初始化一个空的集合G，这将存储分组。</p>
</li>
<li><p>遍历所有层i。</p>
</li>
<li><p>对于每个i，初始化一个包含i的新组g。</p>
</li>
<li><p>在一个循环中，持续更新g，直到没有新的元素可以添加进g为止：</p>
<ul>
<li><p>创建一个未见过的层的集合UNSEEN，这些层还没有被添加到g中。</p>
</li>
<li><p>创建一个新的集合g’，包含那些在UNSEEN中并且与g中的某个层k有依赖关系的层（即Dkj &#x3D; 1）。</p>
</li>
<li><p>更新g，将g’中的所有层添加到g中。</p>
</li>
</ul>
</li>
<li><p>将g添加到G中。</p>
</li>
<li><p>返回所有的分组G。</p>
</li>
</ol>
<p>简单来说，这个算法是通过遍历网络中的所有层，并查看哪些层与已经在组中的层有依赖关系，从而创建一系列分组。每个分组中的所有层都有相互依赖关系。</p>
</blockquote>
<h2 id="3-3-组级剪枝"><a href="#3-3-组级剪枝" class="headerlink" title="3.3 组级剪枝"></a>3.3 组级剪枝</h2><p>这部分主要讨论了如何在神经网络中实现组级别的剪枝。</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221527044.png" alt="image-20230722152727989"></p>
<blockquote>
<p>比较了三种不同的剪枝方法，即如何选择神经网络中应该删除的权重（即神经元之间的连接）。每种方法都有自己的优点和缺点，主要的区别在于他们如何定义哪些权重是“重要的”。</p>
<ol>
<li>方法(a) 是非结构化剪枝：在这种方法中，每个权重都被独立地看待。换句话说，我们只关注每个单独权重的重要性，而不考虑它们之间的关系。这种方法的优点是简单直观，缺点是它可能忽略了权重之间的依赖关系。</li>
<li>方法(b) 是结构化剪枝：这种方法在评估权重重要性时，会考虑到一层内的结构关系，即在同一层内的权重将会一起被考虑。但是，它依然忽视了在不同层之间可能存在的关联关系。</li>
<li>方法(c) 是本文提出的组级剪枝：这种方法考虑到了权重之间的依赖关系，无论这些权重是在同一层还是在不同层。它的目标是将所有关联的参数（即一组）一起设置为零，这样就可以很容易地通过一个简单的幅度方法（magnitude method）来识别它们。</li>
</ol>
<p>这三种方法的主要区别在于他们处理权重之间关系的方式。在组级剪枝中，作者认为在神经网络中，权重之间的关系是非常重要的，应该被一同考虑，从而得到更好的剪枝效果。</p>
</blockquote>
<p>在前面的部分中，作者们已经开发了一种用于分析神经网络内部依赖性的通用方法，这自然引导出了组级别剪枝问题。评估组合参数的重要性对于剪枝来说是一个重大的挑战，因为它涉及到几个关联的层。在这个部分中，作者们利用一个简单的基于范数的标准来为组级别剪枝建立了一个实用的方法。</p>
<p>给定一个参数组g &#x3D; {w1, w2, …, w|g|}，现有的标准，如L2范数的重要性I(w) &#x3D; w2，可以为g中的每个w产生独立的评分。估计组的重要性的一个自然的方式是计算一个聚合的评分I(g) &#x3D; ∑w∈g I(w)。但不幸的是，独立地在不同层上估计的重要性评分可能由于分布和大小的差异而不可加，因此没有意义。为了使这个简单的聚合适用于重要性估计，<strong>作者们提出了一个稀疏训练方法</strong>，用于在组级别上稀疏化参数，这样，那些被零化的组就可以从网络中安全地移除。</p>
<blockquote>
<p>作者在这一部分讨论了如何对神经网络进行“组级别”的剪枝，这是一种对网络进行优化的方法，用以删除网络中不重要或者冗余的部分，以减少网络的复杂性。</p>
<p>在这里，他们提到的”组”指的是一组相关的神经网络参数，比如，这些参数可能来自同一层，或者在不同层间有一些依赖关系。这些参数被看作是一个整体，而不是独立处理，因为他们在网络中的作用是相互关联的。</p>
<p>那么，问题来了，我们应该如何决定哪个“组”重要，哪个不重要呢？作者在这里提出了一种方法，即通过计算组内所有参数的L2范数（一种衡量参数重要性的方法）的总和来判断。如果一个组的总重要性得分低，那么这个组可能就不太重要，可以被剪枝掉。</p>
<p>但是，这个方法有个问题，就是不同层的参数可能具有不同的规模或分布，所以我们不能简单地将他们的重要性得分加在一起。为了解决这个问题，作者提出了一种“稀疏训练”的方法。这种方法的目标是尽量让不重要的组的参数变为0，这样这些参数就不会对网络的输出产生影响，可以安全地被剪枝掉。</p>
<p>作者使用了一个特定的公式，通过对参数进行“惩罚”，使得不重要的参数趋向于0，这就是所谓的”正则化项”。这个公式涉及一些复杂的数学概念，简单来说，就是根据每个参数的重要性对其进行加权，重要性越低的参数，其“惩罚”越大。</p>
<p>在进行了这种“稀疏训练”后，作者再用一个简单的公式来决定哪些参数是不重要的，即他们的分数低于某个阈值，然后将这些参数删除。</p>
<p>最后，作者通过实验证明，这种方法在剪枝效果上能达到与其他现代方法相当的水平，即使它的原理比较简单。</p>
</blockquote>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><ul>
<li><p>a. 详细的实验设置:</p>
<ul>
<li><p>作者在CIFAR数据集上进行了实验，评估了他们修剪算法的性能。</p>
</li>
<li><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221531158.png" alt="image-20230722153111108"></p>
</li>
<li><blockquote>
<ul>
<li>“Model &#x2F; Data”: ResNet56和VGG19都是常见的深度学习模型，它们在许多任务上都表现出色。CIFAR-10和CIFAR-100是两个常用的图像分类数据集，分别包含10个和100个类别。</li>
<li>“Method”: 这是正在测试的剪枝方法。表中列出了一系列的剪枝方法，包括本文作者的方法(“Ours”)。</li>
<li>“Base”: 这是未进行剪枝的模型在测试数据集上的准确率。</li>
<li>“Pruned”: 这是使用特定剪枝方法后的模型在测试数据集上的准确率。</li>
<li>“∆ Acc.”: 这是剪枝前后模型准确率的变化，可以看作是剪枝对模型性能的影响。负值表示准确率下降，正值表示准确率提高。</li>
<li>“Speed Up”: 这是通过剪枝实现的加速比例，表示剪枝后的模型与原始模型相比，执行速度提高了多少倍。这是剪枝的主要目标之一，因为更快的模型可以更有效地处理数据和做出预测。</li>
</ul>
</blockquote>
</li>
<li></li>
<li><p>报告了修剪模型的准确性和理论加速比。</p>
</li>
</ul>
</li>
<li><p>b. 详细的实验结果:</p>
</li>
<li><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307221534860.png" alt="image-20230722153445775"></p>
</li>
<li><blockquote>
<p>这个表格展示了对各种网络结构（如ResNet-56、VGG-19、DenseNet-121、MobileNetv2和GoogleNet）进行不同策略的剪枝时的精确度结果。策略包括随机剪枝、不进行分组剪枝、只对卷积层进行剪枝、以及全分组剪枝。并且分别对比了统一的剪枝和学习到的剪枝的效果。</p>
<p>每行的数据对应一种剪枝策略对应的结果。</p>
<ul>
<li>“Architecture”: 这是正在测试的模型的名称。</li>
<li>“Strategy”: 这是正在测试的剪枝策略。表中列出了一系列的剪枝策略，包括随机剪枝（”Random”）、不进行分组剪枝（”No grouping”）、只对卷积层进行剪枝（”Conv-only”）以及全分组剪枝（”Full Grouping”）。</li>
<li>“Pruned Accuracy with Uniform &#x2F; Learned Sparsity”: 这是剪枝后的模型在测试数据集上的准确率，其中包括了使用统一的剪枝策略和学习到的剪枝策略的结果。剪枝策略分别对应了不同的速度提升比例，例如1.5倍、3.0倍、6.0倍、12倍。</li>
<li>“Avg.”: 这是各个速度提升比例下的平均精确度。</li>
</ul>
<p>表格的主要目的是展示在使用不同的剪枝策略以及不同的速度提升比例时，各个网络模型的精确度的变化情况，以此来评估和比较不同剪枝策略的效果。</p>
</blockquote>
<ul>
<li>结果显示，他们的方法在所有分组层（包括卷积、批归一化和全连接层）上都能促进稀疏性，从而提高修剪模型的准确性。</li>
<li>作者还可视化了他们的方法学习到的分组参数的范数，显示了组内的稀疏性。</li>
<li>进行了消融研究以验证分组的有效性，结果表明忽略分组信息会显著降低他们方法的性能。</li>
<li>作者还比较了均匀稀疏性和学习稀疏性，发现学习稀疏性通常优于均匀稀疏性，尽管有时会导致过度修剪和准确性下降。</li>
<li>他们还将其框架应用于包括DenseNet和GoogleNet在内的各种卷积神经网络，展示了其框架的通用性。</li>
<li>提供了DepGraph可视化，显示了参数的分组，为大型神经网络的修剪过程提供了便利。</li>
</ul>
</li>
</ul>
<h1 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h1><p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1TRvELQDNj9PwM-EERWbF3IQOyxZeDepp?usp=sharing#scrollTo=yaoMwy86Vhxz">Torch-Pruning-Demo.ipynb - Colaboratory (google.com)</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model_dict = &#123;</span><br><span class="line">    <span class="string">&#x27;resnet50&#x27;</span>: torchvision.models.resnet50,</span><br><span class="line">    <span class="string">&#x27;resnet18&#x27;</span>: torchvision.models.resnet18,</span><br><span class="line">    <span class="string">&#x27;convnext&#x27;</span>: torchvision.models.convnext_base,</span><br><span class="line">    <span class="string">&#x27;vgg_19_bn&#x27;</span>: torchvision.models.vgg19_bn,</span><br><span class="line">    <span class="string">&#x27;regnet_x_1_6gf&#x27;</span>: torchvision.models.regnet_x_1_6gf,</span><br><span class="line">    <span class="string">&#x27;efficientnet_b4&#x27;</span>: torchvision.models.efficientnet_b4,</span><br><span class="line">    <span class="string">&#x27;densenet121&#x27;</span>: torchvision.models.densenet121,</span><br><span class="line">    <span class="string">&#x27;vit_b_32&#x27;</span>: torchvision.models.vit_b_32,</span><br><span class="line">    <span class="string">&#x27;mobilenet_v3_large&#x27;</span>: torchvision.models.mobilenet_v3_large,</span><br><span class="line">    <span class="comment"># Register your models here. This demo only covers classification models.</span></span><br><span class="line">    <span class="comment"># Swin Transformers are not supported.</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">model = model_dict[<span class="string">&#x27;vit_b_32&#x27;</span>](pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">imp = tp.importance.MagnitudeImportance(p=<span class="number">2</span>) </span><br><span class="line"><span class="comment"># 创建一个重要性估计器，这基于权重的L2范数来评估神经网络各部分的重要性。</span></span><br><span class="line"></span><br><span class="line">ignored_layers = []</span><br><span class="line"><span class="comment"># 创建一个空的列表，用于保存我们不想剪裁的神经网络层。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> model.modules():</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, torch.nn.Linear) <span class="keyword">and</span> m.out_features == <span class="number">1000</span>: </span><br><span class="line">  <span class="comment"># 遍历模型中的所有模块。如果模块是线性层且输出特征数为1000（即分类器），则忽略这一层，不进行剪裁。</span></span><br><span class="line">    ignored_layers.append(m)</span><br><span class="line"></span><br><span class="line">round_to = <span class="literal">None</span></span><br><span class="line"><span class="comment"># 创建一个变量，用于设定剪裁的粒度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>( model, torchvision.models.vision_transformer.VisionTransformer):</span><br><span class="line">  round_to = model.encoder.layers[<span class="number">0</span>].num_heads </span><br><span class="line">  <span class="comment"># 模型是视觉变换器，则将round_to设为变换器的头部数量，这是模型特定的剪裁限制。</span></span><br><span class="line"></span><br><span class="line">pruner = tp.pruner.MagnitudePruner(</span><br><span class="line">    model = model,</span><br><span class="line">    <span class="comment"># 待剪裁的模型</span></span><br><span class="line"></span><br><span class="line">    example_inputs = torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>),</span><br><span class="line">    <span class="comment"># 输入的样例，用于模型的前向传播</span></span><br><span class="line"></span><br><span class="line">    importance = imp,     </span><br><span class="line">    <span class="comment"># 重要性估计器，用于评估哪些部分可以剪裁</span></span><br><span class="line"></span><br><span class="line">    global_pruning=<span class="literal">False</span>, </span><br><span class="line">    <span class="comment"># 设置全局剪裁为False，这意味着不是对整个网络进行剪裁，而是对各层独立进行剪裁。</span></span><br><span class="line"></span><br><span class="line">    ch_sparsity = <span class="number">0.5</span>,    </span><br><span class="line">    <span class="comment"># 设置剪裁后的稀疏度为0.5，即剪裁后每层保留的神经元或通道的比例。</span></span><br><span class="line"></span><br><span class="line">    iterative_steps = <span class="number">1</span>,  </span><br><span class="line">    <span class="comment"># 设置迭代步骤数为1，即达到目标稀疏度所需的剪裁步骤数。</span></span><br><span class="line"></span><br><span class="line">    ignored_layers = ignored_layers,        </span><br><span class="line">    <span class="comment"># 指定要忽略的层，这些层在剪裁过程中不会被剪裁。</span></span><br><span class="line"></span><br><span class="line">    round_to = round_to,  </span><br><span class="line">    <span class="comment"># 设置剪裁的粒度，即剪裁后的通道数需要是这个数的倍数。</span></span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model size before pruning</span></span><br><span class="line">base_macs, base_nparams = tp.utils.count_ops_and_params(model, torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"><span class="comment"># 计算剪裁前的模型大小。使用的度量包括MACs（乘加操作数）和参数数量。输入数据是随机生成的张量。</span></span><br><span class="line"></span><br><span class="line">pruner.step()</span><br><span class="line"><span class="comment"># 执行剪裁操作。在之前的代码中，已经对pruner对象进行了初始化，并设置了剪裁参数。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># modify some inferece-related attributes if necessary</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(model, torchvision.models.vision_transformer.VisionTransformer):</span><br><span class="line"><span class="comment"># 如果模型是视觉变换器（ViT），那么在剪裁后需要修改一些与推理相关的属性。</span></span><br><span class="line">    model.hidden_dim = model.conv_proj.out_channels</span><br><span class="line"><span class="comment"># 对于视觉变换器，剪裁后其隐藏维度（hidden_dim）需要被修改为卷积投影层的输出通道数。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameter &amp; MACs Counter</span></span><br><span class="line">pruned_macs, pruned_nparams = tp.utils.count_ops_and_params(model, torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"><span class="comment"># 计算剪裁后的模型大小。使用的度量依然是MACs和参数数量。输入数据是随机生成的张量。</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The pruned model:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"><span class="comment"># 打印剪裁后的模型。</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Summary:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Params: &#123;:.2f&#125; M =&gt; &#123;:.2f&#125; M&quot;</span>.<span class="built_in">format</span>(base_nparams/<span class="number">1e6</span>, pruned_nparams/<span class="number">1e6</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MACs: &#123;:.2f&#125; G =&gt; &#123;:.2f&#125; G&quot;</span>.<span class="built_in">format</span>(base_macs/<span class="number">1e9</span>, pruned_macs/<span class="number">1e9</span>))</span><br><span class="line"><span class="comment"># 打印剪裁前后的模型大小，包括参数数量和MACs。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Forward</span></span><br><span class="line">output = model(torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"><span class="comment"># 测试剪裁后模型的正向传播。输入数据是随机生成的张量。</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output.shape: &quot;</span>, output.shape)</span><br><span class="line"><span class="comment"># 打印正向传播的输出形状。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Backward</span></span><br><span class="line">loss = torch.nn.functional.cross_entropy(output, torch.randint(<span class="number">1</span>, <span class="number">1000</span>, (<span class="number">1</span>,)))</span><br><span class="line"><span class="comment"># 计算反向传播的损失。使用的损失函数是交叉熵，目标值是随机生成的整数。</span></span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment"># 执行反向传播，计算梯度。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="论文2：Scaling-TransNormer-to-175-Billion-Parameters"><a href="#论文2：Scaling-TransNormer-to-175-Billion-Parameters" class="headerlink" title="论文2：Scaling TransNormer to 175 Billion Parameters"></a>论文2：Scaling TransNormer to 175 Billion Parameters</h1><p>Zhen Qin♯, 1,2Dong Li♯, 1,2Weigao Sun♯, 1,2Weixuan Sun♯, 1,2Xuyang Shen♯,<br>2Xiaodong Han, 2Yunshen Wei, 2Baohong Lv, 1Fei Yuan, 2Xiao Luo,<br>1Yu Qiao, 1,2Yiran Zhong∗<br>1Shanghai AI Laboratory, 2OpenNLPLab</p>
<h1 id="Basic-Information-1"><a href="#Basic-Information-1" class="headerlink" title="Basic Information:"></a>Basic Information:</h1><ul>
<li>Title: Scaling TransNormer to 175 Billion Parameters (将TransNormer扩展到1750亿参数)</li>
<li>Authors: Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, Yu Qiao, Yiran Zhong</li>
<li>Affiliation: Shanghai AI Laboratory, OpenNLPLab (上海人工智能实验室, OpenNLPLab)</li>
<li>Keywords: Large Language Model, TransNormerLLM, linear attention, efficiency, scalability (大型语言模型, TransNormerLLM, 线性注意力, 效率, 可扩展性)</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.14995v1">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/OpenNLPLab/TransnormerLLM">GitHub</a></li>
</ul>
<h1 id="论文简要-1"><a href="#论文简要-1" class="headerlink" title="论文简要 :"></a>论文简要 :</h1><ul>
<li>本文介绍了TransNormerLLM，这是第一个基于线性注意力的大型语言模型，它在准确性和效率方面优于传统的基于softmax注意力的模型。通过引入位置嵌入、线性注意力加速、门控机制、张量归一化和推理加速等先进修改，TransNormerLLM从之前的线性注意力架构TransNormer发展而来。通过一系列的实验验证，该模型在训练和推理阶段都表现出卓越的效率和性能。</li>
</ul>
<h1 id="背景和创新点"><a href="#背景和创新点" class="headerlink" title="背景和创新点:"></a>背景和创新点:</h1><blockquote>
<ol>
<li><p>尽管现有的 Transformer 模型在许多任务上表现优秀，但是当面对大规模参数时，它们往往会遇到一些问题。首先，计算效率较低，这主要是因为 Transformer 模型需要计算和存储所有 token 之间的注意力权重，这在处理长序列时会导致计算和存储开销极大。其次，Transformer 模型还存在注意力稀释问题，即模型可能无法有效地关注到距离较远的 token。</p>
<p>为了解决这些问题，论文作者提出了 TransNormerLLM。TransNormerLLM 是对 TransNormer 模型的一个改进，它通过一系列创新的设计来解决上述问题。以下是 TransNormerLLM 的一些主要创新点：</p>
<ol>
<li><strong>位置编码</strong>：在 TransNormerLLM 中，作者引入了一种名为 LRPE-d 的新方法，这是一种线性化相对位置编码（LRPE）与指数衰减相结合的方法。这种方法可以让模型对距离较远的 token 给予较小的注意力权重，从而有效地解决了注意力稀释问题。同时，由于 LRPE-d 可以被分解为关于每个输入元素的函数，因此它与线性注意力方法完全兼容。值得一提的是，LRPE-d 的参数可以通过训练进行学习，这为模型提供了更大的灵活性。</li>
<li><strong>门控机制</strong>：TransNormerLLM 引入了一种新的门控机制，被称为 SGLU（Simple Gated Linear Unit）。SGLU 是一种改进的激活函数，它引入了一个门控参数来控制信息的流动。不同于传统的 GLU，SGLU 去除了激活函数，因为门本身就可以引入非线性，从而使得模型更加高效。</li>
<li><strong>张量归一化</strong>：在 TransNormerLLM 中，作者引入了一种新的张量归一化方法，称为 SRMSNorm（Simple RMS Norm）。SRMSNorm 是一种更简单的归一化方法，它不依赖于特征维度，因此在处理大规模模型时更为高效。</li>
</ol>
</li>
</ol>
</blockquote>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291115870.png" alt="image-20230729111523716"></p>
<ul>
<li><blockquote>
<p>当然，下面是这些公式的整理以及对应的详细解释：</p>
<ol>
<li><p><strong>位置编码 (LRPE-d)</strong>:<br>$$<br>a_{st} &#x3D; q_s^T k_t \lambda^{s-t} \exp(i\theta(s-t))<br>$$</p>
<p>  这是 TransNormerLLM 使用的位置编码公式，其中 (q_s) 和 (k_t) 分别是位置 (s) 和 (t) 的查询和键，(\lambda) 是衰减率，(\theta) 是可学习的参数。这个公式表示，模型在计算位置 (s) 和 (t) 之间的注意力时，会考虑它们之间的相对位置，并通过 (\lambda^{s-t}) 和 (\exp(i\theta(s-t))) 来调整这个注意力。</p>
<ol start="2">
<li><strong>门控线性注意力 (GLA)</strong>:</li>
</ol>
</li>
</ol>
<p>$$<br>  O &#x3D; \text{Norm}(QK^TV) \odot U<br>$$</p>
<pre><code>  这是 TransNormerLLM 的门控机制，其中 \(Q\), \(K\), \(V\) 和 \(U\) 是模型的查询、键、值和门控参数，\(\odot\) 是元素级的乘法，\(\text&#123;Norm&#125;(\cdot)\) 是归一化函数。这个公式表示，模型在计算输出 \(O\) 时，会先计算 \(QK^TV\) 的归一化，然后与门控参数 \(U\) 进行元素级的乘法。
</code></pre>
<ol start="3">
<li><p><strong>简单门控线性单元 (SGLU)</strong>:</p>
<p>$$<br> O &#x3D; [V \odot U]W_o<br>$$</p>
<p>   这是 TransNormerLLM 的通道混合机制，其中 (V) 和 (U) 是模型的值和门控参数，(W_o) 是输出的权重矩阵。这个公式表示，模型在计算输出 (O) 时，会先计算 (V) 和 (U) 的元素级乘法，然后乘以权重矩阵 (W_o)。</p>
</li>
<li><p><strong>张量归一化 (SRMSNorm)</strong>:</p>
<p>$$<br> \text{SRMSNorm}(x) &#x3D; \frac{x}{|x|_2&#x2F;\sqrt{d}}<br>$$</p>
<p>   这是 TransNormerLLM 的张量归一化函数，其中 (x) 是输入的张量，(|x|_2) 是 (x) 的 L2 范数，(d) 是特征的维度。这个公式表示，模型在归一化 (x) 时，会先计算 (x) 的 L2 范数，然后除以 (\sqrt{d})，最后用 (x) 除以这个结果。</p>
</li>
</ol>
<p>以上四个公式是 TransNormerLLM 模型的关键部分，它们分别对应了模型的位置编码、门控机制、通道混合和张量归一化等重要功能。理解这些公式对于理解 TransNormerLLM 模型的工作原理是非常重要的。</p>
</blockquote>
</li>
<li><p>a. 理论背景:</p>
<ul>
<li>本文介绍了TransNormerLLM，这是一个基于线性注意力的大型语言模型（LLM），在准确性和效率方面超越了传统的softmax注意力模型。该模型建立在之前的线性注意力架构TransNormer的基础上，并引入了位置嵌入、线性注意力加速、门控机制、张量归一化和推理加速等先进的修改。本文强调了传统Transformer的局限性以及LLM中更高效的序列建模方法的需求。</li>
</ul>
</li>
<li><p>b. 技术路线:</p>
<ul>
<li>本文介绍了TransNormerLLM中的架构改进。它引入了将TransNormer的DiagAttention替换为线性注意力，以增强全局交互。本文还引入了具有指数衰减的LRPE来解决注意力稀释问题。在训练过程中，提出了Lightning Attention技术，显著加速线性注意力，将内存使用减少了四倍。本文简化了GLU和归一化，总体加速了20%。通过稳定的推理算法，确保了数值稳定性和恒定的推理速度。本文强调了TransNormerLLM的可扩展性以及在大规模集群上部署的能力。还提到计划开源预训练模型，促进LLM的社区驱动进展。</li>
</ul>
</li>
</ul>
<h1 id="结果-1"><a href="#结果-1" class="headerlink" title="结果:"></a>结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291141964.png" alt="image-20230729114150913"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291138381.png" alt="image-20230729113850322"></p>
<p>Transformer vs TransNormerLLM.TransNormerLLM在相同配置下，在385M和1B大小上的性能分别优于Transformer 5%和9%。</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291138203.png" alt="image-20230729113857153"></p>
<p>我们比较原始的TransNormer和改进的TransNormerLLM，结果示于表4中。TransNormerLLM分别表现出2%和1%的增强，同时显著更快。</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307291142007.png" alt="image-20230729114258960"></p>
<p>位置编码在位置编码实验中，我们进行了一系列测试，比较了LRPE-d、APE（绝对位置编码）、LRPE和Exp-Decay（指数衰减）。从表5中可以明显看出，我们提出的增强已经显示出对原始模型的改进。此外，最终的方案表现出2%的LRPE方法的改进。</p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>本研究使用PyTorch和Triton在Metaseq框架中实现了TransNormerLLM模型。</li>
<li>使用Adam优化器进行训练，并使用FSDP将模型扩展到NVIDIA A100 80G集群。还采用了模型并行技术进行性能优化。模型在包含300B个标记的样本语料库上进行训练。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>架构消融实验表明，TransNormerLLM在大小为385M时比Transformer性能提高了5%，在大小为1B时提高了9%。</li>
<li>对比了不同的位置编码方法，LRPE+LRPE-d组合的效果最优，比LRPE提高了2%。</li>
<li>对于衰减温度的消融研究表明，添加衰减温度可以降低TransNormerLLM的困惑度。</li>
<li>TransNormerLLM中引入门控机制可以降低损失值。</li>
<li>对GLA激活函数进行了不同的测试，结果表明激活函数的选择对最终结果影响很小。</li>
<li>在Gated Linear Units（GLU）结构中去除激活函数对结果影响微乎其微。</li>
<li>对于归一化方法进行了多种测试，结果表明这些方法在应用于TransNormerLLM时几乎没有区别。然而，使用Triton实现的修改版SRMSNorm在计算速度上比PyTorch实现方法提供了显著的提升。</li>
<li>Lightning Attention的计算速度至少比NormAttention的PyTorch实现快2倍。</li>
<li>Lightning Attention的内存占用随序列长度线性增长，当序列长度为8192时，比基线模型的内存效率提高了4倍。</li>
<li>模型并行显著降低了内存消耗，当模型并行大小设置为8时，TransNormerLLM-7B模型在单个GPU上只需要24.1GB的内存，相比模型并行大小为1时，内存减少了62.3%。</li>
<li>TransNormerLLM在训练速度和内存消耗方面始终优于Transformer，即使启用了模型并行。</li>
<li>TransNormerLLM模型在计算速度上始终优于Transformer模型，即使模型规模更大。</li>
<li>TransNormerLLM能够以更长的上下文长度进行训练，实现更高的计算速度。</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/7%E6%9C%88%E7%AC%AC%E5%9B%9B%E5%91%A8%E5%91%A8%E6%8A%A5%E4%BE%9D%E8%B5%96%E5%9B%BE%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" data-id="cllkmcmf2000dksu71rcabuj4" data-title="7月第四周周报依赖图通用剪枝" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" rel="tag">通用剪枝</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-7月第一周QLORA-有效剪枝-lomo全参数微调" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8QLORA-%E6%9C%89%E6%95%88%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T07:52:06.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8QLORA-%E6%9C%89%E6%95%88%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/">7月第一周QLORA/有效剪枝/lomo全参数微调</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="7月第一周QLORA-大模型有效剪枝-LOMO-全参数微调减少内存占用"><a href="#7月第一周QLORA-大模型有效剪枝-LOMO-全参数微调减少内存占用" class="headerlink" title="7月第一周QLORA&#x2F;大模型有效剪枝&#x2F;LOMO:全参数微调减少内存占用"></a>7月第一周QLORA&#x2F;大模型有效剪枝&#x2F;LOMO:全参数微调减少内存占用</h1><h1 id="论文1：QLORA：量化LLM的有效微调"><a href="#论文1：QLORA：量化LLM的有效微调" class="headerlink" title="论文1：QLORA：量化LLM的有效微调"></a>论文1：QLORA：量化LLM的有效微调</h1><h1 id="基本信息："><a href="#基本信息：" class="headerlink" title="基本信息："></a>基本信息：</h1><ul>
<li>题目：QLORA：量化LLM的有效微调 （QLORA：高效微调量化语言模型）</li>
<li>作者：蒂姆·德特默斯、阿蒂多罗·帕格诺尼、阿里·霍尔茨曼、卢克·泽特勒莫耶 （蒂姆·德特默斯、阿蒂多罗·帕格诺尼、阿里·霍尔茨曼、卢克·泽特尔莫耶）</li>
<li>隶属关系： 华盛顿大学</li>
<li>关键词：语言模型，微调，量化，低秩适配器（LLA），NormalFloat，双重量化，分页优化器，指令遵循，聊天机器人性能</li>
<li>网址：纸张：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14314v1%EF%BC%8CGitHub%EF%BC%9Ahttps://github.com/artidoro/qlora">https://arxiv.org/abs/2305.14314v1，GitHub：https://github.com/artidoro/qlora</a> 和 <a target="_blank" rel="noopener" href="https://github.com/TimDettmers/bitsandbytes">https://github.com/TimDettmers/bitsandbytes</a></li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h1><ul>
<li>a. 理论背景：<ul>
<li>本文讨论了QLORA和LoRA这两种参数高效的神经网络微调方法，以减少训练过程中的内存需求。LoRA通过分解投影来增强线性投影，而QLORA则使用4位NormalFloat量化和双量化来减少内存使用。QLORA还引入了分页优化器来防止梯度检查点过程中的内存溢出错误。</li>
</ul>
</li>
<li>b. 技术路线：<ul>
<li>QLORA使用LoRA方法在每个网络层引入适配器，以避免先前工作中出现的准确性折衷。QLORA使用4位NormalFloat（NF4）作为新的数据类型，该类型对于正态分布的权重是最优的。双量化用于量化量化常数，并使用分页优化器来管理内存峰值。</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927766.png" alt="image-20230625110446430"></p>
<blockquote>
<ol>
<li>LoRA（Low-Rank Adaptation）：这是一种微调方法，它通过低秩适应（low-rank adaptation）的技术来减少微调过程中的内存需求。具体而言，LoRA使用低秩矩阵分解的方法来近似和压缩原始的Transformer模型，以减少参数数量和内存占用。</li>
<li>QLORA（Quantized Low-Rank Adaptation）：这是QLORA对LoRA的改进。QLORA通过对Transformer模型进行量化，将模型参数的精度减少到4位，并使用分页式优化器（paged optimizers）来处理内存峰值，从而减少内存需求。</li>
</ol>
<p>关于QLORA相对于LoRA的改进，主要有两个方面：</p>
<ol>
<li>量化（Quantization）：QLORA使用较低的位精度（4位）来表示模型参数，从而降低了内存需求。通过减少每个参数的位数，可以大幅度减少存储参数所需的内存空间。</li>
<li>分页式优化器（Paged Optimizers）：QLORA使用分页式优化器来处理内存峰值问题。分页式优化器将模型参数分成多个页（pages），每次只加载一部分参数到内存中进行计算，以减少内存使用量。通过逐页加载和计算参数，可以处理大型模型在微调过程中可能产生的内存峰值问题。</li>
</ol>
</blockquote>
<h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><blockquote>
<p><strong>Block-wise k-bit Quantization</strong> 量化是将输入从保持更多信息的表示离散化到具有更少信息的表示的过程。这通常意味着将具有更多位的数据类型转换为更少位，例如从32位浮点数转换为8位整数。为了确保使用低位数据类型的整个范围，输入数据类型通常通过由输入元素的绝对最大值进行归一化而被重新缩放到目标数据类型范围中，输入元素通常被构造为张量。例如，将32位浮点（FP 32）张量量化为Int 8张量，范围为[-127，127]：<br>$$<br>\begin{aligned}<br>&amp; \<br>&amp;\mathbf{X}^{\mathrm{Int}}&amp; {}^{8}&#x3D;\mathrm{round}\left(\frac{127}{\mathrm{absmax}(\mathbf{X}^{\mathrm{FP32}})}\mathbf{X}^{\mathrm{FP32}}\right)&#x3D;\mathrm{round}(c^{\mathrm{FP32}}\cdot\mathbf{X}^{\mathrm{FP32}}),<br>\end{aligned}<br>$$</p>
<p>$$<br>\mathbf{dequant}(c^{\mathrm{FP32}},\mathbf{X}^{\mathrm{Int8}})&#x3D;{\frac{\mathbf{X}^{\mathrm{Int8}}}{c^{\mathrm{FP32}}}}&#x3D;\mathbf{X}^{\mathrm{FP32}}<br>$$</p>
<p><strong>Low-rank Adapters</strong>  低阶适配器（LoRA）微调是一种通过使用一小组可训练参数（通常称为适配器）来降低内存需求的方法，同时不更新保持固定的完整模型参数。随机梯度下降期间的梯度通过固定的预训练模型权重传递到适配器，该适配器被更新以优化损失函数。LoRA通过额外的因子分解投影来增强线性投影。</p>
<p>在参数高效微调（PEFT）方法中，LLM微调的大部分存储器占用来自激活梯度而不是来自学习的LoRA参数。对于在批量大小为1的FLAN v2上训练的7B LLaMA模型，LoRA权重相当于常用的原始模型权重的0.2%[28，37]，LoRA输入梯度的内存占用量为567 MB，而LoRA参数仅占用26 MB。使用梯度检查点[9]，输入梯度减少到每个序列平均18 MB，使其比所有LoRA权重组合更占用内存。相比之下，4位基本模型消耗5，048 MB内存。这突出了梯度检查点设置是重要的，但也突出了积极地减少LoRA参数的量仅产生较小的存储器益处。这意味着我们可以使用更多的适配器，而不会显著增加整体训练内存占用（详细的分类请参见附录G）。如后所述，这对于恢复完整的16位精度性能至关重要。</p>
</blockquote>
<h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>QLORA包含两个组件：4-bit NormalFloat量化和Double Quantization。其中：4-bit NormalFloat数据类型是基于Quantile Quantization技术开发的，通过估计输入张量的分位数来保证每个量化区间分配相等的值。Double Quantization是将额外的量化常数进行量化以减小内存开销的过程。</p>
<h3 id="4-bit-NormalFloat量化"><a href="#4-bit-NormalFloat量化" class="headerlink" title="4-bit NormalFloat量化"></a>4-bit NormalFloat量化</h3><p>NormalFloat（NF）数据类型建立在分位数量化[15]的基础上，这是一种信息理论上的最佳数据类型，可确保每个量化二进制具有从输入张量分配的相等数量的值。分位数量化通过经验累积分布函数估计输入张量的分位数来工作。分位数量化的主要限制是分位数估计的过程是昂贵的。因此，快速分位数近似算法，如SRAM分位数[15]，用于估计它们。由于这些分位数估计算法的近似性质，数据类型对于离群值具有较大的量化误差，离群值通常是最重要的值。当输入张量来自固定到量化常数的分布时，可以避免昂贵的分位数估计和近似误差。在这种情况下，输入张量具有相同的分位数，使得精确的分位数估计在计算上可行。</p>
<p>NF数据类型基于Quantile Quantization方法，它是一种信息论上最优的数据类型，可以确保将输入张量中的每个量化区间分配相等数量的值。Quantile quantization通过估计输入张量的分位数来实现，估计过程基于经验累积分布函数。Quantile quantization的主要限制在于分位数估计的过程比较昂贵。因此，需要使用快速的分位数近似算法（如SRAM quantiles）来进行估计。由于这些分位数估计算法是近似的，所以对于异常值（通常是最重要的值），数据类型存在较大的量化误差。当输入张量来自一个固定到量化常数的分布时，可以避免昂贵的分位数估计和近似误差。在这种情况下，输入张量具有相同的分位数，因此可以通过精确的分位数估计来降低计算成本。由于预训练的神经网络权重通常具有以零为中心的正态分布，标准差为σ（参见附录F），我们可以通过缩放σ来将所有权重转换为单个固定分布，使得该分布完全适配于我们的数据类型范围。对于我们的数据类型，我们将其范围设置为[-1, 1]。因此，数据类型的分位数和神经网络权重都需要被归一化到该范围内。对于零均值正态分布，标准差为σ且落在[-1, 1]范围内的情况，信息论上最优的数据类型计算步骤如下：（1）估计理论N(0, 1)分布的2k+1个分位数，以获得正态分布的k位量化数据类型；（2）将该数据类型的值归一化到[-1, 1]范围内；（3）通过绝对最大重缩放，将输入的权重张量归一化到[-1, 1]范围内。一旦权重范围和数据类型范围匹配，就可以像通常一样进行量化。步骤（3）等价于将权重张量的标准差重新缩放以匹配k位数据类型的标准差。<br>$$<br>q_i&#x3D;\frac{1}{2}\left(Q_X\left(\frac{i}{2^k+1}\right)+Q_X\left(\frac{i+1}{2^k+1}\right)\right),<br>$$</p>
<h3 id="Double-Quantization"><a href="#Double-Quantization" class="headerlink" title="Double Quantization"></a>Double Quantization</h3><p>这段文本介绍了一种称为Double Quantization (DQ) 的过程，用于对量化常数进行量化以实现更高的内存节省。精确的4位量化通常需要较小的块大小，但同时也会带来相当大的内存开销。例如，对于权重W，使用32位常数和块大小为64，平均每个参数需要增加32&#x2F;64 &#x3D; 0.5位的内存。Double Quantization可以帮助减少量化常数的内存占用。</p>
<p>具体而言，Double Quantization将第一次量化的量化常数cFP32作为第二次量化的输入。第二次量化得到了量化后的量化常数cFP8和第一层量化常数cFP32。我们在第二次量化中使用了8位浮点数（Floats），块大小为256，因为研究人员发现8位量化不会降低性能，这与Dettmers和Zettlemoyer的研究结果一致。</p>
<p>由于cFP32为正值，我们在量化之前从c2中减去均值，使值围绕零对称分布，以实现对称量化。平均而言，对于块大小为64，这种量化将每个参数的内存占用从32&#x2F;64 &#x3D; 0.5位减少到8&#x2F;64 + 32&#x2F;(64 · 256) &#x3D; 0.127位，每个参数减少了0.373位的内存占用。</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927895.png" alt="image-20230625141917832"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020927319.png" alt="image-20230625142005506"></p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果:"></a>结果:</h1><ul>
<li>a. 详细的实验设置：<ul>
<li>使用QLORA，Guanaco模型系列在Vicuna基准测试中表现优于所有先前发布的模型，达到ChatGPT性能的99.3%，仅需要在单个GPU上进行24小时的微调。</li>
<li>QLORA用于微调超过1，000个模型，并对8个指令数据集、多个模型类型（LLaMA、T5）和模型规模进行了详细的指令跟随和聊天机器人性能分析。</li>
<li>结果表明，使用QLORA在小型高质量数据集上进行微调可以获得最先进的结果，即使使用比先前最先进的模型更小的模型。</li>
</ul>
</li>
<li>b. 详细的实验结果：<ul>
<li>QLORA在三种架构类型上的实验评估表明，其性能与完整模型微调相当，并优于16位适配器微调。</li>
<li>4位NormalFloat数据类型减少了量化误差，而双量化减少了内存开销，使QLORA成为一种有效且内存高效的微调方法。</li>
</ul>
</li>
</ul>
<h1 id="论文2：A-Simple-and-Effective-Pruning-Approach-for-Large-Language-Models"><a href="#论文2：A-Simple-and-Effective-Pruning-Approach-for-Large-Language-Models" class="headerlink" title="论文2：A Simple and Effective Pruning Approach for Large Language Models"></a>论文2：A Simple and Effective Pruning Approach for Large Language Models</h1><blockquote>
<ul>
<li>Title: A Simple and Effective Pruning Approach for Large Language Models (大型语言模型的简单有效修剪方法)</li>
<li>Authors: Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter</li>
<li>Affiliation: Carnegie Mellon University (卡内基梅隆大学)</li>
<li>Keywords: Large Language Models, Network pruning, Weight reconstruction, Sparsity, Magnitude pruning (大型语言模型，网络修剪，权重重构，稀疏性，幅度修剪)</li>
<li>URLs: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.11695v1">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/locuslab/wanda">GitHub</a></li>
<li>2023.06</li>
</ul>
</blockquote>
<h3 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h3><p>本研究提出了一种简单而有效的修剪方法，名为Wanda（基于权重和激活的修剪），用于大型语言模型（LLMs），通过在预训练的LLMs中修剪具有最小幅度的权重乘以相应的输入激活来诱导稀疏性。该方法无需重新训练或权重更新，并且修剪后的LLM可以直接使用。实验证明，Wanda在各种语言基准测试中明显优于幅度修剪，并与涉及大量权重更新的最新方法竞争激烈。</p>
<h1 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息:"></a>背景信息:</h1><ul>
<li>论文背景: 大型语言模型（LLMs）在复杂语言基准测试中表现出色，但由于其庞大的参数规模，需要大量的计算资源。为了降低LLMs的计算成本，研究人员一直在努力寻找有效的压缩方法。</li>
<li>过去方案: 以往的修剪方法要么需要重新训练，这对于规模庞大的LLMs来说很少可行，要么需要解决依赖于二阶信息的权重重构问题，这可能也会带来计算负担。这些方法在压缩LLMs方面的应用相对较少，这与在LLM之前的模型压缩领域的趋势相矛盾。</li>
<li>论文的Motivation: 鉴于以往方法的局限性，本研究旨在提出一种简单而有效的修剪方法，用于直接从预训练的LLMs中找到高效的稀疏网络，而无需重新训练或权重更新。通过观察LLMs中出现的大幅度特征，作者提出了一种基于权重和激活的修剪度量，并在每个输出上比较权重，以实现对LLMs的稀疏化。作者通过实验证明，该方法在性能上明显优于幅度修剪，并且在计算成本上要低于最新的LLMs修剪方法。</li>
<li>幅度修剪（Magnitude Pruning）：是一种剪枝技术，用于稀疏化神经网络中的权重参数。该技术通过保留模型中最重要的权重，将较小幅度的权重置为零或接近零的值，以达到减少模型复杂度和提高推理速度的目的。</li>
<li>Emergent Large Magnitude Features：（ELM特征）是指在训练神经网络时，由于网络的非线性特性和优化过程中的相互作用，某些具有较大幅度的特征或神经元在网络的上层逐渐变得重要。</li>
</ul>
<h1 id="方法-2"><a href="#方法-2" class="headerlink" title="方法:"></a>方法:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021429688.png" alt="image-20230702142920614"></p>
<blockquote>
<p>举个例子说明传统剪枝的额局限性：考虑一个具有两个输入和相应权重的神经元，其中y &#x3D; w1x1 + w2x2，且|w1| ≤ |w2|。传统的幅度剪枝方法总是会选择移除权重w1，但是在LLMs中，输入特征x1和x2的尺度可能存在显著差异。在某些情况下，可能有|x1| ≫ |x2|，导致|w1x1| ≫ |w2x2|。因此，在这种情况下，我们应该选择移除权重w2，因为相比于移除权重w1，它对神经元输出y的影响更小。</p>
<p>为了解决幅度剪枝方法的这个局限性，该段介绍了一种针对LLMs设计的剪枝度量。对于完全连接的线性层，考虑形状为(Cout, Cin)的权重W，输入激活为形状为(N × L, Cin)的X。对于每个权重，作者提出通过其幅度和相应输入特征范数的乘积来评估其重要性。具体地，当前权重Wij的得分由以下公式定义： Sij &#x3D; |Wij| · ∥Xj∥2 其中|·|表示绝对值运算符，∥Xj∥2计算了聚合在N × L个不同标记上的第j个特征的ℓ2范数，最终得分由这两个标量值的乘积计算而得。作者发现，ℓ2范数在衡量激活幅度时往往比其他范数函数（如ℓ1和ℓ∞范数）效果更好。</p>
<p>2范数（L2 norm），也称为欧几里德范数，用于计算向量的长度或矩阵的平方和的平方根。对于一个向量x&#x3D;(x1, x2, …, xn)，其ℓ2范数定义如下：</p>
<p>∥x∥2 &#x3D; sqrt(x1^2 + x2^2 + … + xn^2)</p>
</blockquote>
<ul>
<li>a. 理论背景:<ul>
<li>大型语言模型（LLMs）的重要性和由于其规模和计算要求而带来的挑战。</li>
<li>网络修剪方法的需求，以减小LLMs的规模。</li>
<li>现有方法要么需要重新训练，要么需要计算昂贵的权重重构。</li>
<li>引入一种名为Wanda的新型修剪方法，根据权重的大小乘以相应的输入激活来修剪权重。</li>
<li>Wanda不需要重新训练或权重更新，可以直接用于预训练的LLMs。</li>
<li>Wanda在性能和计算成本方面优于幅度修剪和其他最近的方法。</li>
</ul>
</li>
<li>b. 技术路线:<ul>
<li>Wanda是为修剪大型语言模型（LLMs）而设计的。</li>
<li>引入了一种修剪度量，根据输入特征激活的范数向量和权重的绝对值的点积来计算权重的重要性。</li>
<li>这种度量是稳健的，并且可以使用适量的校准样本进行估计。</li>
<li>Wanda建议使用更局部的修剪粒度级别，按输出逐个比较和删除权重。</li>
<li>修剪过程可以在LLM模型的单次前向传递中无缝实现，无需权重更新或进一步训练。</li>
<li>Wanda还可以扩展到结构化的N:M稀疏性，其中每M个连续权重中最多有N个非零。</li>
</ul>
</li>
</ul>
<h1 id="结果-1"><a href="#结果-1" class="headerlink" title="结果:"></a>结果:</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021447017.png" alt="image-20230702144742941"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021459806.png" alt="image-20230702145942743"></p>
<ul>
<li>a. 详细的实验设置:<ul>
<li>在LLaMA模型系列上评估了Wanda方法，该系列包括各种参数级别的Transformer语言模型。</li>
<li>将修剪方法应用于四个LLaMA模型：LLaMA-7B，LLaMA-13B，LLaMA-30B和LLaMA-65B。</li>
<li>通过在保留验证集上计算困惑度来评估修剪网络的性能。</li>
<li>使用128个从C4训练数据中采样的序列作为校准数据，用于估计输入统计信息。</li>
<li>Perplexity: 是一种用于评估语言模型性能的指标。它衡量模型对给定序列的预测能力和不确定性。较低的困惑度表示模型在给定序列上的预测更准确和更自信。</li>
</ul>
</li>
<li>b. 详细的实验结果:<ul>
<li>将Wanda与两种先前的修剪方法（幅度修剪和SparseGPT）进行比较。</li>
<li>Wanda保持了幅度修剪的简单性，但在发现预训练LLMs中的稀疏网络方面非常有效。</li>
<li>Wanda比SparseGPT更快，因为它不涉及逆计算。</li>
<li>SparseGPT（稀疏GPT）是一种基于稀疏注意力机制的语言模型，它是对GPT（Generative Pre-trained Transformer）模型的改进和扩展。SparseGPT使用稀疏的注意力权重矩阵，其中只有少数非零元素，而其他元素为零。这种稀疏性允许SparseGPT在保持相对较低的计算成本的同时，仍能保持较好的模型性能。</li>
<li>提供了一个比较表，显示每种方法使用的时间复杂度和修剪度量。</li>
</ul>
</li>
</ul>
<h1 id="论文3：LOMO-Full-Parameter-Fine-tuning-for-Large-Language-Models-with-Limited-Resources"><a href="#论文3：LOMO-Full-Parameter-Fine-tuning-for-Large-Language-Models-with-Limited-Resources" class="headerlink" title="论文3：LOMO:Full Parameter Fine-tuning for Large Language Models with Limited Resources"></a>论文3：LOMO:Full Parameter Fine-tuning for Large Language Models with Limited Resources</h1><p><strong>论文：LOMO：利用有限的资源对大型语言模型进行全参数微调</strong></p>
<blockquote>
<p>ArXiv: <a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/2306.09782.pdf">https://arxiv.org/pdf/2306.09782.pdf</a><br>机构：复旦大学<br>时间：2023.6.16<br>Github：<a href="https://link.zhihu.com/?target=https://github.com/OpenLMLab/LOMO">https://github.com/OpenLMLab/LOMO</a></p>
</blockquote>
<h2 id="摘要：-1"><a href="#摘要：-1" class="headerlink" title="摘要："></a>摘要：</h2><p>大型语言模型（LLMs）已经彻底改变了自然语言处理（NLP），但是训练LLMs需要大量的GPU资源。降低LLMs训练的门槛将鼓励更多研究人员参与，从而使学术界和社会受益。虽然现有的方法着重于参数高效微调，即微调或添加少量参数，但很少有人解决了有限资源下调整LLMs的全部参数的挑战。在本文中，我们提出了一种新的优化器LOw-Memory Optimization（LOMO），它将梯度计算和参数更新融合在一步中以减少内存使用。通过将LOMO与现有的内存节省技术集成，我们将内存使用降低到10.8％，与标准方法（DeepSpeed解决方案）相比。因此，我们的方法使单台机器上的65B模型的全参数微调成为可能，该机器配有8×RTX 3090，每个显存为24GB。</p>
<h2 id="IDEA"><a href="#IDEA" class="headerlink" title="IDEA"></a>IDEA</h2><blockquote>
<p>分析了SGD可以finetune LLM的原因，不用Adam改用SGD，在SGD的基础上提出了一个LOw-Memory Optimization（LOMO）的优化器，来全参数finetune LLM，并在下游任务上获得了比lora等更好的效果。（可能因为资源问题没对比Adam的全参数finetune的结果，这个还不够有说服力）8张3090能微调65B的模型了</p>
</blockquote>
<h2 id="背景知识："><a href="#背景知识：" class="headerlink" title="背景知识："></a>背景知识：</h2><blockquote>
<h3 id="1-SGD代表随机梯度下降（Stochastic-Gradient-Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。"><a href="#1-SGD代表随机梯度下降（Stochastic-Gradient-Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。" class="headerlink" title="1.SGD代表随机梯度下降（Stochastic Gradient Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。"></a>1.SGD代表随机梯度下降（Stochastic Gradient Descent），是一种用于训练机器学习模型的优化算法。它是一种迭代算法，用于最小化损失函数，以便模型能够更好地拟合训练数据。</h3><p>下面是SGD的详细解释：</p>
<ol>
<li><p>梯度下降：梯度下降是一种优化算法，用于找到函数的局部最小值或全局最小值。在机器学习中，我们使用梯度下降来最小化模型的损失函数。梯度表示函数在给定点的变化率方向，通过朝着梯度的反方向调整模型参数，我们可以逐步降低损失函数的值，从而找到最优参数。</p>
</li>
<li><p>随机梯度下降：随机梯度下降是梯度下降的一种变种，其目标是加速收敛过程。与传统的梯度下降一次性使用整个训练数据来计算损失函数的梯度相比，随机梯度下降每次迭代仅使用一个训练样本来估计梯度。这样做的好处是计算成本较低，特别是在大规模数据集上，同时它也使得算法更具随机性，有助于避免陷入局部最优解。</p>
</li>
<li><p>迭代过程：SGD的迭代过程如下：</p>
<ul>
<li>初始化模型参数，例如权重和偏置。</li>
<li>将训练数据打乱顺序，以随机顺序遍历样本。</li>
<li>对于每个训练样本，计算损失函数关于模型参数的梯度。</li>
<li>使用梯度来更新模型参数，通常是通过将梯度乘以一个学习率来确定更新步长。</li>
<li>重复以上步骤，直到达到指定的迭代次数或满足停止准则（例如损失函数的收敛）。</li>
</ul>
</li>
<li><p>学习率：学习率是SGD算法中的一个重要参数，用于控制参数更新的步长。较大的学习率可能导致参数更新过大，错过最优解；而较小的学习率可能导致收敛速度缓慢。因此，选择合适的学习率是使用SGD的关键之一。在实践中，可以使用学习率调度策略来逐步减小学习率，以获得更好的收敛性能。</p>
</li>
<li><p>批量大小：除了使用单个样本计算梯度外，SGD还支持使用多个样本计算梯</p>
</li>
</ol>
<p>度。将一批样本一起计算梯度称为小批量随机梯度下降（mini-batch SGD）。小批量SGD通常比单个样本的SGD更稳定，因为它可以减少参数更新的方差，并利用并行计算的优势。批量大小是一个需要调整的超参数，通常根据可用的计算资源和训练数据的规模进行选择。</p>
<p>总结来说，SGD是一种基于梯度下降的优化算法，通过每次迭代使用一个样本（或一小批样本）的梯度来更新模型参数。它是一种高效的训练算法，特别适用于大规模数据集和复杂模型。然而，SGD也有一些缺点，例如可能陷入局部最优解和对学习率的敏感性。因此，在使用SGD时需要进行适当的参数调整和正则化技术的应用。</p>
</blockquote>
<blockquote>
<p>2.Adam算法</p>
<p>Adam是一种优化算法，全称为自适应矩估计优化算法（Adaptive Moment Estimation）。它是一种基于梯度的优化算法，结合了梯度下降和动量优化的思想。Adam算法具有较快的收敛速度和良好的性能，在深度学习领域广泛使用。</p>
<p>下面是Adam算法的详细解释：</p>
<ol>
<li><p>梯度下降：梯度下降是一种基本的优化算法，用于最小化损失函数。它通过计算损失函数关于模型参数的梯度，并朝着梯度的反方向更新参数，以逐步降低损失函数的值。</p>
</li>
<li><p>动量优化：动量优化是一种改进的梯度下降算法，引入了动量（momentum）的概念。动量表示模型更新的惯性，使得参数更新在当前梯度方向的基础上，还考虑了历史梯度的影响。这有助于加速收敛，尤其在存在平坦区域或峡谷的情况下。</p>
</li>
<li><p>自适应学习率：Adam算法引入了自适应学习率的概念，通过自动调整学习率的大小来适应每个参数的变化情况。它基于梯度的一阶矩估计（mean）和二阶矩估计（variance）来自适应地调整学习率。</p>
</li>
<li><p>Adam算法的更新步骤：</p>
<ul>
<li>初始化模型参数和累计变量（一阶和二阶矩估计的初始值）。</li>
<li>在每次迭代中，计算当前的梯度。</li>
<li>更新一阶矩估计和二阶矩估计。</li>
<li>根据一阶和二阶矩估计计算参数更新的方向和大小。</li>
<li>更新模型参数。</li>
<li>重复以上步骤，直到达到指定的迭代次数或满足停止准则。</li>
</ul>
</li>
<li><p>Adam算法的优点：</p>
<ul>
<li>自适应学习率：Adam算法可以自适应地调整每个参数的学习率，根据梯度的一阶和二阶矩估计进行缩放。这有助于在训练过程中平衡收敛速度和参数稳定性。</li>
<li>适用于大规模数据和高维参数：Adam算法对于大规模数据和高维参数的训练具有较好的效果，因为它可以有效地利用梯度信息和自适应学习率。</li>
<li>低内存要求：相对于其他优化算法（如基于Hessian矩阵的方法），Adam算法的内存要求较低，因为它仅需要存储一阶和二阶矩估计。</li>
</ul>
<p>需要注意的是，Adam算法也有一些超参数需要调整，如学习率、动量系数和指数衰减率等，这些超参数的选择可能会对算法的性能产生影响。因此，在实践中，常常需要进行超参数调优来获得最佳的性能。</p>
</li>
</ol>
</blockquote>
<h2 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307020953125.png" alt="image-20230702095337036"></p>
<p>为什么会用SGD：以前的研究经常讨论SGD的三个挑战：1）大曲率损失表面，2）局部最优解，以及3）鞍点（Ruder，2016; Sun等，2020）。现代优化器已经显示出处理1）问题的有效性，并且在某些情况下可以减轻2）和3）问题。然而，当我们将范围限定为微调LLM时，这三个挑战可能会有所不同。</p>
<p>更平滑的损失面（<strong>Smoother loss surface</strong>）</p>
<p>一个重要的假设是LLM的参数空间非常平滑，对参数进行小的扰动不会显着改变损失。有实证结果和理论分析支持这一假设（Hao等，2019）。如果我们相信更大的模型具有更平滑的损失曲面，那么我们可以得出结论：由于LLM的损失表面不应具有很大的曲率，因此1）的问题不是问题。请注意，仅当我们教LLM基于自然语言的任务（或者像以前一样用代码预训练）时，才有效。与预训练任务无关的合成损失函数确实会面临大曲率问题。</p>
<p>局部最优解已经足够（<strong>Local optimum is good enough</strong>）</p>
<p>微调的目标是将LLM调整到新任务和领域中，并且不会显着改变模型本身。因此，局部最优解通常是足够好的解决方案，并且有限的训练数据（与预训练语料库相比）使其难以将模型推向遥远的全局最优解。</p>
<p>算法： LOMO中的融合更新</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021044049.png" alt="image-20230702104423997"></p>
<blockquote>
<ol>
<li><p>输入：模型f(·)具有L层和p个参数，参数θ ∈ Rp，学习率α，最大步数T，训练数据集D，损失函数L。</p>
</li>
<li><p>对于每个步骤t &#x3D; 1, . . . , T 进行以下操作：</p>
<p>2.1 从训练数据集D中随机抽取一个批次B &#x3D; (x, y)，其中x是输入数据，y是对应的目标标签。</p>
<p>2.2 通过模型f(·)进行前向传播，计算预测值ŷ： ŷ ← f(x, θ) 这一步骤用于获取模型的输出。</p>
<p>2.3 计算损失函数ℓ： ℓ ← L(y, ŷ) 这一步骤用于计算模型预测值ŷ与真实标签y之间的差异。</p>
<p>2.4 对于每一层l &#x3D; L, . . . , 1 进行以下操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1 反向传播（Backward propagation）：</span><br><span class="line">      获取当前层的参数集合θl。</span><br><span class="line">2 计算当前层的梯度gl：</span><br><span class="line">      gl ← ∂ℓ/∂θl</span><br><span class="line">      这一步骤通过计算损失函数ℓ关于当前层参数θl的偏导数来获取梯度。</span><br><span class="line"></span><br><span class="line">3 参数更新：</span><br><span class="line">      θl ← θl - α * gl</span><br><span class="line">      使用梯度下降的方式更新当前层的参数θl。学习率α确定了参数更新的步长。</span><br><span class="line">4 清除梯度：</span><br><span class="line">      将当前层的梯度gl设置为None，以清除梯度信息。</span><br></pre></td></tr></table></figure>

<p>2.5 结束当前步骤的循环。</p>
</li>
<li><p>结束算法。</p>
</li>
</ol>
</blockquote>
<blockquote>
<p>这个算法是LOMO中的融合更新算法，它使用随机梯度下降（SGD）的方式对模型的参数进行更新。它在每个步骤中，通过随机抽取一个批次样本进行前向传播和损失计算，然后进行反向传播来计算梯度，并使用梯度下降的方式更新模型的参数。整个过程重复T个步骤，直到达到最大步数T。</p>
</blockquote>
<h2 id="实验："><a href="#实验：" class="headerlink" title="实验："></a>实验：</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021046738.png" alt="image-20230702104644688"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202307021048215.png" alt="image-20230702104809171"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8QLORA-%E6%9C%89%E6%95%88%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/" data-id="cllkmcmel0000ksu7edo7dq8o" data-title="7月第一周QLORA/有效剪枝/lomo全参数微调" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/QLORA-%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/" rel="tag">QLORA 剪枝 lomo全参数微调</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-6月第一周RWKV渐进promptLora-1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/08/21/6%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8RWKV%E6%B8%90%E8%BF%9BpromptLora-1/" class="article-date">
  <time class="dt-published" datetime="2023-08-21T07:47:51.000Z" itemprop="datePublished">2023-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/08/21/6%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8RWKV%E6%B8%90%E8%BF%9BpromptLora-1/">6月第一周RWKV渐进promptLora</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="6月第一周RWKV-渐进prompt-Lora"><a href="#6月第一周RWKV-渐进prompt-Lora" class="headerlink" title="6月第一周RWKV&#x2F;渐进prompt&#x2F;Lora"></a>6月第一周RWKV&#x2F;渐进prompt&#x2F;Lora</h1><h1 id="论文1：RWKV-Reinventing-RNNs-for-the-Transformer-Era"><a href="#论文1：RWKV-Reinventing-RNNs-for-the-Transformer-Era" class="headerlink" title="论文1：RWKV: Reinventing RNNs for the Transformer Era"></a>论文1：RWKV: Reinventing RNNs for the Transformer Era</h1><p>RWKV：为Transformer时代重塑RNN</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.13048.pdf">2305.13048.pdf (arxiv.org)</a></p>
</blockquote>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230609162340.png" alt="image-20230609162340406"></p>
<ul>
<li>a. 本文的研究背景:<ul>
<li>通过介绍RNN和Transformer的优劣势，针对长序列的任务提出了Receptance Weighted Key Value (RWKV)模型，将Transformer和RNN结合起来，实现并行可扩展性及高效率。</li>
</ul>
</li>
<li>b. 过去的方法及其问题和动机:<ul>
<li>介绍了替换或修改神经网络中的注意力机制以实现长序列的可扩展性的各种模型，其中包括MLP-Mixer、Attention Free Transformer (AFT)、Recurrent Memory Transformer和Linear Recurrent Units等。</li>
</ul>
</li>
<li>c. 本文提出的研究方法:<ul>
<li>提出了RWKV模型，利用时间混合组件，将长序列任务的Transformer和RNN的优点相结合，采用线性注意力机制，提高了可扩展性和并行可训练性，同时引入了多种策略以捕捉局部性和长程依赖关系。</li>
</ul>
</li>
<li>d. 方法在任务中的表现:<ul>
<li>在基准数据集上进行了全面系列的实验，在与相似大小的Transformer相当的性能的同时，在规模从1.69亿到140亿的Pile上训练了一个预先训练模型。</li>
</ul>
</li>
</ul>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景:"></a>背景:</h2><ul>
<li>a. 主题及其特点:<ul>
<li>本文的主题为神经网络的注意力机制在处理长序列时面临的挑战，以及RWKV模型的解决方法。</li>
</ul>
</li>
<li>b. 历史发展:<ul>
<li>在过去的相关研究中，RNNs在序列处理任务中具有高效的计算能力，但由于存在并行性和可扩展性方面的限制，无法达到Transformer的性能表现。对于长序列任务而言，Transformer具有卓越的性能，但由于其内存和计算复杂度随序列长度呈二次增长，其对于大规模模型的处理存在困难。</li>
</ul>
</li>
<li>c. 过去的方法:<ul>
<li>过去的研究着眼于替换或修改神经网络中的注意力机制。例如，MLP-Mixer等模型将注意力机制替换为多层感知机 (MLPs)；Attention Free Transformer (AFT)将点积自注意力机制替换为计算效率较高的替代机制；Recurrent Memory Transformer和Linear Recurrent Units等模型则将RNN风格的递归组件进行了修改以增加局部上下文的长度；S4等基于状态空间的模型 (SSM) 也被提出。</li>
</ul>
</li>
<li>d. 过去研究的不足:<ul>
<li>过去的研究中存在的问题包括：替换机制的有效性和性能、长序列的可扩展性、模型的拟合能力、训练效率等问题。</li>
</ul>
</li>
<li>e. 解决当前问题的必要性:<ul>
<li>随着神经网络在各个领域的应用，越来越多的任务需要处理长序列数据，因此提高神经网络的可扩展性和计算效率尤为重要。</li>
</ul>
</li>
</ul>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法:"></a>方法:</h2><ul>
<li>a. 研究的理论基础:<ul>
<li>RWKV模型利用时间混合组件融合了Transformer和RNN的优点，采用线性注意力机制提高了可扩展性和并行可训练性。</li>
</ul>
</li>
<li>b. 文章的技术路线:<ul>
<li>RWKV模型由多个堆叠的残差块组成，在其中包含时间混合和通道混合子块。回归过程的公式化表达如方程14所示，该模型可在时间并行模式下高效并行化。模型的性能在多种NLP任务中进行了基准测试，证明了其训练和内存效率方面的良好表现。</li>
</ul>
</li>
<li>c. 方法的创新性、性能和工作量:<ul>
<li>RWKV模型通过比较特殊的初始化技巧以及乘性交互方法，使得引入线性复杂度的注意力机制变得可行，同时提供了新技术来捕捉局部性和长程依赖关系， 改进了Transformer的局限性。该方法与以前的相似方法不同，RWKV模型采用可并行化和可扩展训练的形式。它在性能上与Transformer相当，并且展现了处理大规模模型的潜力。</li>
</ul>
</li>
<li>d. 研究结论:<ul>
<li>RWKV模型的出现解决了长序列任务中RNN和Transformer所面临的各自的限制。但本文也强调了模型的潜在局限性，包括模型的能力可能受限于之前token的数量，以及 prompt engineering的重要性。最后，该文总结了与RWKV的分块计算方案类似的最近工作，并讨论了无注意力模型作为改进Transformer效率的替代方法。RWKV开辟了一扇新门，可用于建模序列数据中的复杂关系。</li>
</ul>
</li>
</ul>
<h2 id="模型架构图"><a href="#模型架构图" class="headerlink" title="模型架构图"></a>模型架构图</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110548.png" alt="image-20230609204310375"></p>
<ol>
<li><p>Time Mixing</p>
<ol>
<li><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110617.png" alt="image-20230609204719541"></p>
</li>
<li><blockquote>
<p>这个函数描述了处理一个经过RNN网络后的状态的操作。它针对这个状态进行了三种操作，混合、扩展和归一化，并计算输出。这些操作通过一组输入变量(the current input state(x), last input state(last_x), numerator(last_num), denominator(last_den), exponential decay(decay), bonus值(bonus), mixing_coefficients(mix_k&#x2F;mix_v&#x2F;mix_r), query、key和value的weight(Wk, Wv, Wr)以及输出的weight(Wout))来实现。混合是一种对 <code>x</code> 和 <code>last_x</code> 执行加权操作的方法，其中参数是 <code>mix_k/mix_v/mix_r</code>。对于这个混合的结果计算权重 <code>kw</code>, <code>kw</code>取决于值的相似度和重要性。其中，重要性通过进一步加权得到，这里使用exp函数来实现。接下来，<code>numerator</code>和<code>denominator</code>分别用于计算 <code>kw</code>的平均值和标准差，然后再将这个值乘以查询结果得到输出结果。最后函数返回了两个值，输出结果和更新后的状态值。</p>
</blockquote>
</li>
</ol>
</li>
<li><p>Channel Mixing</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110845.png" alt="image-20230609205438997"></p>
<blockquote>
<p>这个函数执行一种基于 <code>x</code>和 <code>last_x</code>的加权和归一化操作，通过一组权重来实现，并返回一个新的值。权重是通过查询值和重要性(用sigmod函数计算)来计算的，其中的值经过了一些加权操作，包括在第二个权重层计算关键值。这个函数仍然返回两个值，输出结果以及状态值。</p>
</blockquote>
<h3 id="增加：传统transformer模型：生成长度为T的序列，需要O-T-2-复杂度"><a href="#增加：传统transformer模型：生成长度为T的序列，需要O-T-2-复杂度" class="headerlink" title="增加：传统transformer模型：生成长度为T的序列，需要O(T^2) 复杂度"></a>增加：传统transformer模型：生成长度为T的序列，需要O(T^2) 复杂度</h3><p>令 F[t] 为 t 时刻的系统状态（高维矢量）。</p>
<p>令 x[t] 为 t 时刻的外部输入信息状态。</p>
<p>预测 F[t+1] 时，需考虑 F[0], F[1], .. F[t]。因此，生成长度 T 的序列，需 O(T^2) 复杂度。</p>
<p>简化版本的公式：</p>
<p>$$<br>F[\mathrm{t}+1]&#x3D;\frac{\sum_{\mathrm{i}&#x3D;0}^{\mathrm{t}} \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}]) \cdot(\mathbf{V}F[\mathrm{i}])}{\sum_{\mathrm{i}&#x3D;0}^{\mathrm{t}} \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}])}<br>$$<br>这里 <strong>Q K V</strong> 是三个可训练的矩阵。</p>
<p>其意义为：</p>
<ul>
<li><p>每个状态 i 对于后续的潜在贡献是 <strong>V</strong>F[i]。</p>
</li>
<li><p>用 <strong>Q</strong>x[t] 矢量，与此前的所有 <strong>K</strong>F[i] 矢量分别做点乘，再 exp，得到 x[t] 与之前各个 F[i] 状态的匹配度。（为什么复杂度高）</p>
</li>
<li><p>如果匹配度<br>$$<br> \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}])<br>$$</p>
<p>越大，<strong>V</strong>F[i] 的权重越大。</p>
</li>
<li><p>分母为归一化因子。</p>
</li>
</ul>
<h2 id="RWKV-2-模型：生成长度-T-的序列，只需-O-T-复杂度（"><a href="#RWKV-2-模型：生成长度-T-的序列，只需-O-T-复杂度（" class="headerlink" title="RWKV-2 模型：生成长度 T 的序列，只需 O(T) 复杂度（&#x3D;&#x3D;"></a><strong>RWKV-2 模型：生成长度 T 的序列，只需 O(T) 复杂度</strong>（&#x3D;&#x3D;</h2><h2 id="启发：用于图片生成或分类transformer改造-）"><a href="#启发：用于图片生成或分类transformer改造-）" class="headerlink" title="启发：用于图片生成或分类transformer改造&#x3D;&#x3D;）"></a>启发：用于图片生成或分类transformer改造&#x3D;&#x3D;）</h2><p>$$<br>F[\mathrm{t}+1]&#x3D;\sigma(\mathbf{R}x[\mathrm{t}]) \cdot \frac{\sum_{\mathrm{i}&#x3D;0}^{\mathrm{t}} \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K}F[\mathrm{i}]) \cdot(\mathbf{V}F[\mathrm{i}])}{\sum_{\mathrm{i}&#x3D;0}^{\mathrm{t}} \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K }F[\mathrm{i}])}<br>$$</p>
<p>这里 <strong>R K V</strong> 是三个可训练的矩阵，<strong>W</strong> 是一个可训练的矢量（代表时间衰减率）。</p>
<p>其意义为：</p>
<ol>
<li><p>每个状态 i 对于后续的潜在贡献&#x3D;&#x3D;是 <strong>V</strong>F[i]。&#x3D;&#x3D;</p>
</li>
<li><p>匹配度由</p>
</li>
<li><p>$$<br>\exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}]) 改为 \sigma(\mathbf{R}x[\mathrm{t}]) \cdot \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K}F[\mathrm{i}])<br>$$</p>
</li>
<li><p>决定，其中</p>
</li>
<li><p>$$<br>中 \sigma 是非线性函数，经实验采用 sigmoid 函数的效果较好。注意 \sigma(\mathbf{R}x[\mathrm{t}]) 不参与归一化，所以将 R 称为 receptance。<br>$$</p>
</li>
<li><p>$$<br>这里 \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) 是显式的距离因子<br>$$</p>
</li>
<li><p>我们进一步变换，将其变为 RNN 递归形式。即，生成 F[t+1] 时，只需考虑 x[t]，以及固定大小的隐状态 A[t] 和 B[t]（A[t] 和 B[t] 是上一步的分子和分母。），无需与此前 F[i] 都进行计算。因此，生成长度 T 的序列，只需 O(T) 复杂度。</p>
</li>
<li><p>$$<br>F[1]&#x3D;\sigma(\mathbf{R }x[0]) \cdot \frac{ \exp (\mathbf{K }F[0]) \cdot(\mathbf{V }F[0])}{\exp (\mathbf{K }F[0])}</p>
<p>F[2]&#x3D;\sigma(\mathbf{R }x[1]) \cdot \frac{ \exp (\mathbf{K }F[1]) \cdot(\mathbf{V }F[1])+\exp (\mathbf{W} ) \cdot \exp (\mathbf{K }F[0]) \cdot(\mathbf{V }F[0])}{ \exp (\mathbf{K }F[1])+\exp (\mathbf{W} ) \cdot \exp (\mathbf{K }F[0])}<br>$$</p>
</li>
<li><p>可以推导得到</p>
</li>
<li><p>$$<br>F[t+1]&#x3D;\sigma(\mathbf{R }x[t]) \cdot \frac{\exp (\mathbf{K}F[\mathrm{t}]) \cdot(\mathbf{V}F[\mathrm{t}])+\exp (\mathbf{W}) \cdot A[\mathrm{t}]}{ \exp (\mathbf{K}F[\mathrm{t}])+\exp (\mathbf{W}) \cdot B[\mathrm{t}]}<br>$$</p>
</li>
</ol>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110854.png" alt="image-20230609210057755"></p>
<p>不同LLM的文本生成期间的累积时间。</p>
<h2 id="Zero-Shot-Performance-of-the-model-on-Common-Sense-Reasoning-Tasks"><a href="#Zero-Shot-Performance-of-the-model-on-Common-Sense-Reasoning-Tasks" class="headerlink" title="Zero-Shot Performance of the model on Common Sense Reasoning Tasks."></a>Zero-Shot Performance of the model on Common Sense Reasoning Tasks.</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110856.png" alt="image-20230609210350167"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110903.png" alt="image-20230609210608382"></p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110900.png" alt="image-20230609210627551"></p>
</li>
</ol>
<h1 id="chatpaper相关"><a href="#chatpaper相关" class="headerlink" title="chatpaper相关"></a>chatpaper相关</h1><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/613055271">我把ChatPaper开源了！用来速读PDF和刷ArXiv论文 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://juejin.cn/post/7220775341727399991">推荐几个可以免费使用的ChatGPT工具 - 掘金 (juejin.cn)</a></p>
</blockquote>
<p>之前准备写一篇专门介绍上述工具类的原理介绍（其实ChatGPT的 插件——chatgpt-retrieval-plugin），但是后来查看了几个项目的源码之后发现，这类工具的主要原理其实比较直观：</p>
<ul>
<li>解析相关输入为文本</li>
<li>将文本分句后获取句子的embedding（这一步目前处理的处理方式大都是根据长度截断）并存储至数据库</li>
<li>用户输入转换为embedding，并在数据库中召回相关性最高的句子集合langchain</li>
<li>将召回的句子与用户输入句子组装为ChaptGPT的输入，获取输出</li>
</ul>
<p>上述思路虽然直观，但要获取更好的结果，其实除了第三步，其余每一步都有优化的空间：</p>
<ul>
<li>文本解析可以针对不同类型的数据针对性解析</li>
<li>文本分句方式可以采取特殊标点进行分句，同时句子embedding也有很多可选生成方法</li>
<li>召回的句子与用户输入句子组装为ChaptGPT的输入，结合任务特定的prompt，获取更适合任务的输出</li>
</ul>
<p>参考langchain</p>
<h1 id="论文2：Progressive-Prompts-Continual-Learning-for-Language-Models-《渐进提示：持续学习的语言模型》"><a href="#论文2：Progressive-Prompts-Continual-Learning-for-Language-Models-《渐进提示：持续学习的语言模型》" class="headerlink" title="论文2：Progressive Prompts: Continual Learning for Language Models 《渐进提示：持续学习的语言模型》"></a>论文2：Progressive Prompts: Continual Learning for Language Models 《渐进提示：持续学习的语言模型》</h1><h2 id="启发：经典论文阅读，"><a href="#启发：经典论文阅读，" class="headerlink" title="启发：经典论文阅读，"></a>启发：经典论文阅读，</h2><p>ICLR2023</p>
<blockquote>
<p>[[2301.12314] 渐进式提示：语言模型的持续学习 (arxiv.org)](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.12314#:~:text=We">https://arxiv.org/abs/2301.12314#:~:text=We</a> introduce Progressive Prompts - a simple and,replay or a large number of task-specific parameters.)</p>
</blockquote>
<h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><ul>
<li>a. 这篇文章的研究背景:<ul>
<li>本文介绍了一种新的连续学习方法Progressive Prompts，用于语言模型的学习。</li>
</ul>
</li>
<li>b. 以往的方法、问题和动机:<ul>
<li>以往的方法存在遗忘问题，在新增任务学习后，之前学习的任务的性能会下降，这限制了语言模型的应用。</li>
</ul>
</li>
<li>c. 本文提出的研究方法:<ul>
<li>Progressive Prompts方法，通过重新描述提示嵌入和添加剩余连接，以防止遗忘，提高性能。</li>
</ul>
</li>
<li>d. 方法在任务上达到的性能:<ul>
<li>在多个数据集上进行测试，结果表明，Progressive Prompts方法在15个文本分类任务中表现优异，比之前的方法平均测试准确率提高20%以上，同时在长序列的任务中也表现出色。</li>
</ul>
</li>
</ul>
<h1 id="背景-1"><a href="#背景-1" class="headerlink" title="背景:"></a>背景:</h1><ul>
<li>a. 主题和特征:<ul>
<li>本文的主题为语言模型的连续学习方法研究，旨在应对以往方法在遗忘问题上的短板。</li>
</ul>
</li>
<li>b. 历史发展:<ul>
<li>以往的连续学习方法遇到了遗忘问题的挑战，需要一个新的方法来应对。</li>
</ul>
</li>
<li>c. 以往的方法:<ul>
<li>以往的方法主要采用重放机制，在每个任务后，使用之前学习任务的数据重新训练模型，但是这样有可能会忘记已掌握的知识。</li>
</ul>
</li>
<li>d. 以往研究中的不足:<ul>
<li>以往的连续学习方法限制了语言模型的应用范围，需要一种能够防止遗忘并提高性能的方法。</li>
</ul>
</li>
<li>e. 当前需要解决的问题:<ul>
<li>如何解决遗忘问题，并且提高模型性能，以便扩大应用范围。</li>
</ul>
</li>
</ul>
<h1 id="方法-1"><a href="#方法-1" class="headerlink" title="方法:"></a>方法:</h1><ul>
<li>a. 本研究的理论基础:<ul>
<li>Progressive Prompts方法，通过重新描述提示嵌入和添加剩余连接，以防止遗忘，提高性能。</li>
</ul>
</li>
<li>b. 文章的技术路线 (逐步):<ul>
<li>(1) 采用模型无关的方式重新描述提示嵌入，使用多层感知器 (MLP) 对提示嵌入进行重新参数化；</li>
<li>(2) 添加剩余连接，以改进优化和避免学习身份映射；</li>
<li>(3) 在多个数据集上进行测试，以比较其性能和可扩展性。</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110907.png" alt="image-20230610085314675"></p>
<blockquote>
<p>第一种方法叫做”progressive networks using prompt tuning”，简称”简单适应法”。这种方法是为每个新任务学习一个单独的提示(prompt)，并且对于每个新任务都会重复使用之前冻结的输入嵌入(frozen input embeddings)。这意味着每个任务都需要重复输入令牌(input tokens)。而第二种方法是”Progressive Prompts”，简称”逐步提示法”。在这种方法中，我们使用相同的输入，并逐步添加新的提示来处理每个新任务。添加新提示时不会修改先前任务的提示。简单来说，逐步提示法是一种更为高效的方法，不需要每次都重复输入令牌，而是逐渐添加新的提示来处理新任务。</p>
</blockquote>
<p>体现在公式上：<br>$$<br>\mathcal{L}(\theta_{P_k})&#x3D;-\sum_{x,y\in T_k}\log p(y|[P_k,…,P_1,x],\theta,\theta_{P_1},…,\theta_{P_k})<br>$$</p>
<blockquote>
<p>这个公式是用来计算在训练当前任务Tk时，对于当前任务的提示参数θPk进行训练的损失函数。其中，x和y分别是当前任务Tk的输入和输出，[P_k,…,P_1,x]表示在之前所有任务的提示P1到Pk以及当前输入x的基础上产生当前输出y的条件概率，这个概率是由语言模型(LM)的参数θ以及提示参数θPk共同决定的。损失函数的目的是最小化负对数似然(log-likelihood)的值，即最大化对数似然函数值。在这个公式中，θ表示LM的参数，θPk表示当前任务的提示参数，其中θ是不可训练的，而θPk只在学习当前任务的过程中可以训练，之后将被冻结。</p>
<p>这个方法的主要目的是在每个新任务中，逐步学习一个任务相关的提示，然后将该提示与之前的所有任务的提示拼接起来，并在输入嵌入前加入该提示。对于每个任务，都需要训练相应的提示参数θPk，以最大化预测正确的概率。可以通过逐步改变所有提示参数的值来使预测输出更准确。此外，在学习每个新任务时，之前学习的任务提示参数将会被冻结，只有当前任务的提示参数可以进行训练。</p>
</blockquote>
<h1 id="增加：公式解读："><a href="#增加：公式解读：" class="headerlink" title="增加：公式解读："></a>增加：公式解读：</h1><p>首先是prompt tuning常使用的目标函数如下：<br>$$<br>\max_{\theta, \theta_P} \log p_{\theta, \theta_P}(y | [P; x])<br>$$</p>
<p>$$</p>
<p>$$</p>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202306171000958.png" alt="image-20230617095731351"></p>
<p>在这项工作中，我们关注的是一种连续学习的设置，其中语言模型将连续地面对一系列的文本分类任务（T_1, \ldots, T_m）。在每个任务中，我们有一组独立同分布的训练样本 (x_i, y_i)<em>{i&#x3D;1}^N，其中 x_i 是输入文本，y_i 是与任务 T_k 相关联的一组预定义标签 Y_k 中的一个标签。我们假设模型的参数为 \Theta，并且在训练和推断过程中可以获得任务的标识信息。因此，跨所有任务的学习目标可以表示为：<br>$$<br>\max</em>{\Theta} \sum_{k&#x3D;1}^m \sum_{(x,y)\in T_k} \log p_{\Theta}(y|x)<br>$$<br>连续学习中最直接的方法是微调（finetuning），它通过按顺序更新所有模型参数来逐个优化每个任务 k（其中 k 的范围是从 1 到 m）的损失函数。任务 k 的损失函数，记为 L_k(\Theta)，可以表示为:<br>$$<br>L_k(\Theta) &#x3D; -\sum_{(x,y)\in T_k} \log p(y|x, \Theta)<br>$$<br>尽管连续微调可以向未来任务传递前向知识，但它经常导致灾难性遗忘。灾难性遗忘是指在学习新任务后，之前任务的性能下降，并最终导致更高的泛化损失。</p>
<p>Progressive Prompts是一种连续学习方法，它为每个新任务Tk逐步学习一个提示P<sub>k</sub>（见图1）。通过Progressive Prompts，我们为任务Tk学习一个独立的提示P<sub>k</sub>，并将其与之前学习的所有提示P<sub>i</sub>（其中i &lt; k）连接起来，然后将其添加到输入嵌入之前。在训练过程中，语言模型的参数θ始终保持冻结，而与提示P<sub>k</sub>相关的参数θP<sub>k</sub>仅在学习任务Tk时可训练，然后冻结。</p>
<p>任务Tk（其中k ∈ {1…m}）的训练目标是找到提示参数θP<sub>k</sub>，使得在我们的逐步提示和冻结的基础模型下，训练样本的负对数概率最小化：</p>
<p>$$<br>L(θPk) &#x3D; − Σ(log p(y|[Pk, …, P1, x], θ, θP1, …, θPk))<br>$$</p>
<p>其中x,y∈Tk表示任务Tk中的训练示例，p(y|[Pk, …, P1, x], θ, θP1, …, θPk)表示在给定提示序列[Pk, …, P1]、模型参数θ和提示参数θP1, …, θPk的情况下预测目标y的条件概率。</p>
<p>简而言之，Progressive Prompts通过学习逐步添加的提示序列，将每个任务的特定信息引入到语言模型中，同时保持之前任务的知识。这种方法通过冻结基础模型的参数，只训练特定任务的提示参数，从而避免了对整个模型进行大规模调整，提高了模型的可扩展性和效果。</p>
<p>不同点：</p>
<ol>
<li>渐进学习：Progressive Prompts通过逐步学习的方式处理每个新任务。对于每个新任务Tk，系统学习一个独立的提示P_k，该提示包含了任务特定的信息。这种渐进的学习方式允许模型在处理新任务时逐步积累知识。</li>
<li>提示序列：Progressive Prompts引入了一个提示序列[P_k, …, P_1]，其中P_k是针对任务Tk的提示，P_i是之前任务Ti的提示（i &lt; k）。在模型输入中，将提示序列与输入嵌入进行连接，以将任务特定的信息引入到模型中。</li>
<li>提示参数：与渐进学习相对应的是冻结和可训练的参数。在Progressive Prompts中，模型参数θ是冻结的，不进行训练，而与每个任务相关的提示参数θPk只在学习任务Tk期间进行训练，然后被冻结。这种方式保持了基础模型的稳定性，只对特定任务的提示参数进行更新，避免了对整个模型进行大规模调整。</li>
</ol>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论:"></a>结论:</h1><ul>
<li>a. 工作的意义:<ul>
<li>本文介绍了一种新的连续学习方法Progressive Prompts，解决了以往方法遗忘问题和性能下降问题，具有重要的实际应用价值。</li>
</ul>
</li>
<li>b. 创新、性能和工作量:<ul>
<li>Progressive Prompts方法只需要训练语言模型总参数的0.1%，是传统连续学习方法的轻量级替代方法，具有更好的性能和扩展性。</li>
</ul>
</li>
<li>c. 研究结论:<ul>
<li>在多个数据集上的测试结果表明，Progressive Prompts方法在15个文本分类任务中表现优异，能够防止遗忘并提高性能。在长序列的任务中，也比之前的方法表现出色。</li>
</ul>
</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230612154037.png" alt="image-20230610090348552"></p>
<blockquote>
<p>Finetune：在任务序列上训练所有模型参数（不添加任何正则化约束或重放来自先前任务的样本）。</p>
<p>· EWC：使用正则化损失来微调整个模型，以防止更新可能干扰先前学习的任务的参数。</p>
<p>· A-GEM：保存过去任务中的示例，并基于检索到的示例限制用于在新任务上更新模型的梯度。·</p>
<p>体验回放：使用内存缓冲区微调整个模型，并在学习新任务时重放旧任务的样本以避免遗忘。·</p>
<p>MBPA++（de Masson D ‘Autume等人，2019）：用情景记忆增强BERT，保存所有看到的例子。在训练期间执行回放，在测试期间执行本地调整。· </p>
<p>IDBR（Huang等人，2021）：BERT特定的方法，使用数据重放和正则化损失连续训练整个模型，将句子表示分解应用到特定于任务的空间和任务通用空间。当前SOTA对BERT的CL基准测试。·</p>
<p>progprompt：为每个任务训练单独的软提示，同时保持原始模型冻结。这种设置将消除灾难性遗忘，因为当学习新任务时，每个任务的参数不会改变，但不会导致向前传输。</p>
<p>·PromptTuning：在所有任务上按顺序训练共享软提示，同时保持原始模型参数冻结。</p>
<p>· LFPT5：连续地训练软提示，该软提示同时学习解决任务并生成训练样本，该训练样本随后用于体验重放。当前SOTA对T5的CL基准测试。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202308211548121.png" alt="image-20230610090708450"></p>
<blockquote>
<p>表2比较了常见的CL方法，包括T5和BERT模型的SOTA方法（Qin &amp; Joty，2021; Huang等人，2021年），连续学习15个任务。我们报告了T5-Large和BERT-base模型获得的三个任务订单（8，9和10）的平均结果。我们在附录B.1中提供了每个订单的完整非平均结果。为了研究有限数据设置的影响，我们在不同的数据集大小上进行训练，每个类有20，200和1000个样本。</p>
<p>MTL表示多任务学习。标有 * 的方法只训练一个软提示，同时保持模型冻结，其他方法训练整个模型。</p>
</blockquote>
<h1 id="论文3-LORA-LOW-RANK-ADAPTATION-OF-LARGE-LANGUAGE-MODELS"><a href="#论文3-LORA-LOW-RANK-ADAPTATION-OF-LARGE-LANGUAGE-MODELS" class="headerlink" title="论文3 LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS"></a>论文3 LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</h1><h2 id="LoRA-大模型的低秩自适应微调模型-qlora"><a href="#LoRA-大模型的低秩自适应微调模型-qlora" class="headerlink" title="LoRA:大模型的低秩自适应微调模型 qlora"></a>LoRA:大模型的低秩自适应微调模型 qlora</h2><h3 id="启发：用在cv-qlora-怎么找到分解的矩阵"><a href="#启发：用在cv-qlora-怎么找到分解的矩阵" class="headerlink" title="启发：用在cv qlora 怎么找到分解的矩阵"></a>启发：用在cv qlora 怎么找到分解的矩阵</h3><h1 id="基本信息："><a href="#基本信息：" class="headerlink" title="基本信息："></a>基本信息：</h1><ul>
<li>标题： LORA：大型语言模型的低秩适应 （LORA： 大型语言模型的低秩自适应）</li>
<li>作者：Edward Hu， Yelong Shen， Phillip Wallis， Zeyuan Allen-Zhu， Yuan， Yuanzhi Li， Shean Wang， Lu Wang， Weizhu Chen</li>
<li>隶属关系： 微软公司</li>
<li>关键词：自然语言处理;预培训;微调;低秩适配</li>
<li>网址： <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685%EF%BC%8C">https://arxiv.org/abs/2106.09685，</a> GitHub： <a target="_blank" rel="noopener" href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a></li>
</ul>
<h1 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h1><ul>
<li>a. 本文研究背景：<ul>
<li>简述了适用于下游任务大型预训练语言模型的低秩自适应方法LoRA的提出</li>
</ul>
</li>
<li>b. 过去的方法、问题和动机：<ul>
<li>描述了adapter层和prompt直接优化等现有方法的缺点，以及提出本文方法的动机</li>
</ul>
</li>
<li>c. 本文提出的研究方法：<ul>
<li>本文提出了在预训练模型的每个层中引入可训练秩分解矩阵的方法，从而大大降低下游任务的可训练参数数量</li>
</ul>
</li>
<li>d. 方法在下游任务中的表现：<ul>
<li>在RoBERTa、DeBERTa、GPT-2和GPT-3等模型下，本文提出的方法可以使可训练参数和GPU内存需求分别减少10，000倍和3倍，并保持达到或超过模型质量的水平。本文所提出的方法计算和内存效率均优于完全微调方法，无需增加推理延迟，与其他方法兼容</li>
</ul>
</li>
</ul>
<h1 id="背景："><a href="#背景：" class="headerlink" title="背景："></a>背景：</h1><ul>
<li>a. 主题和特点：<ul>
<li>本文主题是下游任务大型预训练语言模型的自适应问题，特点在于提出了使用可训练秩分解矩阵的方法</li>
</ul>
</li>
<li>b. 历史发展：<ul>
<li>描述了早期下游任务自适应方法的缺点和局限性</li>
</ul>
</li>
<li>c. 过去的方法：<ul>
<li>描述了adapter层和prompt直接优化等现有方法的问题，以及本文方法的创新之处</li>
</ul>
</li>
<li>d. 过去研究的缺陷：<ul>
<li>描述了现有方法的推理延迟、计算和内存效率不高等问题</li>
</ul>
</li>
<li>e. 当前需要解决的问题：<ul>
<li>描述了大规模生产中的适应问题，如何提高计算和内存效率</li>
</ul>
</li>
</ul>
<h1 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h1><ul>
<li>a. 研究的理论基础：<ul>
<li>描述了使用可训练秩分解矩阵的理论基础</li>
</ul>
</li>
<li>b. 文章的技术路线（步骤）：<ul>
<li>对于预训练模型的注意力权重，本文利用了低秩参数化和秩分解矩阵的方法</li>
<li>本文方法对每个预训练模型层进行秩分解，有效减少可训练参数数量</li>
<li>本文方法可以切换在下游任务中是否冻结MLP和LayerNorm层</li>
</ul>
</li>
<li>c. 实现效果：<ul>
<li>描述了在RoBERTa、DeBERTa、GPT-2和GPT-3等模型下，本文所提出的方法可以使可训练参数和GPU内存需求分别减少10，000倍和3倍，并保持达到或超过模型质量的水平</li>
<li>描述了本文所提出的方法计算和内存效率均优于完全微调方法，无需增加推理延迟，与其他方法兼容</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110917.png" alt="image-20230610092930638"></p>
<blockquote>
<p><strong>对于预训练的权重矩阵W0，可以让其更新受到用低秩分解表示后者的约束:</strong><br>$$<br>W_{0}+\Delta W &#x3D;W_0+BA,<br>$$<br><strong>在训练过程中，W0被冻结，不接受梯度更新，而A和B包含可训练参数。当h&#x3D;W0x时，修正后的正向传播变为:</strong><br>$$<br>h&#x3D;W_0x+\Delta Wx&#x3D;W_0x+BAx<br>$$<br><strong>对A使用随机高斯初始化，对B使用零初始化，因此ΔW&#x3D;BA在训练开始时为零（这点需要注意）。</strong></p>
<p><strong>这种方法的一个优点是，当部署到生产环境中时，只需要计算和存储W&#x3D;W0+BA，并像往常一样执行推理。与其他方法相比，没有额外的延迟，因为不需要附加更多的层。</strong></p>
<p><strong>在Transformer体系结构中，自关注模块中有四个权重矩阵(Wq、Wk、Wv、Wo)， MLP模块中有两个权重矩阵。LoRA只对下游任务调整关注权重，并冻结MLP模块。所以对于大型Transformer，使用LoRA可减少高达2&#x2F;3的VRAM使用量。比如在GPT-3 175B上，使用LoRA可以将训练期间的VRAM消耗从1.2TB减少到350GB。</strong></p>
</blockquote>
<p>补充公式解释：</p>
<blockquote>
<ol>
<li>神经网络中的权重矩阵：神经网络包含多个密集层，这些层执行矩阵乘法操作。这些操作涉及权重矩阵，该矩阵在每一层中都有不同的值。</li>
<li>全秩权重矩阵和内在维度：通常情况下，这些权重矩阵是全秩的，即它们的秩等于它们的最小维度。然而，Aghajanyan等人（2020）的研究表明，预训练的语言模型具有较低的”内在维度”，即尽管权重矩阵具有全秩，但它们在实际学习中可以被投影到较小的子空间中。</li>
<li>假设和约束：基于上述观察，论文假设权重矩阵在适应期间也具有较低的”内在秩”。为了约束权重的更新，他们使用了低秩分解。对于预训练的权重矩阵W0，它被分解为W0 + ΔW，其中B和A是较低秩的矩阵（B的维度为d×r，A的维度为r×k），并且秩r远小于min(d, k)。这样的低秩分解有助于减少模型参数和计算成本。</li>
<li>参数更新和冻结：在训练期间，原始的预训练权重矩阵W0被冻结，不接收梯度更新，而A和B成为可训练的参数。因此，更新的权重矩阵为W0 + ΔW，其中ΔW &#x3D; BA。这样的约束确保了更新的权重矩阵仍然具有较低的秩。</li>
<li>前向传播和缩放：在前向传播过程中，对于输入x，修改后的权重矩阵的前向传播计算为h &#x3D; W0x + ΔWx &#x3D; W0x + BAx。这样的修改确保了权重矩阵与原始输入相乘后，它们各自的输出向量在坐标方向上相加。</li>
<li>缩放因子αr：在训练开始时，ΔW &#x3D; BA为零，通过对A使用随机高斯初始化，对B使用零初始化。然后，用常数αr来缩放ΔWx，其中α是r中的常数。在使用Adam等优化算法进行训练时，适当缩放初始化可以使α的调整类似于学习率的调整。因此，只需将α设置为第一个尝试的r值，而不进行进一步的调整。这样的缩放有助于减少重新调整超参数的需求。</li>
</ol>
</blockquote>
<h2 id="低秩分解用于transformer架构的好处"><a href="#低秩分解用于transformer架构的好处" class="headerlink" title="低秩分解用于transformer架构的好处"></a>低秩分解用于transformer架构的好处</h2><blockquote>
<p>将低秩分解应用于Transformer模型中的注意力权重矩阵的好处和限制。</p>
<ol>
<li>权重矩阵子集：在Transformer架构中，自注意力模块包含四个权重矩阵（Wq、Wk、Wv、Wo），而MLP模块包含两个权重矩阵。这里的讨论限制在仅调整注意力权重，即Wq（或Wk、Wv），并将MLP模块冻结，不对其进行训练。</li>
<li>好处：最显著的好处是减少内存和存储使用量。对于大型的Transformer模型，在使用Adam优化器训练时，如果秩r远小于注意力权重矩阵的维度dmodel，那么VRAM（显存）的使用量可以减少2&#x2F;3，因为不需要存储优化器状态的冻结参数。例如，在GPT-3175B模型上，训练期间的VRAM消耗从1.2TB减少到350GB。此外，当r&#x3D;4且仅调整查询和值投影矩阵时，检查点文件的大小约减少了10000倍，从350GB减少到35MB。这使得可以使用较少的GPU进行训练，并避免I&#x2F;O瓶颈。另一个好处是，在部署时可以以更低的成本切换任务，只需交换低秩分解的权重，而不需要交换所有参数。这允许创建许多定制模型，可以在将预训练权重存储在VRAM中的机器上进行动态切换。此外，相对于完全微调（fine-tuning）的方法，观察到GPT-3175B在训练期间的速度提高了25%，因为不需要计算大多数参数的梯度。</li>
<li>限制：这种方法的限制是它仅适用于调整注意力权重，而不调整MLP模块的权重。这意味着MLP模块在下游任务中不会被训练，可能会限制其适应性和性能。此外，低秩分解的方法也会引入近似误差，因为原始的全秩权重矩阵被近似为低秩的矩阵。</li>
</ol>
<p>总之，将低秩分解应用于Transformer模型中的注意力权重矩阵可以减少内存和存储使用量，并提高训练速度。这种方法还允许在部署时以较低的成本切换任务，并创建定制模型。然而，它的限制是仅适用于注意力权重，而MLP模块不会进行训练，并且近似误差可能会对模型性能产生影响。</p>
</blockquote>
<h3 id="问题补充1：什么是低秩分解？"><a href="#问题补充1：什么是低秩分解？" class="headerlink" title="问题补充1：什么是低秩分解？"></a>问题补充1：什么是低秩分解？</h3><blockquote>
<p>低秩分解是一种矩阵分解的技术，它将一个高维矩阵分解为两个或多个低维矩阵的乘积。这种分解可以帮助我们降低数据的维度，提取数据中的重要信息，并减少存储和计算的成本。</p>
<p>下面是一步一步详细解释低秩分解的过程：</p>
<ol>
<li>原始矩阵：我们首先有一个原始矩阵，通常表示为一个m行n列的矩阵，记作M。</li>
<li>目标：我们的目标是将原始矩阵M分解为两个低秩矩阵的乘积，以便能够在降低维度的同时保留尽可能多的信息。</li>
<li>假设：假设我们将原始矩阵M分解为两个低秩矩阵U和V的乘积，其中U是一个m行k列的矩阵，V是一个k行n列的矩阵。k是一个较小的值，通常远小于m和n，表示我们希望降低的维度。</li>
<li>乘积：根据上述假设，我们得到分解后的矩阵表达式为M ≈ UV，其中≈表示近似等于。U和V的乘积近似地重构了原始矩阵M。</li>
<li>低秩性：由于U和V的维度较小，因此它们的秩也较小。这意味着它们包含的信息量较少，而丢弃的信息则通过U和V的乘积进行近似重构。</li>
<li>参数学习：我们的目标是找到合适的U和V，使得它们的乘积近似地重构原始矩阵M。这涉及到通过优化方法（如最小二乘法、梯度下降等）来学习U和V的参数。</li>
<li>选择低秩：选择合适的低秩值k是低秩分解的关键。较小的k可以帮助我们减少数据的维度，但可能会导致信息的损失。而较大的k可以更好地重构原始矩阵，但会增加存储和计算的成本。因此，需要在维度降低和信息保留之间进行权衡。</li>
</ol>
<p>总之，低秩分解是通过将原始矩阵分解为两个或多个低秩矩阵的乘积来降低数据维度并提取重要信息的技术。这种分解可以通过学习适当的参数来实现。</p>
</blockquote>
<h2 id="问题补充2：什么是矩阵的秩？"><a href="#问题补充2：什么是矩阵的秩？" class="headerlink" title="问题补充2：什么是矩阵的秩？"></a>问题补充2：什么是矩阵的秩？</h2><blockquote>
<p>矩阵的秩是指矩阵中线性无关的行或列的最大数量。简单来说，它表示矩阵中包含的独立信息的维度。</p>
<p>具体来说，对于一个m行n列的矩阵，它的秩可以通过以下方式确定：</p>
<ol>
<li>将矩阵转化为行简化阶梯形式（也称为行最简形式）或列简化阶梯形式。</li>
<li>计算行（或列）简化阶梯形式中非零行（或列）的数量。</li>
</ol>
<p>这个数量就是矩阵的秩。在行简化阶梯形式中，非零行的数量即为矩阵的行秩；在列简化阶梯形式中，非零列的数量即为矩阵的列秩。</p>
<p>矩阵的秩具有一些重要的性质：</p>
<ol>
<li>矩阵的行秩和列秩是相等的，它们都是矩阵的秩。</li>
<li>矩阵的秩不会超过它的行数或列数。即秩的最大值为min(m, n)。</li>
<li>当矩阵的秩等于min(m, n)时，该矩阵被称为满秩矩阵。</li>
</ol>
<p>矩阵的秩在线性代数和矩阵计算中具有广泛的应用。它可以用于判断矩阵的线性相关性、解决线性方程组、计算矩阵的逆、确定矩阵的特征值等。秩的概念也扩展到更一般的线性空间中，不仅仅适用于矩阵。</p>
</blockquote>
<h1 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h1><ul>
<li>a. 研究工作的意义：<ul>
<li>描述了本文所提出的方法可以在大型预训练语言模型自适应任务中显著减少计算和内存开销，同时达到或超过原有微调方法的模型质量水平</li>
</ul>
</li>
<li>b. 创新性、性能和工作量：<ul>
<li>描述了本文所提出的方法可以将下游任务的可训练参数数量降低数千倍，提高计算效率；而在模型质量上的表现达到或超过了原有微调方法</li>
</ul>
</li>
<li>c. 研究结论（列出重要点）：<ul>
<li>描述了低秩自适应方法可以使计算和内存效率显著提高，同时保证模型质量可达到或超过原有微调方法的水平</li>
<li>描述了超参优化需在不同任务中分别进行</li>
<li>描述了本文作者提供了代码和模型，方便其他学者和开发者使用</li>
</ul>
</li>
</ul>
<p>本文介绍了一种名为LoRA的低秩自适应方法，用于大型预训练语言模型的下游任务。相较于当前已有的方法，本文所提出的方法可以大幅减少可训练参数数量和GPU内存需求，同时保持达到或超过原有微调方法的模型质量水平，使计算和内存效率大幅提高，无需增加推理延迟，与其他方法兼容。本文的贡献在于在下游任务自适应问题中提出一种更加精细化的自适应方法，有望在大规模生产中得到更广泛的应用。回顾历史发展，早期下游任务自适应方法的推理效率较低，而当前的adapter层和prompt直接优化等现有方法仍存在效率和质量问题。未来工作中，需要在不同任务和不同模型上继续优化超参和算法，开发更有效的自适应策略，以面对不断增长的大规模自然语言处理任务。</p>
<h1 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h1><p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110922.png" alt="image-20230610094344439"></p>
<ol>
<li>MNLI (Multi-Genre Natural Language Inference) - 包含来自不同来源和类型的句子对，用于推断它们之间的关系。</li>
<li>SST-2 (Stanford Sentiment Treebank) - 包含电影评论，用于情感分析任务。</li>
<li>MRPC (Microsoft Research Paraphrase Corpus) - 包含语义相似度匹配的句子对，用于文本匹配任务。</li>
<li>CoLA (Corpus of Linguistic Acceptability) - 包含含有不自然或不合乎语法的句子，用于句法分析的任务。</li>
<li>QNLI (Question NLI) - 包含问题和答案段落，用于推断问题和答案之间的关系，类似于MNLI数据集。</li>
<li>QQP (Quora Question Pairs) - 包含Quora上的问题对，目的是判断两个问题是否语义上相似。</li>
<li>RTE (Recognizing Textual Entailment) - 包含语言推理任务中的句子对，用于推断逻辑关系。</li>
<li>STS-B (Semantic Textual Similarity Benchmark) - 包含语义相似度匹配的句子对，用于评估不同模型在语义相似性任务上的表现。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/img/20230611110922.png" alt="image-20230610094538158"></p>
<blockquote>
<p>E2E NLG Challenge是一个用于测试和评估自然语言生成（NLG）系统的国际评估比赛。 E2E代表End-to-End，因为该竞赛关注的是完整的NLG系统，即从输入到输出的完整流程，而不仅仅是一个单一的任务。该比赛目标是推动NLG系统的发展和进步，包括对话系统、虚拟助手、智能个人助理等各种应用。竞赛的任务是将给定的语言输入转换为文本输出，例如在餐馆推荐中自动产生口头和书面指南。竞赛数据集基于各种领域来反映不同应用场景，包括餐馆、旅游、酒店等。评估基于多个自动生成描述的相关度和多样性的质量，以及诸如BLEU，NIST，METEOR，ROUGE - L和CIDEr等常见NLG评估指标。 E2E NLG Challenge自2017年以来已经成为自然语言生成研究中的重要活动，吸引了来自全球各地的研究人员和团队的广泛参与。</p>
<ul>
<li>Trainable Parameters：模型的可训练参数数量。</li>
<li>BLEU：一种常用的评估机器翻译质量的指标，使用n-gram重叠度来计算预测文本与参考文本之间的相似度。</li>
<li>NIST：另一种机器翻译质量的指标，比BLEU更加复杂，使用变换权重计算机生成文本与参考文本之间的相似度。</li>
<li>METEOR：综合评估指标，考虑机器翻译的准确率和流畅性，以及文本中出现的词汇、短语或实体名称等。</li>
<li>ROUGE-L：用于评估机器生成文本与参考文本之间的相似度的指标。它基于最长公共子序列（LCS）的概念，考虑到了长短不一的文本之间的差异。</li>
<li>CIDEr：评估多个自动生成描述的相对质量，并鼓励生成多样性和质量的指标。</li>
</ul>
</blockquote>
<h1 id="论文4-pile-of-law"><a href="#论文4-pile-of-law" class="headerlink" title="论文4 pile of law"></a>论文4 pile of law</h1><h1 id="基本信息：-1"><a href="#基本信息：-1" class="headerlink" title="基本信息："></a>基本信息：</h1><ul>
<li>题目：从法律和256GB开源法律数据集中学习负责任的数据过滤</li>
<li>作者：Peter Henderson，Mark S. Krass，Lucia Zheng，Neel Guha，Christopher D. Manning，Dan Jurafsky，Daniel E. Ho</li>
<li>隶属关系：斯坦福大学（Peter Henderson，Mark S. Krass，Lucia Zheng，Christopher D. Manning，Dan Jurafsky），斯坦福法学院（Daniel E. Ho）</li>
<li>关键词：大语言模型，负责任的数据过滤，预训练，法律，数据审查</li>
<li>网址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2207.00220%E3%80%81https://github.com/PileFoundation/pile">https://arxiv.org/abs/2207.00220、https://github.com/PileFoundation/pile</a></li>
</ul>
<p>注意：该论文尚未正式出版，但预印本可在arXiv上找到。GitHub 代码包含 256GB Pile of Law 数据集、用于训练模型的代码以及用于探索数据集的 Jupyter 笔记本。</p>
<h1 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h1><ul>
<li>a. 本文的研究背景：<ul>
<li>本文研究大型语言模型对于基于既定偏见或不当材料的预训练所带来的风险，同时探讨了现有的内容过滤方法存在的问题。本文提供了一个新的数据集，称为Pile of Law，包含256GB的开源英语法律和行政数据，可以供研究者预训练法律领域语言模型，提高人们获取司法方面的信息的准确性和便利性。同时，本文将法律领域对有毒内容和隐私信息处理的规范转化成可操作的经验教训，指导研究者们开发更加细致的过滤机制。另外，本文提供了一种新的基于模型的处理方法，通过学习Pile of Law中的隐式卫生规则，向研究模型过滤提供了一个新的方向。</li>
</ul>
</li>
<li>b. 过去的方法、问题和动机：<ul>
<li>过去的内容过滤方法存在不少问题，如偏见或者会给下游应用带来威胁。一些团队采取了内容过滤处理的流程，但是其他团队则没有采取。过滤有毒内容或隐私信息会涉及到一些复杂的权衡，在透明性和保护隐私之间寻求平衡的挑战是很大的。</li>
</ul>
</li>
<li>c. 本文提出的研究方法：<ul>
<li>本文提供了一个新的数据集Pile of Law，用于解决大型语言模型预训练过程中的过滤问题，并从法律和行政的规范出发，探讨内容过滤机制的实现方法。本文指出，从法律标准出发去制定过滤机制可能是提供更透明、更有责任感的过滤方法的一种途径。</li>
</ul>
</li>
<li>d. 本文方法所获得的结果和性能：<ul>
<li>Pile of Law数据集包含35个公共法律记录、行政规则和立法记录的数据源。通过研究数据过滤问题和相关挑战，本文提出了一种更全面更细致的过滤机制，使用上下文来确定是否包含可能涉及到隐私或有毒的信息。通过三个案例研究，本文展示了Pile of Law如何被用来识别特定场景下的隐私和有毒信号。研究结果表明，基于上下文的过滤机制或许能够帮助确保生成模型的准确性同时保护隐私权。</li>
</ul>
</li>
</ul>
<h1 id="背景：-1"><a href="#背景：-1" class="headerlink" title="背景："></a>背景：</h1><ul>
<li>a. 主题和特征：<ul>
<li>本文的主题是大型语言模型的训练和过滤问题。</li>
</ul>
</li>
<li>b. 历史发展：<ul>
<li>本文并没有涉及到历史发展部分的描述。</li>
</ul>
</li>
<li>c. 过去的方法：<ul>
<li>本文提到，过去的内容过滤方法存在一些问题，但并没有具体介绍。</li>
</ul>
</li>
<li>d. 过去研究的不足：<ul>
<li>本文并未进一步具体描述过去研究的不足在何处。</li>
</ul>
</li>
<li>e. 当前需要解决的问题：<ul>
<li>当前需要解决的问题围绕大型语言模型的内容过滤方法展开，同时需要考虑透明度和隐私保护之间的平衡问题。</li>
</ul>
</li>
</ul>
<h1 id="方法：-1"><a href="#方法：-1" class="headerlink" title="方法："></a>方法：</h1><ul>
<li>a. 研究的理论基础：<ul>
<li>本文研究基于法律和行政规范的内容过滤方法，具有较强的实践应用价值。</li>
</ul>
</li>
<li>b. 本文的技术路线：<ul>
<li>本文提出了一个新的数据集Pile of Law，同时对过去研究中存在的问题进行了探究，总结了法律标准对于内容过滤的经验教训。为此，本文首先描述了数据集的构建方法，同时对数据集的使用范围、特点和可能的应用做了具体介绍;其次，本文提出了一种基于上下文的过滤方法，借鉴了Pile of Law中的卫生规则;同时，本文阐述了如何使用Pile of Law数据集在有毒和隐私数据过滤中加入规则。总体而言，本文提供了一种完整的、跨学科的解决方案，可以帮助研究人员学习将法律准则应用于内容过滤，实现透明性和隐私保护之间的平衡。</li>
</ul>
</li>
</ul>
<h1 id="结论：-1"><a href="#结论：-1" class="headerlink" title="结论："></a>结论：</h1><ul>
<li>a. 研究的意义：<ul>
<li>本文的研究意义在于提供了一个新的数据集和基于法律标准的内容过滤机制，可以用于更准确、更可靠的大型语言模型的训练。</li>
</ul>
</li>
<li>b. 创新、性能和工作量：<ul>
<li>本文提出了大型语言模型预训练过程中从法律角度出发制定过滤机制的新思路，为解决文本数据中隐私和有毒的问题提供了新的方法和思路；同时，借助Pile of Law数据集，本文演示了如何开发基于上下文的过滤器，以平衡准确性和隐私保护；同时还特别强调了研究如何实现准确的、价值导向的有毒信息过滤，以防止内容过滤机制因意识形态偏见而不稳定的情况出现。</li>
</ul>
</li>
<li>c. 研究结论：<ul>
<li>本文的研究结论主要有三点：一是，Pile of Law数据集是一个适用于大型语言模型预训练的重要数据集，为语言模型的应用提供了更加准确、更有信任的文本数据;二是，从法律标准出发制定过滤机制的思路不仅可以解决大型语言模型训练过程中存在的问题，同时也有助于制定更可靠、更负责任的内容过滤规则;三是，通过借鉴Pile of Law中的卫生规则，采用基于上下文的过滤方式，可以在保护隐私权的前提下实现更精准的过滤机制。</li>
</ul>
</li>
</ul>
<h2 id="数据集细分"><a href="#数据集细分" class="headerlink" title="数据集细分"></a>数据集细分</h2><blockquote>
<ol>
<li>法律的案例意见和归档<ol>
<li>法院意见书、法院备审记录和法院文件。</li>
<li>美国最高法院案卷条目和法院文件。</li>
<li>美国退伍军人上诉委员会决定。</li>
<li>美国联邦贸易委员会咨询意见</li>
<li>美国国家劳工关系委员会的决定。</li>
<li>美国司法部移民审查移民和国籍决定执行办公室。</li>
<li>美国税务法院PLR语料库。</li>
<li>美国劳工部雇员补偿上诉委员会的决定。</li>
<li>欧洲人权法院的意见</li>
<li>加拿大法院意见。</li>
</ol>
</li>
<li>Legal Analyses法律分析<ol>
<li>美国法律的顾问办公室备忘录。</li>
<li>美国司法部监察长报告。</li>
</ol>
</li>
<li>Laws<ol>
<li>美国联邦法规美国州代码美国联邦证据规则联邦民事诉讼法。</li>
<li>美钞，美钞联邦登记册。 U.S. Bills, U.S. Federal Register.</li>
<li>美国创始人信。U.S. Founders Letters.</li>
<li>世界宪法。World Constitutions.</li>
<li>EUR-Lex.’</li>
</ol>
</li>
<li>合同&#x2F;业务文件<ol>
<li>信用卡协议，服务条款，埃德加Contracts，Atticus Contracts。</li>
</ol>
</li>
<li>Conversations 谈话？<ol>
<li>美国国会听证会。</li>
<li>欧洲议会议事平行语料库。</li>
<li>U.S. Supreme Court Oral Argument Transcripts.美国最高法院口头辩论记录。</li>
<li>联合国一般性辩论语料库。</li>
<li>Reddit r&#x2F;legaladvice &amp; r&#x2F;legaladviceofftopic. <ol>
<li>由于大多数法律的语言对于外行人来说通常难以理解，并且无法对简单的法律问题进行明确的回答，因此我们试图找到一个产生“简单英语”问答格式的数据集。我们选择了两个子标题r&#x2F;legaladvice和r&#x2F;legaladviceofftopic。由于存在对不正确的法律的建议进行编码的风险，我们对数据进行了严格的过滤。我们使用profanity-check过滤掉任何带有亵渎的帖子[130]。我们也只包括至少有一个答案的帖子，得分超过8净赞成票。然后，我们将数据重构为：标题：[文章标题]问题：[帖子内容]主题：[Post Flair]答案#[N]：[热门答案]…我们使用PushShift API来抓取每个subreddit的全部内容[9]。</li>
</ol>
</li>
</ol>
</li>
<li>Study Materials<ol>
<li>律师资格考试大纲。</li>
<li>开源案例簿。</li>
</ol>
</li>
</ol>
</blockquote>
<p><img src="https://raw.githubusercontent.com/zhaijingzhi/imgtable/main/202306171117865.png" alt="image-20230617111706742"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/08/21/6%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8RWKV%E6%B8%90%E8%BF%9BpromptLora-1/" data-id="cllkmcmew0004ksu7a1apb7gr" data-title="6月第一周RWKV渐进promptLora" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%8Cpromot%EF%BC%8Clora/" rel="tag">线性注意力，promot，lora</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/QLORA-%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/" rel="tag">QLORA 剪枝 lomo全参数微调</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dbgpt-kagnet/" rel="tag">dbgpt kagnet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BD%8D%E7%BD%AE%E6%8F%92%E5%80%BC-yarn%E6%8F%92%E5%80%BC/" rel="tag">位置插值&#x2F;yarn插值</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%89%AA%E6%9E%9D-%E8%92%B8%E9%A6%8F/" rel="tag">剪枝+蒸馏</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E9%9B%86%E8%BF%9E%E6%8E%A5%E3%80%81%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%89%AA%E6%9E%9D/" rel="tag">密集连接、滤波器剪枝</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%8Cpromot%EF%BC%8Clora/" rel="tag">线性注意力，promot，lora</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B7%A8%E8%AF%AD%E8%A8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" rel="tag">跨语言多模态知识蒸馏</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" rel="tag">通用剪枝</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/QLORA-%E5%89%AA%E6%9E%9D-lomo%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83/" style="font-size: 10px;">QLORA 剪枝 lomo全参数微调</a> <a href="/tags/dbgpt-kagnet/" style="font-size: 10px;">dbgpt kagnet</a> <a href="/tags/%E4%BD%8D%E7%BD%AE%E6%8F%92%E5%80%BC-yarn%E6%8F%92%E5%80%BC/" style="font-size: 10px;">位置插值/yarn插值</a> <a href="/tags/%E5%89%AA%E6%9E%9D-%E8%92%B8%E9%A6%8F/" style="font-size: 10px;">剪枝+蒸馏</a> <a href="/tags/%E5%AF%86%E9%9B%86%E8%BF%9E%E6%8E%A5%E3%80%81%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%89%AA%E6%9E%9D/" style="font-size: 10px;">密集连接、滤波器剪枝</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%8Cpromot%EF%BC%8Clora/" style="font-size: 10px;">线性注意力，promot，lora</a> <a href="/tags/%E8%B7%A8%E8%AF%AD%E8%A8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" style="font-size: 10px;">跨语言多模态知识蒸馏</a> <a href="/tags/%E9%80%9A%E7%94%A8%E5%89%AA%E6%9E%9D/" style="font-size: 10px;">通用剪枝</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/10/08/10%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8chatdb/">10月第一周chatdb</a>
          </li>
        
          <li>
            <a href="/2023/10/08/9%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8%E5%91%A8%E6%8A%A5/">9月第二周周报</a>
          </li>
        
          <li>
            <a href="/2023/10/08/9%E6%9C%88%E7%AC%AC%E4%B8%80%E5%91%A8%E4%BD%8D%E7%BD%AE%E6%8F%92%E5%80%BC/">9月第一周</a>
          </li>
        
          <li>
            <a href="/2023/08/21/7%E6%9C%88%E7%AC%AC%E4%BA%8C%E5%91%A8cvil%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">7月第二周cvil多模态知识蒸馏</a>
          </li>
        
          <li>
            <a href="/2023/08/21/8%E6%9C%88%E7%AC%AC%E4%B8%89%E5%91%A8/">8月第三周HomoDistil</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>